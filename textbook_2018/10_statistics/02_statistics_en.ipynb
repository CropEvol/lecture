{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis(2)\n",
    "To learn various statistical tests and estimates using Scipy, machine learning using Scikit-Learn, and how to use it for genome analysis.\n",
    "\n",
    "## Contents\n",
    "\n",
    "### \"The difference between Machine Learning & Statistics\" & \"How to apply these methods to genome analysis\"\n",
    "1. [Slide](#0.1)\n",
    "\n",
    "Referrence:[Machine Learning vs. Statistics](https://www.svds.com/machine-learning-vs-statistics)\n",
    "\n",
    "### Previous review & supplement\n",
    "1. [Multi Regression](#1.1)\n",
    "1. [Problem](#1.2)\n",
    "1. [Solution](#1.3)\n",
    "\n",
    "### Today's contents\n",
    "1. [Basic flow of Machine Learning](#2.1)\n",
    "1. [Example of variable selection](#2.2)\n",
    "1. [Decision Tree](#2.3)\n",
    "1. [Data preparation](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous review & supplement\n",
    "\n",
    "### 1. Multi Regression<a name=\"1.1\"></a>\n",
    "\n",
    "This model predict the value of the target variable from the value of the explanatory variable.\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/regression_base_en.png?raw=true\" alt=\"reg_base\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "each $x_k$ is 0 or 1. ex) the sample has gene_1 or doesn't has it (1 or 0)\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{e}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} = \\left[\\begin{array}{c}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            ... \\\\\n",
    "            y_n \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{X} = \\left[\\begin{array}{c}\n",
    "            x_{11} & x_{21} & x_{31} & ... & x_{k1} \\\\\n",
    "            x_{12} & x_{22} & x_{32} & ... & x_{k2} \\\\\n",
    "            ... & ... & ... & ... & ...\\\\\n",
    "            x_{1n} & x_{2n} & x_{3n} & ... & x_{kn} \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{\\beta} = \\left[\\begin{array}{c}\n",
    "            \\beta_1 \\\\\n",
    "            \\beta_2 \\\\\n",
    "            ... \\\\\n",
    "            \\beta_k \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{e} = \\left[\\begin{array}{c}\n",
    "            e_1 \\\\\n",
    "            e_2 \\\\\n",
    "            ... \\\\\n",
    "            e_n \\\\\n",
    "        \\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_i} = \\boldsymbol{X_i} \\boldsymbol{\\beta}$, To minimize $\\sum{e_i^2} = \\sum{(\\hat{y_i} - y_i)^2}$, calculate $\\boldsymbol{\\beta}$ by $\\boldsymbol{X'}\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X'}\\boldsymbol{y}$<br><br>\n",
    "→ $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model \n",
    "\n",
    "# loading dataset & convert categorical variables\n",
    "gene_data = pd.read_csv(\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/gene_data.csv?raw=true\", index_col=0)\n",
    "convert_gene_data = pd.get_dummies(gene_data, drop_first=True)\n",
    "convert_gene_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select model\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Explanatory variable: base data of chr10(after fourth col)\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# Target variable: \"LeafWidth\"\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# Prediction\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# display coefficient\n",
    "print(clf.coef_)\n",
    " \n",
    "# display intercept\n",
    "print(clf.intercept_)\n",
    " \n",
    "# display coefficient of determination\n",
    "print(clf.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display the effects of all base\n",
    "plt.figure(figsize=(25, 5), dpi=50)\n",
    "col_names = convert_gene_data.iloc[:, 3:].columns.values\n",
    "col_num = len(col_names)\n",
    "plt.scatter(range(col_num), clf.coef_)\n",
    "plt.xticks(range(col_num), col_names)\n",
    "plt.show()\n",
    "\n",
    "# the effects of chr_3,4,5 are specifically large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Problem<a name=\"1.2\"></a>\n",
    "\n",
    "#### Fundamental problem\n",
    "* Actually effective bases are only chr3,4,5. → But, adding information of bases that have no effect increase the coefficient of determination. \n",
    "* In other words, even the information that is meaningless in practice, the more you add information, the higher the coefficient of determination will be.\n",
    "\n",
    "\n",
    "#### Biological problem\n",
    "* It's unnatural that all bases have effect to specific phenotype.\n",
    "* The number of genes are tremendous large. So if you add the information of all gene, the above fundamental problems arise.\n",
    "* This model doesn't consider the interactions between genes.\n",
    "* ...etc\n",
    "\n",
    "→　This model can perfectly explain about samples for making model. But this model can't adjust to new dataset. <br>\n",
    "　　(Overfitting)<br>\n",
    "\n",
    "#### Mathmatical Problem\n",
    "* Estimating effect of gene is done by $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$ → you need to calculate the inverse matrix of  $\\boldsymbol{X'}\\boldsymbol{X}$\n",
    "* When the number of variables(genes) are large, the dataset may include homologous columns. In this case, the rank drop of the matrix occurs, and the inverse matrix may not be obtained correctly in some cases.\n",
    "* Especially linear regression of genomic data tends to cause above problem, because of the linkage.\n",
    "* To solve such kind of problem, we use a method called \"regularization\" (like this: $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X}+\\lambda\\boldsymbol{I_{p+1}})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example) the more adding meaningless information, the higher the coefficient of determination will be\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/11_statistics/data/variables_en.png?raw=true\" alt=\"reg_base\" width=\"50%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### c.f. how to add new column to data of pandas style\n",
    "df = pd.DataFrame({'A': ['A1', 'A2', 'A3'],\n",
    "                   'B': ['B1', 'B2', 'B3'],\n",
    "                   'C': ['C1', 'C2', 'C3']},\n",
    "                  index=['ONE', 'TWO', 'THREE'])\n",
    "df\n",
    "\n",
    "# df[\"D\"] = [\"D1\", \"D2\", \"D3\"]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add random base data to test_data\n",
    "import random\n",
    "\n",
    "test_data = pd.read_csv(\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/gene_data.csv?raw=true\", index_col=0)\n",
    "\n",
    "# 200 bases list consist of mixed A,T,G,C randomly.\n",
    "random.choices([\"A\", \"T\", \"G\", \"C\"], k=200)\n",
    "\n",
    "### try to write code adding random base data to test_data ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Example) the more adding meaningless information, the higher the coefficient of determination will be\n",
    "import random\n",
    "\n",
    "# copy y(phenotype) value\n",
    "test_data = pd.read_csv(\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/gene_data.csv?raw=true\", index_col=0)\n",
    "\n",
    "# make random meaningless base information\n",
    "for i in range(20):\n",
    "    test_data[\"gene_test_{}\".format(i)] = random.choices([\"A\", \"T\", \"G\", \"C\"], k=200)\n",
    "\n",
    "# convert categorical data\n",
    "convert_test_data = pd.get_dummies(test_data, drop_first=True)\n",
    "\n",
    "# select model\n",
    "clf = linear_model.LinearRegression()\n",
    "    \n",
    "X = convert_test_data.iloc[:, 2:]\n",
    "Y = convert_test_data.iloc[:, 1]\n",
    "\n",
    "# predict\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# display coefficient of determination\n",
    "print(clf.score(X, Y))\n",
    "\n",
    "# show the effects of bases\n",
    "plt.title(\"the effect of each base\")\n",
    "plt.scatter(range(len(clf.coef_)), clf.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example) if the dataset includes homologous columns, you can't estimate valid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Example) if the dataset includes homologous columns, you can't estimate valid values\n",
    "import random\n",
    "\n",
    "# add same dataset\n",
    "test_data = convert_gene_data.iloc[:, 3:]\n",
    "test_data = pd.concat([convert_gene_data, test_data], axis=1)\n",
    "\n",
    "# select model\n",
    "clf = linear_model.LinearRegression()\n",
    "    \n",
    "X = test_data.iloc[:, 3:]\n",
    "Y = test_data.iloc[:, 1]\n",
    "\n",
    "# predict\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# display coefficient\n",
    "print(clf.coef_)\n",
    "\n",
    "# display coefficient of determination\n",
    "print(clf.score(X, Y))\n",
    "\n",
    "# show the effects of bases\n",
    "plt.title(\"the effect of each gene\")\n",
    "plt.scatter(range(len(clf.coef_)), clf.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solution Example<a name=\"1.3\"></a>\n",
    "\n",
    "#### Increase the number of samples\n",
    "* Of course, regression analysis based on 1,000,000 samples is better than regression analysis based on 200 samples\n",
    "\n",
    "#### Variable selection\n",
    "If you can select only explanatory variables which related phenotype($y$), the regression equation may become meaningfull.\n",
    "\n",
    "* biological selection\n",
    "    * select meaningfull variables by GWAS(Genome Wide Association Study)\n",
    "    * etc\n",
    "    \n",
    "\n",
    "* selection by analysis method\n",
    "    * Using an indicator of validity of the model which is different from the determination coefficient.\n",
    "    * There are some method which consider variable selection.\n",
    "\n",
    "\n",
    "* ...etc\n",
    "\n",
    "There is no perfect method, because of prediction. You should choose models according to what you want (Model validity? Prediction accuracy? ...etc)\n",
    "\n",
    "#### Then, when we try to add new samples or use variable selection, we want to know how well the model got better.\n",
    "#### But...How to judge the goodness of model ?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's contents\n",
    "\n",
    "### Basic flow of Machine Learning<a name=\"2.1\"></a>\n",
    "\n",
    "First, how to measure how realistic (meaningful) the regression equation / model is?<br>\n",
    "\n",
    "##### Regression analysis of last lecture\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/data_split1_en.png?raw=true\" alt=\"reg_base\" width=\"70%\" height=\"70%\">\n",
    "<br>\n",
    "In this process, even though coefficient of determination is high, there is a possibility that explanatory power for new data is weak.<br>\n",
    "例)\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/overfit_en.png?raw=true\" alt=\"reg_base\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "→It's difficult to know the prediction accuracy of new datasets. How to know it ?<br>\n",
    "\n",
    "→Split datasets, and intentionally prepare what to treat as a new data set.<br>\n",
    "\n",
    "##### To measure the prediction accuracy (example of regression analysis)\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/data_split2_en.png?raw=true\" alt=\"reg_base\" width=\"65%\" height=\"65%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading library\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Explanatory variable: base data of chr10(after fourth col)\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# Target variable: \"LeafWidth\"\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# Divided into Training Data and Test Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.1)\n",
    "\n",
    "# check the number of each dataset\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First, make model by Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select model\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Predict from Training Data\n",
    "# clf.\n",
    "\n",
    "# display coefficient\n",
    "print(clf.coef_)\n",
    "\n",
    "# display intercept\n",
    "print(clf.intercept_)\n",
    "\n",
    "# display coefficient of determination in Training Dataset\n",
    "print(clf.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict y of Test Data\n",
    "#clf.\n",
    "\n",
    "# display coefficient of determination in Test Dataset\n",
    "#clf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select model\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Predict from Training Data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# display coefficient\n",
    "print(clf.coef_)\n",
    "\n",
    "# display intercept\n",
    "print(clf.intercept_)\n",
    "\n",
    "# display coefficient of determination in Training Dataset\n",
    "print(clf.score(X_train, y_train))\n",
    "\n",
    "# predict y of Test Data\n",
    "print(clf.predict(X_test))\n",
    "\n",
    "# display coefficient of determination in Test Dataset\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# compare the predicted value & the actual value in Test Dataset\n",
    "predict_value = clf.predict(X_test)\n",
    "\n",
    "plt.plot(range(len(y_test)), y_test, color=\"b\")\n",
    "plt.plot(range(len(y_test)), predict_value, color=\"r\")\n",
    "\n",
    "plt.title(\"Measured value vs Prediction\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Example of variable selection<a name=\"2.2\"></a>\n",
    "\n",
    "Especially genome analysis, as in previous examples of multiple regression analysis, there are many cases in which only some of the variables actually have effects.\n",
    "\n",
    "Though it's better if you already know which variables(genes) are important, it's impossible in many cases. So, you need to select variables with some criteria.\n",
    "\n",
    "Then, I show some method to select variables.\n",
    "\n",
    "#### 2.1. Significance test for each variable\n",
    "\n",
    "$y=\\beta_1x_1+\\beta_2x_2+\\beta_3x_3...+e$<br>\n",
    "\n",
    "Check each variables has effect or not, like this: $H_0:\\beta_1=0 \\quad H_1:\\beta_1\\neq0$ and select meaningfull variables<br>\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/yuui_en.png?raw=true\" alt=\"reg_base\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "The library : `statsmodels` is useful to do this analysis.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Exponential Variables\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# Target Variables\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# add Intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# analysis\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p_values of the bases in chr_3,chr_4,chr_5 are small and it's significant.\n",
    "\n",
    "To study statistical modeling, the book : [データ解析のための統計モデリング入門](http://goo.gl/Ufq2) is famous(Japanese only).\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Select model by AIC, BIC\n",
    "\n",
    "The indicator for measuring the goodness of the model:<br>\n",
    "* AIC: Indicator that the model with the best prediction ability is good<br>\n",
    "* BIC: Indicator that the model with the highest probability as a true model is good<br>\n",
    "\n",
    "→ Create several models and select a good model based on AIC.\n",
    "\n",
    "ex)<br>\n",
    "　$y = {gene_1}\\_effect + {gene_2}\\_effect + {gene_3}\\_effect + e$...model(1), AIC=243.8<br>\n",
    "　$y = {gene_1}\\_effect + {gene_2}\\_effect + e$...model(2), AIC=198.3<br>\n",
    "\n",
    "　AIC of model(2) is smaller → model(2) is better for prediction ability\n",
    "\n",
    "`statsmodels` is useful for calculate AIC, BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model based on all bases\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "print(\"all base model's AIC:\", results.aic)\n",
    "print(\"all base model's BIC:\", results.bic)\n",
    "\n",
    "# model based on only bases in chr_3,4,5\n",
    "X = convert_gene_data.iloc[:, 5:14]\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "print(\"chr_3,4,5 base model's AIC:\", results.aic)\n",
    "print(\"chr_3,4,5 base model's BIC:\", results.bic)\n",
    "\n",
    "##### chr_3,4,5 base model has smaller AIC(BIC). So chr_3,4,5 base model is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 2.3 Method with variable selection\n",
    "\n",
    "one of the method with variable selection is Lasso.<br>\n",
    "\n",
    "Lasso(Least absolute shrinkage and selection operator)[[Tibshirani, 1996]](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)<br>\n",
    "\n",
    "In method 2.1, 2.2, Model estimation and variable selection were done separately.<br>\n",
    "\n",
    "In Lasso, some variable's $\\beta$ are estimated 0, So It is possible to estimate models and select variables at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select model(change part: LinearRegression() → Lasso())\n",
    "# the more increasing alpha values, the less variables that are selected\n",
    "clf = linear_model.Lasso(alpha=0.01)\n",
    "\n",
    "# Predict\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# display coefficient\n",
    "print(clf.coef_)\n",
    "\n",
    "# display intercept\n",
    "print(clf.intercept_)\n",
    "\n",
    "# display coefficient of determination in Training Data\n",
    "print(clf.score(X_train, y_train))\n",
    "\n",
    "# display coefficient of determination in Test Data\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (参考) L1正則化\n",
    "\n",
    "* In case of least squares method of multiple regression analysis<br>\n",
    "\n",
    "　　$\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{e}$<br>\n",
    "\n",
    "　　Estimate $\\boldsymbol{\\beta}$ values to minimize $\\sum{e_i^2} = \\sum{(\\boldsymbol{X_i} \\boldsymbol{\\beta} - y_i)^2}$→　 $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$\n",
    "<br><br>\n",
    "\n",
    "* In case of Lasso\n",
    "\n",
    "　　Estimate $\\boldsymbol{\\beta}$ values to minimize $\\sum{(\\boldsymbol{X_i} \\boldsymbol{\\beta} - y_i)^2} + \\lambda||\\boldsymbol{\\beta}||_1 \\quad (||\\boldsymbol{\\beta}||_1=\\sum{|\\beta_i|})$<br><br>\n",
    "　　→　$||\\boldsymbol{\\beta}||_1$ can not differentiate, so it can not be solved analytically, it is obtained by numerical calculation method.<br><br>\n",
    "　　[Coordinate Descent](https://core.ac.uk/download/pdf/6287975.pdf?repositoryId=153), [LARS](http://statweb.stanford.edu/~imj/WEBLIST/2004/LarsAnnStat04.pdf) is famous algorithm to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DecisionTree<a name=\"2.3\"></a>\n",
    "\n",
    "In the machine learning methods, Decision tree is the method which is easy to understand.<br>\n",
    "\n",
    "This method is easy to capture data features, so it's useful to understand not only genome analysis but also some wet experiments.<br>\n",
    "\n",
    "Also, some of the latest methods of machine learning are sometimes based on this decision tree, so it is good to know.<br>\n",
    "\n",
    "##### Example of decision tree\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/tree.png?raw=true\" alt=\"tree\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "Decision tree method split dataset to change dataset understandable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_train)\n",
    "\n",
    "print(\"Training DataのScore\", clf.score(X_train, y_train))\n",
    "print(\"Test DataのScore\", clf.score(X_test, y_test))\n",
    "\n",
    "# display decision tree(!!!It's not good code!!!)\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "dot_data = StringIO()\n",
    "with open(\"tree.dot\", 'w') as f:\n",
    "    tree.export_graphviz(clf, out_file=f, feature_names=convert_gene_data.iloc[:, 3:].columns.values, filled=True, rounded=True, impurity=False, proportion=False,)\n",
    "!dot -T png tree.dot > tree.png\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree method has weak point. Decision tree tends to overfit to samples.<br>\n",
    "\n",
    "The more increasing max_depth, the more increasing score of Training Data & the more decreasing score of Test Data.(The prediction accuracy of new dataset is decreasing.)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Data Preparation<a name=\"2.4\"></a>\n",
    "\n",
    "There are many methods in Machine Learning.　(refference)[scikit-learn algorithm cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)<br>\n",
    "\n",
    "Almost all methods are powerful and useful. But, Quality of Data is also very important. There is a concept of Garbage in, Garbage out.\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/gigo.png?raw=true\" alt=\"gigo\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "It means, No matter how powerful and perfect models are used, if input data quality is bad, the results become bad quality like garbage.<br>\n",
    "\n",
    "In such a meaning, it is highly necessary to think about the quality of the data itself to be analyzed before proceeding with data analysis.<br>\n",
    "\n",
    "In the world of agriculture and biology, it is also possible to obtain more interpretable analysis results by controlling data well.<br>\n",
    "\n",
    "##### (ex) RIL\n",
    "\n",
    "For example, There are RIL(Recombinant Inbred Lines)<br>\n",
    "\n",
    "The genome architecture of RIL population has only parental homozygous, through long term self-mating.<br>\n",
    "\n",
    "This population is very useful for genome analysis because we don't need to consider heterozygous.\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/ril.png?raw=true\" alt=\"gigo\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "In the field of agriculture not targeting humans, it is possible to control the growing environment and genetic structure to some extent with respect to target crops and organisms<br>\n",
    "\n",
    "Therefore, consciousness and ingenuity for what kind of data set and how much scale is to be created is very important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
