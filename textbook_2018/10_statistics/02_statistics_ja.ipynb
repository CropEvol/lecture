{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 統計解析(2)\n",
    "Scipyを用いた各種統計検定・推定、Scikit-Learnを用いた機械学習について触れ、ゲノム解析にどの様に活用出来るのかを学ぶ。\n",
    "\n",
    "## Contents\n",
    "\n",
    "### 機械学習と統計学の違い、ゲノム解析への応用について\n",
    "1. [スライド](#0.1)\n",
    "\n",
    "参考:[Machine Learning vs. Statistics](https://www.svds.com/machine-learning-vs-statistics)\n",
    "\n",
    "### 前回の復習・補足\n",
    "1. [重回帰分析](#1.1)\n",
    "1. [問題点](#1.2)\n",
    "1. [解決法](#1.3)\n",
    "\n",
    "### 今回の実習\n",
    "1. [機械学習の基本的な流れ](#2.1)\n",
    "1. [変数選択手法の一例](#2.2)\n",
    "1. [決定木](#2.3)\n",
    "1. [データの準備](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前回の復習・補足\n",
    "\n",
    "### 1. 重回帰分析<a name=\"1.1\"></a>\n",
    "\n",
    "説明変数によって目的変数の値を説明しようとするモデル。\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/regression_base.png?raw=true\" alt=\"reg_base\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{e}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\boldsymbol{y} = \\left[\\begin{array}{c}\n",
    "            y_1 \\\\\n",
    "            y_2 \\\\\n",
    "            ... \\\\\n",
    "            y_n \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{X} = \\left[\\begin{array}{c}\n",
    "            x_{11} & x_{21} & x_{31} & ... & x_{k1} \\\\\n",
    "            x_{12} & x_{22} & x_{32} & ... & x_{k2} \\\\\n",
    "            ... & ... & ... & ... & ...\\\\\n",
    "            x_{1n} & x_{2n} & x_{3n} & ... & x_{kn} \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{\\beta} = \\left[\\begin{array}{c}\n",
    "            \\beta_1 \\\\\n",
    "            \\beta_2 \\\\\n",
    "            ... \\\\\n",
    "            \\beta_k \\\\\n",
    "        \\end{array}\\right] \\quad\n",
    "\\boldsymbol{e} = \\left[\\begin{array}{c}\n",
    "            e_1 \\\\\n",
    "            e_2 \\\\\n",
    "            ... \\\\\n",
    "            e_n \\\\\n",
    "        \\end{array}\\right]\n",
    "\\end{align}\n",
    "\n",
    "$\\hat{y_i} = \\boldsymbol{X_i} \\boldsymbol{\\beta}$, $\\sum{e_i^2} = \\sum{(\\hat{y_i} - y_i)^2}$を最小とする$\\boldsymbol{\\beta}$を$\\boldsymbol{X'}\\boldsymbol{X}\\boldsymbol{\\beta} = \\boldsymbol{X'}\\boldsymbol{y}$を計算することによって求める。<br><br>\n",
    "→ $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ライブラリの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "\n",
    "# データの読み込み&変換\n",
    "gene_data = pd.read_csv(\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/09_statistics/data/gene_data.csv?raw=true\", index_col=0)\n",
    "convert_gene_data = pd.get_dummies(gene_data, drop_first=True)\n",
    "convert_gene_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデル選択\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# 説明変数にchr10の塩基データ(４列目以降)を利用\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# 目的変数に \"LeafWidth\" を利用\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# 予測モデルを作成\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# 回帰係数\n",
    "print(clf.coef_)\n",
    " \n",
    "# 切片 (誤差)\n",
    "print(clf.intercept_)\n",
    " \n",
    "# 決定係数\n",
    "print(clf.score(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 各遺伝子の効果を見てみる\n",
    "plt.figure(figsize=(25, 5), dpi=50)\n",
    "col_names = convert_gene_data.iloc[:, 3:].columns.values\n",
    "col_num = len(col_names)\n",
    "plt.scatter(range(col_num), clf.coef_)\n",
    "plt.xticks(range(col_num), col_names)\n",
    "plt.show()\n",
    "\n",
    "# 3,4,5番目の塩基の効果だけ特異的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 問題点<a name=\"1.2\"></a>\n",
    "\n",
    "#### 根本的な問題点\n",
    "* 実際に効果のある塩基はchr3,4,5のものだけ　→　しかし、効果の無いはずの塩基の情報を加えると決定係数が上昇する。\n",
    "* つまり、実際には意味の無い遺伝子情報でも、加えれば加えるほど決定係数が上がることになってしまう。\n",
    "\n",
    "\n",
    "#### 生物学的な問題点\n",
    "* 全ての塩基に効果があるという状況は不自然\n",
    "* 遺伝子の数は更に多く、全てを考慮しようとすると上述した問題が発生。\n",
    "* 遺伝子間相互作用など、変数間の交互作用の影響を考慮していない。\n",
    "* ...etc\n",
    "\n",
    "→　予測モデルを作るためのサンプルに関してのみ上手く説明できるが、新たなデータに対し当てはまりの悪い予測モデルになってしまう。<br>\n",
    "　(Overfittingと呼ぶ)<br>\n",
    "\n",
    "#### 数学的な問題点\n",
    "* 遺伝子の効果を推定は$\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$により求める→ $\\boldsymbol{X'}\\boldsymbol{X}$の逆行列が存在する必要がある。\n",
    "* あまりにも変数(遺伝子の数)が多いと全て0の列や相同な列が増え、行列のランク落ちという現象が生じ、逆行列が正しく求められない場合がある。\n",
    "* ゲノムデータを線形回帰すると、連鎖の影響で上述した現象が生じやすい。\n",
    "* この様な問題には\"正則化\"と呼ばれる手法を用いたりする。($\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X}+\\lambda\\boldsymbol{I_{p+1}})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$の様にする)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### 無意味なデータをたくさん加えて回帰分析すると決定係数が上昇する例\n",
    "import random\n",
    "\n",
    "# yの値だけコピー\n",
    "test_data = convert_gene_data.iloc[:, 0:2]\n",
    "\n",
    "# 説明変数にランダムな塩基のデータを発生させる\n",
    "for i in range(20):\n",
    "    test_data[\"gene_test_{}\".format(i)] = random.choices([\"A\", \"T\", \"G\", \"C\"], k=200)\n",
    "\n",
    "test_data\n",
    "# カテゴリカルデータの変換\n",
    "convert_test_data = pd.get_dummies(test_data, drop_first=True)\n",
    "\n",
    "# モデル選択\n",
    "clf = linear_model.LinearRegression()\n",
    "    \n",
    "X = convert_test_data.iloc[:, 2:]\n",
    "Y = convert_test_data.iloc[:, 1]\n",
    "\n",
    "# 予測モデルを作成\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# 決定係数\n",
    "print(clf.score(X, Y))\n",
    "\n",
    "# 各塩基の効果を見てみる\n",
    "plt.title(\"the effect of each base\")\n",
    "plt.scatter(range(len(clf.coef_)), clf.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### 相同な列が存在すると、妥当な推定値が得られない例\n",
    "import random\n",
    "\n",
    "# 全く同じ遺伝子のデータを付け加える\n",
    "test_data = convert_gene_data.iloc[:, 3:]\n",
    "test_data = pd.concat([convert_gene_data, test_data], axis=1)\n",
    "\n",
    "# モデル選択\n",
    "clf = linear_model.LinearRegression()\n",
    "    \n",
    "X = test_data.iloc[:, 3:]\n",
    "Y = test_data.iloc[:, 1]\n",
    "\n",
    "# 予測モデルを作成\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# 偏回帰係数\n",
    "print(clf.coef_)\n",
    "\n",
    "# 決定係数\n",
    "print(clf.score(X, Y))\n",
    "\n",
    "# 各遺伝子の効果を見てみる\n",
    "plt.title(\"the effect of each gene\")\n",
    "plt.scatter(range(len(clf.coef_)), clf.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 解決策の例<a name=\"1.3\"></a>\n",
    "\n",
    "#### データを増やす\n",
    "* 当たり前だが200個体ではなく1,000,000個体のデータから回帰式を作ることができれば…\n",
    "\n",
    "#### 変数選択\n",
    "出来る限り形質($y$)に関与していると考えられる説明変数のみで回帰式を作成することで、意味のある回帰式になると考えられる。\n",
    "\n",
    "* 生物学的な選択方法\n",
    "    * GWAS(Genome Wide Association Study)と呼ばれる手法で有意なものを選ぶ。\n",
    "\n",
    "    \n",
    "* 分析手法による選択方法\n",
    "    * 決定係数とは異なった、モデルの妥当性の指標が存在する。\n",
    "    * 変数選択や次元削減を伴った分析手法が存在する。\n",
    "\n",
    "\n",
    "* ...etc\n",
    "\n",
    "予測である以上完璧な方法というものは存在しない。求めたいもの(予測精度なのか、モデルの妥当性なのか、等)に応じて取捨選択をする。\n",
    "\n",
    "#### では、そもそも意味のある回帰式・意味のあるモデルとは何をもって決めれば良いのだろうか？\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回の実習\n",
    "\n",
    "### 機械学習の基本的な流れについて<a name=\"2.1\"></a>\n",
    "\n",
    "まず、作り出した回帰式・モデルがどのくらい現実的なもの(意味のあるもの)なのか、測定するにはどうすれば良いか。<br>\n",
    "##### これまでの回帰分析\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/data_split1.png?raw=true\" alt=\"reg_base\" width=\"60%\" height=\"60%\">\n",
    "<br>\n",
    "→新たなデータセットに対しても説明力があるか知りたい。<br>\n",
    "\n",
    "→データを分割し、新たなデータセットとして扱うものを意図的に用意しておく<br>\n",
    "\n",
    "##### 予測精度を計測するには(回帰分析の例)\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/data_split2.png?raw=true\" alt=\"reg_base\" width=\"75%\" height=\"75%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ分割の処理はtrain_test_splitという関数で行える\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 説明変数にchr10の塩基データ(４列目以降)を利用\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# 目的変数に \"LeafWidth\" を利用\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# Training DataとTest Dataに分ける\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=0.1)\n",
    "\n",
    "# 各Dataの数を確認\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# モデル選択\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# Training Dataから予測モデルを作成\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 回帰係数\n",
    "print(clf.coef_)\n",
    "\n",
    "# 切片 (誤差)\n",
    "print(clf.intercept_)\n",
    "\n",
    "# 決定係数\n",
    "print(clf.score(X, Y))\n",
    "\n",
    "# Test Dataでの決定係数\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test Dataにおける実測値と予測値の比較\n",
    "predict_value = clf.predict(X_test)\n",
    "\n",
    "plt.plot(range(len(y_test)), y_test, color=\"b\")\n",
    "plt.plot(range(len(y_test)), predict_value, color=\"r\")\n",
    "\n",
    "plt.title(\"Measured value vs Prediction\")\n",
    "plt.xlabel(\"n\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 変数選択手法の一例<a name=\"2.2\"></a>\n",
    "\n",
    "特にゲノムデータを解析する際には、前回の重回帰分析の例や、前回課題のExercise2の様に、実際には一部の変数しか効果を持たない場合も多い。<br>\n",
    "\n",
    "事前にどの変数が効果を持っているのか分かっていれば良いが、その様な状況は殆ど無く、なんらかの基準で変数選択をする必要があるかもしれない。<br>\n",
    "\n",
    "そこで、ここでは変数選択の手法を幾つか紹介する。\n",
    "\n",
    "#### 2.1. 各変数毎の有意性検定\n",
    "\n",
    "$y=\\beta_1x_1+\\beta_2x_2+\\beta_3x_3...+e$<br>\n",
    "\n",
    "この時、$H_0:\\beta_1=0 \\quad H_1:\\beta_1\\neq0$のような形で、各変数が効果を持つかどうか調べ、有意なものを選ぶ。<br>\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/yuui.png?raw=true\" alt=\"reg_base\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "`for`文で各変数の検定を行っても良いが、`statsmodels`という統計モデリング用のライブラリを使うと便利。<br>\n",
    "\n",
    "※今回は統計モデリングについては詳しく触れないため、簡単に使ってみるのみに留める。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# 説明変数の設定\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "\n",
    "# 目的変数の設定\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "\n",
    "# 誤差項の追加\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chr_3,chr_4,chr_5の塩基のp値が低く、有意であると考えられる。\n",
    "\n",
    "また、統計モデリングに関しては、[データ解析のための統計モデリング入門](http://goo.gl/Ufq2)という本が評判は良い。\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 AIC, BICを用いてモデル選択を行う\n",
    "\n",
    "モデルの良さを測る指標として、<br>\n",
    "* AIC(赤池情報量基準):予測能力が最良のモデルを良いとする指標<br>\n",
    "* BIC(ベイズ情報量規準):真のモデルである確率が最も大きいモデルを良いとする指標<br>\n",
    "\n",
    "などを用いる場合がある。幾つかモデルを作成し、AIC等を基準に良いモデルを選択する。\n",
    "\n",
    "例)<br>\n",
    "　$y = {gene_1}\\_effect + {gene_2}\\_effect + {gene_3}\\_effect + e$...モデル(1), AIC=243.8<br>\n",
    "　$y = {gene_1}\\_effect + {gene_2}\\_effect + e$...モデル(2), AIC=198.3<br>\n",
    "\n",
    "　AICの小さいモデル(2)が予測能力が最良のモデルである。\n",
    "\n",
    "AICやBICの計算にも、`statsmodels`が使える。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 全塩基を用いて作成したモデルのAIC, BIC\n",
    "X = convert_gene_data.iloc[:, 3:]\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "print(\"全塩基の情報を入れたモデルのAIC:\", results.aic)\n",
    "print(\"全塩基の情報を入れたモデルのBIC:\", results.bic)\n",
    "\n",
    "# 3,4,5番染色体の塩基のみを用いて作成したモデルのAIC, BIC\n",
    "X = convert_gene_data.iloc[:, 5:14]\n",
    "Y = convert_gene_data.loc[:, \"LeafWidth\"]\n",
    "X = sm.add_constant(X)\n",
    "results = sm.OLS(endog=Y, exog=X).fit()\n",
    "print(\"3,4,5番染色体のみの情報を入れたモデルのAIC:\", results.aic)\n",
    "print(\"3,4,5番染色体のみの情報を入れたモデルのBIC:\", results.bic)\n",
    "\n",
    "##### 3,4,5番染色体のみの情報を入れたモデルの方が、AIC(BIC)が小さい。よってモデルとしては後者の方が良いと判断される。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### 2.3 変数選択を伴う手法\n",
    "\n",
    "変数選択を行う手法の一つであるLassoを紹介する。<br>\n",
    "\n",
    "Lasso(Least absolute shrinkage and selection operator)[[Tibshirani, 1996]](http://statweb.stanford.edu/~tibs/lasso/lasso.pdf)<br>\n",
    "\n",
    "2.1, 2.2の手法ではモデルの推定と変数選択は別々に行っていた。<br>\n",
    "Lassoでは$\\beta$の一部が0として推定され、モデルの推定と変数選択を同時に行うことが可能。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# モデル選択(ここをLinearRegression()からLasso()に変えるだけ)\n",
    "# alphaの値が大きくなるほど、選択する変数の数が少なくなる。\n",
    "clf = linear_model.Lasso(alpha=0.01)\n",
    "\n",
    "# Training Dataから予測モデルを作成\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# 回帰係数\n",
    "print(clf.coef_)\n",
    "\n",
    "# 切片 (誤差)\n",
    "print(clf.intercept_)\n",
    "\n",
    "# 決定係数\n",
    "print(clf.score(X_train, y_train))\n",
    "\n",
    "# Test Dataでの決定係数\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (参考) L1正則化\n",
    "\n",
    "* 重回帰分析の最小二乗法の場合<br>\n",
    "\n",
    "　　$\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{e}$<br>\n",
    "\n",
    "　　$\\sum{e_i^2} = \\sum{(\\boldsymbol{X_i} \\boldsymbol{\\beta} - y_i)^2}$を最小とする$\\boldsymbol{\\beta}$を求める。→　 $\\boldsymbol{\\hat{\\beta}} = (\\boldsymbol{X'}\\boldsymbol{X})^{-1}\\boldsymbol{X'}\\boldsymbol{y}$\n",
    "<br><br>\n",
    "\n",
    "* Lasso(L1正則化)の場合\n",
    "\n",
    "　　$\\sum{(\\boldsymbol{X_i} \\boldsymbol{\\beta} - y_i)^2} + \\lambda||\\boldsymbol{\\beta}||_1 \\quad (||\\boldsymbol{\\beta}||_1=\\sum{|\\beta_i|})$<br><br>\n",
    "　　を最小とする$\\boldsymbol{\\beta}$を求める。　→　$||\\boldsymbol{\\beta}||_1$を微分出来ず、解析的には解けないため、数値計算的な手法により求める。<br><br>\n",
    "　　[Coordinate Descent](https://core.ac.uk/download/pdf/6287975.pdf?repositoryId=153), [LARS](http://statweb.stanford.edu/~imj/WEBLIST/2004/LarsAnnStat04.pdf)といったアルゴリズムによる求め方が有名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouplassoとかの例\n",
    "# なぜプログラムするのか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 決定木<a name=\"2.3\"></a>\n",
    "\n",
    "機械学習の手法の中でも、中身の理解しやすい手法として、決定木というものがある。<br>\n",
    "\n",
    "データの特徴を説明しやすい手法であるので、ゲノム解析に限らず、実験系のデータを解釈する際にも便利かと思われる。<br>\n",
    "\n",
    "また、機械学習の最新の手法の幾つかは、この決定木をベースに作られていることもあるため、知っておくと良い。<br>\n",
    "\n",
    "##### 決定木の一例\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/tree.png?raw=true\" alt=\"tree\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "決定木とは上の様に、データを分割し、より分かり易い形に変えていく手法になる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# グラフ描写用\n",
    "!conda install -y graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(max_depth=3)\n",
    "\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = clf.predict(X_train)\n",
    "\n",
    "print(\"Training DataのScore\", clf.score(X_train, y_train))\n",
    "print(\"Test DataのScore\", clf.score(X_test, y_test))\n",
    "\n",
    "# 決定木を描写するコード(無理矢理表示させているので、参考にはしない様に)\n",
    "from sklearn.externals.six import StringIO\n",
    "from IPython.display import Image\n",
    "dot_data = StringIO()\n",
    "with open(\"tree.dot\", 'w') as f:\n",
    "    tree.export_graphviz(clf, out_file=f, feature_names=convert_gene_data.iloc[:, 3:].columns.values, filled=True, rounded=True, impurity=False, proportion=False,)\n",
    "!dot -T png tree.dot > tree.png\n",
    "Image('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "決定木の弱点として、深い木になるほど過学習しやすいという点が挙げられる。<br>\n",
    "max_depthの値を大きくするほど、Training Dataのスコアが上がり、Test Dataのスコアが下がっていく。(新たなデータセットに対する予測精度が下がる。)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. データの準備<a name=\"2.4\"></a>\n",
    "\n",
    "機械学習にはその他様々な手法がある。　(参考)[scikit-learnのアルゴリズムチートシート](\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\")<br>\n",
    "\n",
    "どれも便利で強力な手法だが、一方でGarbage in, Garbage outと呼ばれる概念がある様にデータの質というものもまた重要となる。\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/gigo.png?raw=true\" alt=\"gigo\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "つまり、どれほど強力で完璧なモデルを利用しても、入力するデータの質が悪ければ、得られる予測結果などの質も悪くなる場合が多い。<br>\n",
    "\n",
    "そういった意味で、データ解析を進める前に、解析するデータそのものの質というものを考える必要性は高い。<br>\n",
    "\n",
    "また、農学・生物学の世界では、データを上手くコントロールすることで、より解釈のしやすい解析結果を得ることも可能になる。<br>\n",
    "\n",
    "##### (例) RIL集団\n",
    "\n",
    "例えば、RIL(Recombinant Inbred Lines)と呼ばれるサンプル集団がある。<br>\n",
    "\n",
    "RILは、自家交配を繰り返すことで、ゲノムの構造をどちらかの親のホモ接合体のみに揃えた集団になる。<br>\n",
    "\n",
    "この集団は、解析の際にヘテロな組み合わせを考慮する必要がなく、ゲノム解析を行う際には非常に便利なデータセットと言える。\n",
    "\n",
    "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2018/10_statistics/data/ril.png?raw=true\" alt=\"gigo\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "人を対象としない農学の分野では、対象となる作物や生物に関して、生育する環境や遺伝子構造をある程度コントロール出来る。<br>\n",
    "\n",
    "そのため、どの様なデータセットをどのくらいの規模で作成するのか、といった点への意識や工夫が非常に重要である。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
