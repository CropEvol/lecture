{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L10_ML_classification.ipynb","provenance":[],"collapsed_sections":["mwgZf3SCrogu","y_WHcn8rr5tx","n5DpLAzzfJjx","_hsBGAhUQGK7","LqpX0D6FQb2c","DAuQXi-4QPal","lbRvxEtJRxhb","3jDSoleTSVEY","lr67xF2KxVOA","mbFzM-F8-XL0","F8Z-rCOUlhfT","0O9KBzLqhbR8","aYMhW0RGJ1G9","MWU88Zyyjpxt","aGJvLTNhdT7A","jqzYBkDSdxFZ","R-tImsCQcclG","jpCFpMjlk9_U","SMKI4YILgR6q","V4t7glyTeRPy","1MEfY5fbgJbx","92eNbSuDyvVN","K0bRHgtx6g-K","q90BpMEsCfm7"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DrdvBwenQaTK"},"source":["<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/lec_title.png?raw=true\" alt=\"2019年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HjoFnGBDyAhc","colab":{}},"source":["# /// 実習前に、このセルを実行してください。 ///\n","\n","# サンプルデータをダウンロード\n","!wget -O AB_classification.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/AB_classification.csv\n","# 「分類」アルゴリズムの描画関数などをダウンロード\n","!wget -O classification.py https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/modules/classification.py\n","# 決定木描画用のライブラリをインストール\n","!pip install -q dtreeviz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dwcC4dXNwBbj"},"source":["# 機械学習 - 分類 -"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ikpep3JSwnRO"},"source":["　今回、教師あり学習（Supervised learning）のひとつ、**分類（Classification）**の各種アルゴリズムを勉強します。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"abOkroywxBzb"},"source":["## この実習で使用するデータセット\n","\n","　各種の分類アルゴリズムを試すために、次のような[データセット](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/AB_classification.csv)を使います。\n","\n","- サンプル数（計84サンプル）\n","  - 種A 46サンプル\n","  - 種B 38サンプル\n","\n","- 説明変数（分類では、**特徴量（Feature）**と呼ぶことが多いです）\n","  - x1, x2\n","\n","- 目的変数（分類では、**クラス（Class）**や**ラベル（Label）**と呼ぶことが多いです）\n","  - species\n","\n","\n","<small>*※ このデータセットは、以下の論文のデータセットを一部改変したデータです:  \n","Ohta, A., Yamane, K., & Kawahara, T. (2017). [Relationship between spike morphology and habitat of four Aegilops species of section Sitopsis.](https://link.springer.com/article/10.1007/s10722-016-0408-x) Genetic resources and crop evolution, 64(5), 889-899.*</small>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"lhuGS4vCKAC6","colab_type":"code","colab":{}},"source":["# 読み込み\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","df = pd.read_csv(\"AB_classification.csv\", sep=\",\", header=0)\n","\n","# 説明変数\n","x = df.loc[:,[\"x1\",\"x2\"]]\n","cols = x.columns  # 列名\n","x = np.array(x)    # Numpy配列に変換\n","\n","# ラベル変数\n","y = np.array(df.loc[:,\"species\"])  # 種名\n","\n","# グラフ\n","a = df[df[\"species\"] == \"A\"]\n","b = df[df[\"species\"] == \"B\"]\n","plt.scatter(a[\"x1\"], a[\"x2\"], color=\"red\", label=\"A\")\n","plt.scatter(b[\"x1\"], b[\"x2\"], color=\"blue\", label=\"B\")\n","plt.xlabel(\"x1\")\n","plt.ylabel(\"x2\")\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","plt.show"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R9ztyFWexIJ1"},"source":["## 今回の実習内容\n","1. 前処理\n","1. ロジスティック回帰 Logistic regression\n","1. サポートベクトルマシン Support vector machine (SVM)\n","1. 決定木 Decision tree\n","1. ランダムフォレスト Random Forest\n","1. ニューラルネットワーク Neural network\n","1. 評価\n","\n","---"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OuQtkQE5Fh7D"},"source":["## 前処理\n","\n","　「回帰」のときと同じく、「分類」でも前処理は必要です。ここでは、次の3つの前処理をおこないます。\n","1. データの分割: トレーニングデータとテストデータを作る\n","  - トレーニングデータ 75%、テストデータ 25%\n","  - ラベルの割合が、分割後のデータでも維持されるように分割する（オプション`stratify`）\n","\n","2. スケーリング: 説明変数の尺度を揃える\n","  - トレーニングデータを「標準化」（平均0、標準偏差1となるように変換）\n","  - テストデータも、トレーニングデータの変換水準で「標準化」\n","\n","3. ラベルの数値変換: ラベル変数を整数値に変換する\n","  - `A` → `0`\n","  - `B` → `1`\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DzfEibXIAkv7","colab":{}},"source":["# (1) データの分割\n","from sklearn.model_selection import train_test_split\n","splited_dataset = train_test_split(x, y, test_size=0.25, stratify=y, random_state=0)\n","x_train_raw, x_test_raw = splited_dataset[0], splited_dataset[1]\n","y_train_raw, y_test_raw = splited_dataset[2], splited_dataset[3]\n","\n","# 確認\n","print(\"original: \", np.unique(y, return_counts=True))      # 元のデータセット内の各クラスのサンプル数\n","print(\"training: \", np.unique(y_train_raw, return_counts=True))# トレーニングデータ内の各クラスのサンプル数\n","print(\"test: \", np.unique(y_test_raw, return_counts=True))    # テストデータ内の各クラスのサンプル数\n","\n","# (2) スケーリング\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","x_train_ss = ss.fit_transform(x_train_raw) # トレーニングデータ\n","x_test_ss = ss.transform(x_test_raw) # テストデータ\n","\n","# 確認\n","#print(x_test_ss)\n","\n","# (3) ラベルの数値変換\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y_train_le = le.fit_transform(y_train_raw) # トレーニングデータ\n","y_test_le = le.transform(y_test_raw) # テストデータ\n","\n","# 確認\n","print(y_test_raw)\n","print(y_test_le)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vc2I-x793LlT","colab_type":"text"},"source":["### 実習1\n","\n","　下記コードの`train_test_split()`のオプション`stratify=b`を削除すると、分割後の各ラベルのサンプル数がどうなるか確認してください。"]},{"cell_type":"code","metadata":{"id":"16i1cc1F5Glj","colab_type":"code","colab":{}},"source":["import numpy as np\n","# データ\n","a = np.ones(10)\n","b = np.array([\"rice\"]*5 + [\"wheat\"]*5)\n","\n","# (1) データの分割\n","from sklearn.model_selection import train_test_split\n","a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.4, stratify=b, random_state=30)\n","#a_train, a_test, b_train, b_test = train_test_split(a, b, test_size=0.4, random_state=30)\n","\n","# 確認\n","print(\"original: \", np.unique(b, return_counts=True))      # 元のデータセット内の各クラスのサンプル数\n","print(\"training: \", np.unique(b_train, return_counts=True))# トレーニングデータ内の各クラスのサンプル数\n","print(\"test: \", np.unique(b_test, return_counts=True))    # テストデータ内の各クラスのサンプル数"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mwgZf3SCrogu","colab_type":"text"},"source":["### [詳細] データの分割"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"91jzX-3Wuu9j"},"source":["　「分類」でも、モデルの学習とモデルの評価のために、データセットを分割して、トレーニングデータ（学習用データ）とテストデータ（評価用データ）を作成します。\n","\n","　「回帰」のときとほぼ同じコードを使いますが、一点だけ異なっています。オプションに`stratify`を追加している点です。このオプションは、分割前のデータセットに含まれるラベルの割合を、分割後のトレーニングデータやテストデータでも同じ割合に維持するオプションです。このオプションには、ラベルデータを指定します。\n","\n","```python\n","# 書き方例\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0, stratify=y)\n","```\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DEIaGsigPzcj","colab":{}},"source":["# (1) データの分割: トレーニングデータ80%, テストデータ20%\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, stratify=y, random_state=0)\n","\n","# 確認\n","print(\"original: \", np.unique(y, return_counts=True))      # 元のデータセット内の各クラスのサンプル数\n","print(\"training: \", np.unique(y_train, return_counts=True))# トレーニングデータ内の各クラスのサンプル数\n","print(\"test: \", np.unique(y_test, return_counts=True))    # テストデータ内の各クラスのサンプル数"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_WHcn8rr5tx","colab_type":"text"},"source":["### [詳細] スケーリング"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"u8E8Pd9qW4Vh"},"source":["　分類アルゴリズムのいくつかは、「説明変数」のスケーリングを推奨しています。ここでは、「標準化 Standardization」（平均0、標準偏差1に変換）をおこなっています。\n","\n","- スケーリングを推奨しているアルゴリズム\n","  - ロジスティック回帰\n","  - サポートベクトルマシン\n","  - ニューラルネットワーク\n","- スケーリング不要なアルゴリズム\n","  - 決定木\n","  - ランダムフォレスト\n","\n","```python\n","# 書き方\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","x_train_ss = ss.fit_transform(トレーニングデータの説明変数)\n","x_test_ss = ss.transform(テストデータの説明変数)\n","```\n","\n","<small>*※ スケーリングの有無による違いも確認できるように、スケーリング後のデータには新しい変数名を付けています。*</small>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"M9u9Lkm9W37N","colab":{}},"source":["# (2) スケーリング: 標準化\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","x_train_ss = ss.fit_transform(x_train)\n","x_test_ss = ss.transform(x_test)\n","\n","# 確認\n","print(x_test_ss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n5DpLAzzfJjx"},"source":["### [詳細] ラベルの数値変換"]},{"cell_type":"markdown","metadata":{"id":"im_6_6mIsQh_","colab_type":"text"},"source":["　「ラベル変数」を次のように整数値「0,1,2,...」に変換します。必須でないこともありますが、推奨されている操作です。\n","  - `A` → `0`\n","  - `B` → `1`\n","\n","<small>*※ あとで使用する分類結果を可視化する関数`draw_decision_boundary`には、数値化ラベルを使うので、ここでは必須です。*</small>\n","\n","```python\n","# 書き方\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y_train_le = le.fit_transform(トレーニングデータのラベル変数)\n","y_test_le  = le.transform(テストデータのラベル変数)\n","```"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pczbt9SPjm9G","colab":{}},"source":["# (3) ラベルの数値変換\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","\n","# トレーニングデータ\n","le.fit(y_train)\n","y_train_le = le.transform(y_train)\n","#y_train_le = le.fit_transform(y1_train)　# 上の2行と同じ\n","\n","# テストデータ\n","y_test_le = le.transform(y_test)\n","\n","# 確認\n","print(y_test_le)\n","\n","# 整数変換されたラベルを元に戻すことも可能\n","print(le.inverse_transform(y_test_le))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BM8LsBXDtyv-","colab_type":"text"},"source":["　上述のように、ラベル変数を整数値「0,1,2,...」に変換する方法の他に、**One-hot**表現に変換する方法があります。Irisデータセットを例にすると、`Iris-setosa`の場合、`Iris-setosa=1, Iris-versicolor=0, Iris-virginica=0`のように、一つのスイッチのみが「1」、その他のスイッチは「0」の形に変換する方法です。次のようなリスト状のデータに変換します。\n","- Irisデータセット\n","  - `Iris-setosa` → `[1,0,0]`\n","  - `Iris-versicolor` → `[0,1,0]`\n","  - `Iris-virginica` → `[0,0,1]`\n","\n","- 今回のデータセットの場合、\n","  - `A` → `[1,0]`\n","  - `B` → `[0,1]`\n","\n","```python\n","# 書き方\n","from sklearn.preprocessing import OneHotEncoder\n","oe = OneHotEncoder()\n","y_train_oe = oe.fit_transform(トレーニングデータのラベル変数)\n","y_test_oe  = oe.transform(テストデータのラベル変数)\n","```\n","\n","　今回のscikit-learnのプログラムでは、ラベル変数をOne-hot表現にする必要はありません。しかし、Irisiデータセットのような、多クラス分類ではよく使われる変換方法ですので、こちらもよく使われるので覚えておくと良いでしょう。"]},{"cell_type":"code","metadata":{"id":"KZ3ZT64bydtC","colab_type":"code","colab":{}},"source":["# One-hot表現に変換\n","from sklearn.preprocessing import OneHotEncoder\n","oe = OneHotEncoder(handle_unknown='ignore')\n","\n","# トレーニングデータ\n","y_train_oe = oe.fit_transform(y_train.reshape(-1, 1))\n","\n","# テストデータ\n","y_test_oe = oe.transform(y_test.reshape(-1, 1))\n","\n","# 確認: .toarray()をつけた方がわかりやすい\n","print(y_test_oe.toarray())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2tqJO6PHP3M4"},"source":["## ロジスティック回帰 Logistic regression\n","\n","　**ロジスティック回帰（Logisitic regression）**は、線形回帰に少し手を加えて、**あるクラスに分類される確率を求める手法**です。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/logistic_regression.png?raw=true\" alt=\"logistic_regression\" height=\"250px\">\n","\n","　ロジスティック回帰の式は、次のとおりです（詳しくは「ロジスティック回帰式の導出」参照）。\n","\n","$$ z = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon $$\n","$$ p = \\frac{1}{1+e^{-z}} $$\n","\n","- $k$番目($k=1,2,3,...,m$)の係数$\\beta$と説明変数$x$をそれぞれ$\\beta_k$、$x_k$で表しています。$\\epsilon$（epsilon）は誤差（切片）です。\n","- $p$はあるクラスに属している確率を表しています。$e$はネイピア数（=2.7182...）です。  \n","\n","　このロジスティック回帰モデルをトレーニングすると、係数$\\beta$や誤差$\\epsilon$が求まります。その予測モデルに、新しいデータ$x=(x_1, x_2, ..., x_k, ..., x_m)$を与えると、あるクラスに属している確率$p$がわかります。\n","\n","　どうやってモデルの学習するかについては、後述の「ロジスティック回帰のトレーニング方法」を参照してください。\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_hsBGAhUQGK7"},"source":["### ロジスティック回帰の式を詳しく...\n","<small>*※ 読み飛ばしてOKです。ロジスティック回帰の式の背景を数式を使って説明しています。*</small>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1QY0Jd4DQLrg"},"source":["　ロジスティック回帰の式を理解するためには、**オッズ(odds)**から始める必要があります。\n","\n","　ある事象Aが起こる確率を$p$としたとき、Aが起こらない確率は$1-p$となります。このとき、Aの起こる確率とAが起こらない確率の比は、$$\\frac{p}{1-p}$$となります。これがオッズと呼ばれるもので、Aが起こる「見込み」を表しています。ここで、オッズの意味を深く理解する必要はありません。最終的に知りたいことは、**データから確率$p$を推測する式**です。\n","\n","　オッズの対数（**対数オッズ**）をとり、関数とみなすと、$$f(p) = \\log{\\frac{p}{1-p}}$$となります。これは、**ロジット関数（logit）**として知られる関数です。\n","\n","　ロジット関数を逆関数にすると、$f(p)$から$p$を求める関数が得られます。\n","\\begin{align*}\n","f(p) &= \\log{\\frac{p}{1-p}} \\\\\n","e^{f(p)} &= \\frac{p}{1-p} \\\\\n","e^{f(p)} - e^{f(p)} p &= p \\\\\n","p &= {\\frac{e^{f(p)}}{e^{f(p)}+1}} \\\\ &= \\frac{1}{1+e^{-f(p)}}\n","\\end{align*}\n","\n","　ここで、$f(p) = z$ とすると、ロジスティック回帰のふたつ目の式が得られます。これは、**ロジスティック関数（logistic）**として知られる関数です。$$ p = \\frac{1}{1+e^{-z}} $$この関数は、入力値$z$からAが起こる確率$p$を求める関数とみなせます。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/logistic_curve.png?raw=true\" alt=\"logistic_curve\" height=\"200px\">\n","\n","　ロジスティック回帰は、線形回帰の式をロジスティック関数の入力$z$につなげたものです。これは、「データ$x$」を「Aがおこる確率$p$」に変換する式です。\n","\n","$$ z = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon $$\n","\n","$$ p = \\frac{1}{1+e^{-(\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon)}} $$\n","\n","　"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LqpX0D6FQb2c"},"source":["### ロジスティック回帰モデルのコスト関数\n","<small>*※ 読み飛ばしてOKです。*</small>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NBaUKxOcQiQ0"},"source":["　ロジスティック回帰モデルも、線形回帰と同じく、正解ラベルと予測ラベルが最も一致するように学習をおこないます。異なるのは、ロジスティック回帰の出力が「確率」である点です。そこで、ロジスティック回帰は、正解ラベルと一致している確率が最も高くなるように学習します。それを表したのが次の式です。\n","\n","$$L = \\prod_{i=1}^{n}(p_{i})^{y_{i}}(1-p_{i})^{1-y_{i}}$$\n","\n","ここで、$i$はサンプル番号、$p_i$はラベル「1」である予測確率、$y_i$は正解ラベルを表しています。この式は、**尤度 Likelihood**と呼ばれる式です。\n","\n","　仮に、1番目のサンプルの正解ラベルが$y_1=1$であった場合、\n","$$L_1 = (p_1)^{1}（1-p_1）^{1-1}=p_1$$\n","になります。これは、ラベル「1」である予測確率です。一方で、2番目のサンプルの正解ラベルが$y_2=0$であった場合、\n","$$L_2 = (p_2)^{0}（1-p_2）^{1-0}=1-p_2$$\n","になり、ラベル「0」である予測確率（ラベル「1」ではない予測確率）となります。このような計算をすべてのサンプルにわたっておこない、尤度を求めます。\n","$$例: L = p_1 (1-p_2) p_3 p_4 (1-p_5) ... (1-p_{n-1}) p_n$$\n","\n","　ロジスティック回帰モデルでは、目的関数「尤度（予測確率の積）」が最大となる$\\beta_{optimum}$や$\\epsilon_{optimum}$を探索します。実際には、微分を容易にするために（また、小数点数の総積によるアンダフローを防ぐ目的で）、尤度の式を対数（**対数尤度 Log likelihood**）にします。\n","$$\\log{L} = \\sum_{i=1}^{n}{ [ y_{i} \\log{p_{i}} + (1-y_{i}) \\log{(1-p_{i})} ] }  $$\n","\n","　さらに、勾配法などで最小値を調べるために、上の式に-1をかけて「コスト関数」にしています。これは、**交差エントロピー誤差関数（Cross-entropy error function）**と呼ばれる関数です。\n","\n","$$ Cost = -\\log{L} = \\sum_{i=1}^{n}{ [ -y_{i} \\log{p_{i}} - (1-y_{i}) \\log{(1-p_{i})} ] }$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DAuQXi-4QPal"},"source":["### ロジスティック回帰モデルのトレーニング方法"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wIfI0dKDQVH4"},"source":["　ロジスティック回帰の $\\beta$ や $\\epsilon$ の最適解をどうやって求めるか? ロジスティック回帰では、**交差エントロピー誤差関数（Cross-entropy error function）**と呼ばれる関数を最小化する$\\beta$ や $\\epsilon$ を探索します（Wikipedia「[Cross-entropy error function and logistic regression](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression)」）。この関数は、非常に複雑で、解析的に（最小二乗法のような計算で）$\\beta$ や $\\epsilon$ を求められません。そこで、勾配法などのパラメータを繰り返し更新する「反復法 Iterative method」で最適解に近づきます。どのような手法が使われているか知りたい場合は、[scikit-learn ユーザーガイド](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)などを参照してください。\n","\n","\n","　勾配法を使った最適化については、[Python Machine Learning](https://www.packtpub.com/data/python-machine-learning-third-edition)（原書）/ [Python機械学習プログラミング](https://book.impress.co.jp/books/1117101099)（日本語訳）にも概説されています。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lbRvxEtJRxhb"},"source":["### 多クラス(Multi class)の分類"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C0pK6DcER0y6"},"source":["　ここでは2種類の方法を紹介します。\n","\n","- 一対他 One-vs-Rest (OvR)  \n","　各クラスに対して、2クラスの分類をおこないます（例えば、「種Aかそれ以外か」「種Bかそれ以外か」「種Cかそれ以外か」）。そうすると、各クラスである予測確率が得られます。そのうち、最も予測確率が高いクラスを「予測値」として採用します。\n","  - 各確率の合計が1にならないこともあります\n","  - Aの確率 $P_{A} = 0.70$、A以外の確率 $P_{A} = 0.30$\n","  - Bの確率 $P_{B} = 0.20$、B以外の確率 $P_{B} = 0.80$\n","  - Cの確率 $P_{C} = 0.01$、C以外の確率 $P_{C} = 0.99$\n","  - 予測ラベル: A\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/OvR.png?raw=true\" alt=\"OvR\" height=\"200px\">\n","\n","- Multinomial  \n","　各クラスの予測確率を同時に得る方法です。最も確率が高いものを「予測値」として採用します。\n","  - 各確率の合計は1になります\n","  - Aの確率 $P_{A} = 0.70$\n","  - Bの確率 $P_{B} = 0.25$\n","  - Cの確率 $P_{C} = 0.05$\n","  - 予測ラベル: A\n","\n","　どの方法が使えるかについては、分類アルゴリズムや最適化手法で決まります。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pc4aMjCWR7YX"},"source":["### 実装\n","\n","　ロジスティック回帰を実装してみましょう。\n","\n","```python\n","# 書き方\n","from sklearn.linear_model import LogisticRegression\n","model_lr = LogisticRegression(ハイパーパラメータ)\n","model_lr.fit(トレーニングデータ)\n","```\n","\n","以下で使っているハイパーパラメータ（詳しくはscikit-learn [LogisiticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)参照）:\n","- `solver=\"sag\"`: 最適化アルゴリズムに「Stochastic Average Gradient (SAG)」と呼ばれる確率的勾配法を使用する\n","- `max_iter=100`: トレーニング回数（反復回数）を100回にする\n","\n","\n","<small>*※ グラフ描画の関数[`draw_decision_boundary`](https://github.com/CropEvol/lecture/blob/master/textbook_2019/modules/classification.py)は、こちらで用意した自作関数です。*</small>\n","\n","<small>*※ 予測モデルを評価するために、正解率（Accuracy）を調べるコードも書いています。詳しくは、後述の「モデルの評価」で学びます。*</small>\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ap3wyAa0SB8Y","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","# モデルを作成\n","from sklearn.linear_model import LogisticRegression\n","model_lr = LogisticRegression(solver=\"sag\", max_iter=100, random_state=0)\n","# モデルをトレーニング\n","model_lr.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model_lr, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","print(\"Coefficient=\", model_lr.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model_lr.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model_lr.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FY94mJutSJAL"},"source":["### 実習2\n","\n","　`LogisticRegression()`内のオプションを変更して、実行結果がどう変わるか（または、変わらないか）を観察してください。なお、オプションの変更には、[`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)のマニュアルページを参照してください。\n","\n","変更するオプション:\n","- 最適化手法`solver=\"sag\"`を変更し、別の最適化手法を使用してください。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1jO2ZUEVSKjR","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.linear_model import LogisticRegression\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = LogisticRegression(solver=\"sag\", max_iter=100, random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","print(\"Coefficient=\", model2.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3jDSoleTSVEY"},"source":["#### 解答例\n"]},{"cell_type":"markdown","metadata":{"id":"EtN2-bYoUP_-","colab_type":"text"},"source":["　以下では、`solver=\"lbfgs\"`を設定し、「L-BFGS法」と言われる反復法を使ったロジスティック回帰をおこなっています。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ESaMgNXNSWqK","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.linear_model import LogisticRegression\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = LogisticRegression(solver=\"lbfgs\", max_iter=100, random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","print(\"Coefficient=\", model2.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2fmZcYm5ehi_"},"source":["## サポートベクトルマシン Suport vector machine (SVM)\n","\n","　**サポートベクトルマシン（Suport vector machine: SVM）**は、たいへん人気のある分類手法です。この手法は、**最も近いデータまでの距離（マージン margin）が最大となる境界を作り、データを二分する方法**です。この予測モデルは、線形関数+[符号関数（sign関数）](https://ja.wikipedia.org/wiki/%E7%AC%A6%E5%8F%B7%E9%96%A2%E6%95%B0)です。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm.png?raw=true\" alt=\"svm\" height=\"250px\">\n","\n","$$ 予測モデル: f(x) = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + e $$\n","\n","$$\n","y = \\begin{cases} \n","1 & (f(x) \\geq 0) \\\\\n","-1 & (f(x) < 0)\n","\\end{cases} $$\n","\n","$$ 境界: \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + e = 0 $$\n","\n","- $k$番目($k=1,2,3,...,m$)の係数$\\beta$と説明変数$x$をそれぞれ$\\beta_k$、$x_k$で表しています。$e$は誤差（切片）です。\n","- $y=1$であれば「正クラス」、$y=-1$であれば「正クラスではない（負クラス）」と予測します。\n","\n","　サポートベクトルマシンは、下図のようなイメージです。トレーニングデータを学習して、マージンを最大化できる境界線（または境界面）を探索します。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lr67xF2KxVOA"},"source":["### サポートベクトルマシンを詳しく...\n","<small>*※ 読み飛ばしてOKです。数式を使って、サポートベクトルマシンを詳しく説明しています。*</small>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FDhE6aARJTwO"},"source":["　まず、2つのクラスを完璧に分割できる境界$\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_mx_m + e = 0$を考えます。ベクトルを使って記述をすると、次のように書けます。\n","\n","$$\n","境界: \\beta^{T} x + e = 0 \\\\\n","\\beta=\\begin{pmatrix}\n","\\beta_1 \\\\\n","\\beta_2 \\\\\n","\\vdots \\\\\n","\\beta_m\n","\\end{pmatrix}, \n","x=\\begin{pmatrix}\n","x_1 \\\\\n","x_2 \\\\\n","\\vdots \\\\\n","x_m\n","\\end{pmatrix}\n","$$\n","\n","　次に、この境界に最も近い正クラスデータ $x_+$ と負クラスデータ $x_-$ について考えます。このような境界に最も近いデータのことを**サポートベクトル**と言います。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail1.png?raw=true\" alt=\"svm_detail1\" height=\"200px\">\n","\n","　さらに、正クラス側のサポートベクトルを通り、境界に平行な直線（または面）を考えます。同様に、負クラス側の直線も考えると、次の2つの式を用意できます。これらは、マージンの両端を表す式です。右辺を1と-1にしていますが、この値はなんでも構いません（$e$の値を調整することで、右辺を1や-1にできます）。\n","\n","\\begin{align*}\n","\\beta^{T} x_+ + e &= 1 \\\\\n","\\beta^{T} x_- + e &= -1\n","\\end{align*}\n","すなわち、\n","$$|\\beta^{T} x_- + e| = 1$$\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail2.png?raw=true\" alt=\"svm_detail2\" height=\"200px\">\n","\n","　正ラベル側のすべての点について考えると、それらの点は、マージンの外側、すなわち、次の不等式で表されるエリアにあるはずです。\n","$$ \\beta^{T} x_i + e \\geq 1 $$\n","正ラベルの値は「1」で表されます。ラベルの値 $t_i=1$ を上の式にかけた不等式は、\n","$$ t_i (\\beta^{T} x_i + e) \\geq 1 $$\n","\n","負ラベル（$t_i=-1$）側についても同様に、\n","$$ \\beta^{T} x_i + e \\leq -1 $$\n","$$ t_i (\\beta^{T} x_i + e) \\geq 1 $$\n","\n","したがって、すべての点は次の式で表せます。この式は、最適な$\\beta$や$e$を探索する際の条件のひとつです。\n","$$ t_i (\\beta^{T} x_i + e) \\geq 1 $$\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail3.png?raw=true\" alt=\"svm_detail3\" height=\"200px\">\n","\n","　今度は、最小化関数（コスト関数）について考えていきます。サポートベクトルと境界の距離$d$は、次の式で表されます（これは高校で習った「垂線の長さ（点と直線の距離）」を求める式と同じです）。\n","$$ d = \\frac{| \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_m x_m + e |}{\\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2 }} = \\frac{ |\\beta^{T} x + e | }{ \\|\\beta\\| } $$\n","\n","この式の分子は、$ |\\beta^{T} x + e | = 1$より、\n","$$ d =  \\frac{ 1 }{ \\|\\beta\\| } $$\n","\n","　したがって、境界間の距離（マージン）は、次のとおりです。\n","$$ \\frac{ 2 }{ \\|\\beta\\| },  \\|\\beta\\| = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2} $$\n","\n","　サポートベクトルマシンは、このマージンを最大化するように学習していきます。「マージンの最大化」は、「$\\|w\\|$の最小化」と同じです。また、$\\|\\beta\\| = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2} \\geq 0 $ のため、「$\\|w\\|$の最小化」は「$\\|w\\|^2$の最小化」と同じです。そこで、サポートベクトルマシンは、この$\\|\\beta\\|^2$を2で割った関数を「コスト関数」として、問題を解いています（2で割っているのは、この関数を微分したときの計算が簡単になるからです）。\n","$$Cost = \\frac{\\|\\beta\\|^2}{2}$$\n","\n","\n","　以上をまとめると、サポートベクトルマシンでは、\n","$$ 制約条件: t_i (\\beta^{T} x_i + e) \\geq 1 $$を満たしつつ、\n","$$ コスト関数: Cost = \\frac{\\|\\beta\\|^2}{2}$$\n","を最小化させる$\\beta$と$e$を求めます。\n","\n","　この$\\beta$と$e$の最適値を求める方法については、さらに複雑です。通常、「逐次最小問題最適化法 Sequential Minimal Optimization (SMO)」と呼ばれる方法で、解を探索します。\n","\n","　なお、上述したサポートベクトルマシンの説明は、2つのクラスを完全に線形分離できるという仮定で話をしていました。そのマージンのことを **ハードマージン（Hard margin）** と言います。しかし、実際の問題では、多くの場合、線形方程式では完全に分離できません。そこで、ある程度の誤分類を許容した方法も考案されており、よく利用されています。なお、この誤分類を許容したマージンのことを**ソフトマージン（Soft margin）**と言います。"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mbFzM-F8-XL0"},"source":["### カーネルサポートベクトルマシン Kernel support vector machine (Kernel SVM)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TII5jowD_K-G"},"source":["　サポートベクトルマシンは、線形方程式では決して分離できない問題にも対応可能です。\n","\n","　どのように対応するか? **カーネルトリック Kernel trick**と言われる方法を使います。この方法は、オリジナルな低次元のデータから、高次元のデータを作り出し、その高次元データで問題を解決しようという方法です。\n","\n","　例えば、2個の説明変数 $(x_1, x_2)$ で線形分離できない問題に対して、次のようなカーネル化をおこないます（なお、カーネル化方法は他にもあります）。\n","$$(x_1, x_2) \\rightarrow (x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$$\n","そして、この新しくできた$(x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$を使って、SVMモデルを構築します。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/kernelSVM.png?raw=true\" alt=\"kernelSVM\" height=\"200px\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mhw6M-jlsmDg"},"source":["### 実装\n","\n","　サポートベクトルマシンを実装してみましょう。scikit-learnを使うと簡単にコーディングできます。\n","\n","```python\n","# 書き方\n","from sklearn.svm import SVC\n","model_lr = SVC(ハイパーパラメータ)\n","model_lr.fit(トレーニングデータ)\n","```\n","\n","\n","以下で使っているハイパーパラメータ（詳しくはscikit-learn [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)参照）:\n","- `kernel=\"linear\"`: 線形SVM\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z4chSOv9spE9","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","# モデルを作成\n","from sklearn.svm import SVC\n","model_svm = SVC(kernel=\"linear\", random_state=0) # 線形SVM\n","# モデルをトレーニング\n","model_svm.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model_svm, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","if model_svm.kernel == \"linear\": \n","  print(\"Coefficient=\", model_svm.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model_svm.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model_svm.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Kw8hXCU0leQL"},"source":["### 実習3\n","\n","　`SVC()`内のオプションを変更して、その実行結果を観察してください。なお、オプションの変更には、[`SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)のマニュアルページを参照してください。\n","\n","変更するオプション:  \n","- カーネル化オプション`kernel=\"linear\"`（線形SVM）を変更し、他のカーネル化手法を使用してください。\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pf6mf3SDlhLy","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.svm import SVC\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = SVC(kernel=\"linear\", random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","if model2.kernel == \"linear\": \n","  print(\"Coefficient=\", model2.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F8Z-rCOUlhfT"},"source":["#### 解答例"]},{"cell_type":"markdown","metadata":{"id":"ptqls-XfW3bv","colab_type":"text"},"source":["　以下のコードでは、カーネル化の手法に「RBF」を使っています。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OD3dmXllli2i","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.svm import SVC\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = SVC(kernel=\"rbf\", random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 係数b\n","if model2.kernel == \"linear\": \n","  print(\"Coefficient=\", model2.coef_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZUtOrUcAewf2"},"source":["## 決定木 Decision tree\n","\n","　**決定木（Decision tree）**は、**説明変数に対する問い（True or False）を繰り返して、木構造の分類モデルを作る方法**です。出来上がった予測モデルは、他の機械学習のモデルに比べると、理解しやすいです。しかし、決定木の予測精度はそれほど高くありません。\n","\n","　ここで決定木の勉強をするのは、次に紹介する「ランダムフォレスト」のためです。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree.png?raw=true\" alt=\"decision_tree\" height=\"350px\">\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0O9KBzLqhbR8"},"source":["### 各分岐をどのように設定するか？\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BaxJ_dxJjovz"},"source":["　決定木の各分岐は、適当に設定されているわけではありません。\"なるべく少ない質問でクラス分けする\"ように木構造を作っていきます（木に例えると、枝を伸ばしていきます）。\n","\n","　どの説明変数を分岐の質問に使うか？　分割前と分割後の「**情報量**または**不純度**」の差（**情報利得 Information gain**）が大きくなる質問を、あらゆる質問の中から選択します。「情報量」や「不純度」は、多様なラベル（クラス）がそのデータセットにどのぐらい含まれているかを表す指標です。「情報利得」は、分岐によりどれだけクラス分けできるかを表す指標です。\n","\n","$$情報利得 = I_{pre} - (I_{true} + I_{false})$$\n","\n","- $I_{pre}$は分割前データの情報量（不純度）\n","- $I_{true}$は分割後Trueデータの情報量（不純度）\n","- $I_{false}$は分割後Falseデータの情報量（不純度）\n","\n","　決定木では、情報量を表す指標として、**エントロピー（Entropy）**が使われます。また、不純度を表す指標には、**ジニ不純度（Gini impurity）**と呼ばれる指標が使われます。\n","\n","\n","　\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aYMhW0RGJ1G9","colab_type":"text"},"source":["### 分岐の設定方法を直感的に理解する"]},{"cell_type":"markdown","metadata":{"id":"KLsIUwZWJ28a","colab_type":"text"},"source":["　次のようなデータセットから、鳥類・哺乳類の分類モデル（決定木）を作りたいとします。最初の質問にはどの説明変数を使うと良いでしょうか？\n","\n","||A. 食性|B. 発生形態|C. 体温|分類ラベル（マーク）|\n","|:---|---:|---:|---:|---:|\n","|ニワトリ|草食|卵生|恒温|鳥類（●）|\n","|ペンギン|肉食|卵生|恒温|鳥類（●）|\n","|カモノハシ|肉食|卵生|恒温|哺乳類（○）|\n","|ウシ|草食|胎生|恒温|哺乳類（○）|\n","|ヒツジ|草食|胎生|恒温|哺乳類（○）|\n","|ライオン|肉食|胎生|恒温|哺乳類（○）|\n","\n","　それぞれの説明変数で分けてみると、次のようになります。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree_which_features.png?raw=true\" alt=\"decision_tree_which_features\" height=\"230px\">\n","\n","- 「A. 食性」で分けた場合、分割後のそれぞれのサブセットの中には、まだ両方のラベルが混ざっています。ラベルの比率は、どちらのサブセットも元のデータセットの比率が維持されています。この質問は、ラベル分割に関して言えば、ほとんど情報を得ることができない質問です。\n","- 「B. 発生形態」で分けた場合、「胎生」サブセットは、両方のラベルが含まれています。ラベル比率は、元のデータセットからすると、鳥類（●）の割合が高くなっています。一方、「卵生」サブセット側は、1種類のラベルのみとなっています。この質問は、\"元のデータセットから哺乳類（○）の多くを分ける\"質問であることがわかります。すなわち、ラベル分割に関して、情報を得ることが可能な質問です。\n","- 「C. 体温」を使った場合、全く分割できません。この説明変数を使ったとしても、元のデータセットと同じ状態が維持されており、何も情報を得ることができていません。\n","\n","\n","\n","　\"なるべく少ない質問でクラス分け\" するためには、最も情報が得られそうな「B. 発生形態」を最初の質問に採用するのが良さそうです。\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MWU88Zyyjpxt"},"source":["### エントロピー 、ジニ不純度、情報利得\n","<small>*※ 読み飛ばしてOKです。「各分岐をどのように設定するか？」や「分岐の設定方法を直感的に理解する」を数式を使って、詳しく説明しています。*</small>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IijqnJmpjsU0"},"source":["#### 情報利得\n","\n","　決定木の学習アルゴリズムにおける目的関数は、「情報利得（Information gain）」です。情報利得を最大化するように、分岐を作成し、木構造を発達させていきます。情報利得を定式化すると次のようになります。\n","\n","$$ 情報利得: IG = I(D_{pre}) - (\\frac{N_{true}}{N_{pre}}I(D_{true}) + \\frac{N_{false}}{N_{pre}}I(D_{false})) $$ \n","\n","- $I(D_{pre})$は、データセットにおけるエントロピーまたはジニ不純度\n","- $I(D_{true})$は、分割後のTrueデータセットにおけるエントロピーまたはジニ不純度\n","- $I(D_{false})$は、分割後のFalseデータセットにおけるエントロピーまたはジニ不純度\n","- $N_{pre}$は、データセットのサンプル数\n","- $N_{false}$は、分割後のTrueデータセットのサンプル数\n","- $N_{false}$は、分割後のFalseデータセットのサンプル数\n","\n","　次に、$I(D)$についてです。これにはいくつか種類あり、「エントロピー（Entropy）」または「ジニ不純度（Gini impurity）」がよく使われます。どちらの指標も、データセット中に含まれる各クラスの割合をその計算に使います。以下では、ラベルAとラベルBの分類を例にしており、データセット中のそれぞれのラベルの割合を$p_{A}$、$p_{B}$と表記しています。\n","\n","#### エントロピー\n","　データセットの「エントロピー」は、次の式で計算されます。なお、詳細は省きますが、この式は、データセットの平均情報量（情報量の期待値）を求める式でもあります。\n","\n","$$エントロピー: I(D) = p_{A}log_2 \\frac{1}{p_{A}} + p_{B}log_2 \\frac{1}{p_{B}} = -p_{A}log_2 p_{A} - p_{B}log_2 p_{B}$$\n","\n","#### ジニ不純度\n","　　データセットの「ジニ不純度」は、次の式で求めます。\n","\n","$$ジニ不純度: I(D) = p_{A}(1 - p_{A}) + p_{B}(1 - p_{B})$$\n","\n","#### 計算例\n","　上述「分岐の設定方法を直感的に理解する」の例において、それぞれの説明変数の「エントロピーを使った情報利得」や「ジニ不純度を使った情報利得」がいくつになるか、以下で求めてみます。\n","\n","||A. 食性|B. 発生形態|C. 体温|分類ラベル（マーク）|\n","|:---|---:|---:|---:|---:|\n","|ニワトリ|草食|卵生|恒温|鳥類（●）|\n","|ペンギン|肉食|卵生|恒温|鳥類（●）|\n","|カモノハシ|肉食|卵生|恒温|哺乳類（○）|\n","|ウシ|草食|胎生|恒温|哺乳類（○）|\n","|ヒツジ|草食|胎生|恒温|哺乳類（○）|\n","|ライオン|肉食|胎生|恒温|哺乳類（○）|\n","\n","- エントロピーを使った情報利得\n","  - A. 食性\n","$$\n","I(D_{pre}) = - \\frac{2}{6} log_{2} \\frac{2}{6} - \\frac{4}{6} log_{2} \\frac{4}{6} = 0.9182... \\\\\n","I(D_{true}) = -\\frac{1}{3} log_{2} \\frac{1}{3} - \\frac{2}{3} log_{2} \\frac{2}{3} = 0.9182... \\\\\n","I(D_{false}) = - \\frac{1}{3} log_{2} \\frac{1}{3} - \\frac{2}{3} log_{2} \\frac{2}{3} = 0.9182... \\\\\n","IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0\n","$$\n","  - B. 発生形態\n","$$\n","I(D_{pre}) = 0.9182... \\\\\n","I(D_{true}) = 0.9182... \\\\\n","I(D_{false}) = 0 \\\\\n","IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0.4591...\n","$$\n","  - C. 体温\n","$$\n","I(D_{pre}) = 0.9182... \\\\\n","I(D_{true}) = 0.9182... \\\\\n","IG = I(D_{pre}) - \\frac{6}{6} I(D_{true}) = 0\n","$$\n","  - 分岐の質問には『「B. 発生形態」は卵生/胎生？』が選ばれます。\n","\n","- ジニ不純度を使った情報利得\n","  - A. 食性\n","$$\n","I(D_{pre}) = \\frac{2}{6} (1-\\frac{4}{6}) + \\frac{4}{6} (1-\\frac{2}{6}) = 0.4444... \\\\\n","I(D_{true}) = \\frac{1}{3} (1-\\frac{1}{3}) + \\frac{2}{3} (1-\\frac{2}{3}) = 0.4444... \\\\\n","I(D_{false}) = \\frac{1}{3} (1-\\frac{1}{3}) + \\frac{2}{3} (1-\\frac{2}{3}) = 0.4444... \\\\\n","IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0\n","$$\n","  - B. 発生形態\n","$$\n","I(D_{pre}) = 0.4444... \\\\\n","I(D_{true}) = 0.4444... \\\\\n","I(D_{false}) = 0 \\\\\n","IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0.2222...\n","$$\n","  - C. 体温\n","$$\n","I(D_{pre}) = 0.4444... \\\\\n","I(D_{true}) = 0.4444... \\\\\n","IG = I(D_{pre}) - \\frac{6}{6} I(D_{true}) = 0\n","$$\n","  - 分岐の質問には『「B. 発生形態」は卵生/胎生？』が選ばれます。\n","　\n","- どちらの評価指標を使っても、通常はほとんど同じ結果になります。"]},{"cell_type":"markdown","metadata":{"id":"aGJvLTNhdT7A","colab_type":"text"},"source":["### 決定木では、説明変数をスケーリングしなくて良い"]},{"cell_type":"markdown","metadata":{"id":"gt_OiS06dizx","colab_type":"text"},"source":["　各データの順序は、スケーリングの前後で変わりません。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/scalling.png?raw=true\" alt=\"scalling\" height=\"300px\">\n","\n","　決定木による解析において、スケーリングする前とスケーリングした後では、閾値の値が変更されるのみで、スケーリングの有無で分岐の結果が変わることはありません。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree_with_scalling.png?raw=true\" alt=\"decision_tree_with_scalling\" height=\"250px\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"BuxG11Qij6AH"},"source":["### 実装\n","\n","　scikit-learnを使って、決定木を実装してみましょう。\n","\n","```python\n","# 書き方\n","from sklearn.tree import DecisionTreeClassifier\n","model_dt = DecisionTreeClassifier(ハイパーパラメータ)\n","model_dt.fit(トレーニングデータ)\n","```\n","\n","\n","以下で使っているハイパーパラメータ（詳しくはscikit-learn [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)参照）:\n","- `criterion=\"gini\"`: 不純度の指標として「ジニ不純度」を使う\n","- `max_depth=None`: 決定木の深さの最大値を「上限なし」に設定する\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ChwiFfVxj7pU","colab":{}},"source":["# 使用するデータ\n","# （説明変数は、スケーリング前のデータ）\n","x_train, y_train = x_train_raw, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le   # テストデータ\n","\n","# モデルを作成\n","from sklearn.tree import DecisionTreeClassifier\n","model_dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=None, random_state=0)\n","\n","# モデルをトレーニング\n","model_dt.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model_dt, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 評価（正解率）\n","print(\"training data: \", model_dt.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model_dt.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y8l2hioqtVJV"},"source":["　決定木を可視化すると、"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xiBSOWv94TE0","colab":{}},"source":["# === メジャーな方法 ===\n","import graphviz\n","from sklearn.tree import export_graphviz\n","# 予測モデルをグラフ用データに変換\n","export_graphviz(model_dt, out_file=\"tree.dot\", \n","                feature_names=cols, \n","                class_names=[\"A\", \"B\"], \n","                filled=True, rounded=True)\n","# 表示\n","with open('tree.dot') as g:\n","  dot_graph = g.read()\n","graphviz.Source(dot_graph)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7jT95vTetbWE","colab":{}},"source":["# === マイナーな方法（おすすめ） ===\n","from dtreeviz.trees import dtreeviz\n","\n","# 予測モデルをグラフ用データに変換\n","viz = dtreeviz(model_dt, x_train, y_train, \n","    feature_names=cols, \n","    class_names=[\"A\", \"B\"], \n","    target_name='species',\n","    orientation ='LR')\n","# 表示\n","viz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqzYBkDSdxFZ","colab_type":"text"},"source":["### 過学習（オーバーフィッティング Overfitting）"]},{"cell_type":"markdown","metadata":{"id":"1nta8azGdxzF","colab_type":"text"},"source":["　トレーニングデータの正解率が高い一方で、テストデータの正解率が低い状態を、**過学習（オーバーフィッティング overfitting）**と言います。トレーニングデータに\"合わせ過ぎた\"予測モデルが作られており、新しいデータに対する予測精度を損なっている状態です。\n","\n","　過学習に陥るおもな要因は2つです。\n","- トレーニングデータの量が少ない。もしくは、偏ったデータのみをトレーニングデータとして使っている。\n","- モデルが複雑すぎる。\n","\n","　ひとつ目のトレーニングデータ量や質の問題は、データの量を増やすか、質を改善するかしかありません。\n","\n","　ふたつ目のモデルの複雑さについては、より単純なモデルを選択することで解決できる場合があります。\n","\n","　決定木の場合、木の深さを設定しない場合（すなわち、複雑なモデルの場合）、すべてのサンプルを強引にクラス分けするため、過学習になりやすいです。木の深さを制限して、単純なモデルを構築するか、別の手法（ランダムフォレストなど）を用いることで解決できるかもしれません。"]},{"cell_type":"markdown","metadata":{"id":"26IeTgPBZ-hu","colab_type":"text"},"source":["### 実習4\n","\n","　`DecisionTreeClassifier()`内のオプションを変更して、その実行結果を観察してください。なお、オプションの変更には、[`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)のマニュアルページを参照してください。\n","\n","変更するオプション:  \n","- 決定木の深さ`max_depth=None`（上限なし）を、`max_depth=2`に変更してください。\n","- 不純度の指標`criterion=\"gini\"`（ジニ不純度）を変更し、「エントロピー」を使用するようにしてください。\n","\n","\n","　また、使用する説明変数を、スケーリングありに変更しても、同じ結果が得られることを確認してください。"]},{"cell_type":"code","metadata":{"id":"LSN5OKm1aAtv","colab_type":"code","colab":{}},"source":["# 使用するデータ\n","# スケーリング前のデータ\n","x_train, y_train = x_train_raw, y_train_le     # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le      # テストデータ\n","# スケーリング後のデータ\n","#x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","#x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.tree import DecisionTreeClassifier\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = DecisionTreeClassifier(criterion=\"gini\", max_depth=None, random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ\n","\n","# 予測モデルをグラフ用データに変換\n","from dtreeviz.trees import dtreeviz\n","viz = dtreeviz(model2, x_train, y_train, \n","    feature_names=cols, \n","    class_names=[\"A\", \"B\"], \n","    target_name='species',\n","    orientation ='LR')\n","# 表示\n","viz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R-tImsCQcclG","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"markdown","metadata":{"id":"RIJczUPRc_IK","colab_type":"text"},"source":["　以下では、`criterion=\"entropy\"`と`max_depth=2`を同時に設定しています。"]},{"cell_type":"code","metadata":{"id":"_6bRtm_2ceoE","colab_type":"code","colab":{}},"source":["# 使用するデータ\n","# スケーリング前のデータ\n","x_train, y_train = x_train_raw, y_train_le     # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le      # テストデータ\n","# スケーリング後のデータ\n","#x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","#x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.tree import DecisionTreeClassifier\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","#model2 = DecisionTreeClassifier(criterion=\"gini\", max_depth=2, random_state=0)\n","#model2 = DecisionTreeClassifier(criterion=\"entropy\", max_depth=None, random_state=0)\n","model2 = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2, random_state=0)\n","\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ\n","\n","from dtreeviz.trees import dtreeviz\n","\n","# 予測モデルをグラフ用データに変換\n","viz = dtreeviz(model2, x_train, y_train, \n","    feature_names=cols, \n","    class_names=[\"A\", \"B\"], \n","    target_name='species',\n","    orientation ='LR')\n","# 表示\n","viz"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"01FrjDefeLJ9","colab_type":"text"},"source":["## ランダムフォレスト Random forest\n","\n","　**ランダムフォレスト（Random forest）**は、複数の決定木を作り、**各決定木の予測結果を多数決して、最終的な予測値を決める手法**です。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/random_forest.png?raw=true\" alt=\"random_forest\" height=\"250px\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jpCFpMjlk9_U"},"source":["### アルゴリズム"]},{"cell_type":"markdown","metadata":{"id":"70bam4bHfo6l","colab_type":"text"},"source":["ランダムフォレストは、複数の決定木を使ったモデルです。このモデルのアルゴリズムは次のとおりです。\n","1. オリジナルのデータセットから、$n$個のサンプルをランダムに抽出します。この時、復元抽出（重複を許して抽出）をおこないます。このようなサンプリング方法を、**ブートストラップサンプリング（Bootstrap sampling）**といいます。\n","\n","1. このブートストラップ標本を使って、決定木を構築します。\n","\n","1. 1,2のステップを$k$回繰り返します。得られたk個の決定木がランダムフォレストです。\n","\n","1. 新しいデータのラベルを予測する際には、各決定木から予測値を集め、多数決により最終的な予測値を決定します。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/random_forest_algorithm.png?raw=true\" alt=\"random_forest_algorithm\" height=\"180px\">\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FWHd65v3la71"},"source":["### 実装\n","\n","　ランダムフォレストを実装してみましょう。\n","\n","```python\n","# 書き方\n","from sklearn.ensemble import RandomForestClassifier\n","model_dt = RandomForestClassifier(ハイパーパラメータ)\n","model_dt.fit(トレーニングデータ)\n","```\n","\n","以下で使っているハイパーパラメータ（詳しくはscikit-learn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)参照）:\n","- `n_estimators=100`: 決定木の数\n","- `criterion=\"gini\"`: 不純度の指標として「ジニ不純度」を使う \n","- `max_depth=2`: 木の深さの最大値を「2」に設定する\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PUb7WplklcVR","colab":{}},"source":["# 使用するデータ\n","# （説明変数は、スケーリング前のデータ）\n","x_train, y_train = x_train_raw, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le   # テストデータ\n","\n","# モデルを作成\n","from sklearn.ensemble import RandomForestClassifier\n","model_rf = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=2, random_state=0)\n","\n","# モデルをトレーニング\n","model_rf.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model_rf, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 各説明変数の重要度\n","print('Importances:', model_rf.feature_importances_ )\n","\n","# 評価（正解率）\n","print(\"training data: \", model_rf.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model_rf.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SMKI4YILgR6q","colab_type":"text"},"source":["### 各説明変数の重要度"]},{"cell_type":"markdown","metadata":{"id":"4YO9q15-gXNR","colab_type":"text"},"source":["　ランダムフォレストでは、`モデル名.feature_importances_`により、**各説明変数の重要度（Feature importances）**、すなわち、クラス分けに重要な説明変数を調べることが可能です。この重要度はどのようにして算出されているか？ **Permutation Importance**と呼ばれる手法を使って、各説明変数の重要度を調べています。\n","\n","　「Permutation Importance」の概要は次のとおりです。\n","\n","1. 説明変数をひとつ選びます。\n","\n","1. 選んだ説明変数のデータをシャッフルします。\n","1. その結果、精度がどの程度下がるかを調べます。\n","  - ほとんど精度が変わらないようであれば、その説明変数の重要度は低い\n","  - 精度が劇的に下がるようであれば、その重要度は高い\n","1. 1-3をすべての説明変数についておこないます。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/permutation_importance.png?raw=true\" alt=\"permutation_importance\" height=\"300px\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-KGTyA8OeOrm","colab_type":"text"},"source":["### 実習5\n","\n","　`RandomForestClassifier()`内のオプションを変更して、その実行結果を観察してください。なお、オプションの変更には、[`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)のマニュアルページを参照してください。\n","\n","変更するオプション:  \n","- ブートストラップサンプリングの数（決定木の数）を`n_estimators=10`に変更してください。\n","- ブートストラップサンプリングの数（決定木の数）を`n_estimators=1000`に変更してください。\n","\n","　また、あるオプションを追加して、ブートストラッピング（Bootstrapping）、すなわち、復元抽出（重複を許して抽出）をOFFにすると、「決定木」と同じ結果が得られます。マニュアルページで追加するオプションを調べて、それを確かめてください。\n"]},{"cell_type":"code","metadata":{"id":"tYSuo79GeQ0y","colab_type":"code","colab":{}},"source":["# 使用するデータ\n","# スケーリング前のデータ\n","x_train, y_train = x_train_raw, y_train_le     # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le      # テストデータ\n","\n","from sklearn.ensemble import RandomForestClassifier\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=2, random_state=0)\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 各説明変数の重要度\n","print('Importances:', model2.feature_importances_ )\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V4t7glyTeRPy","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"markdown","metadata":{"id":"ZiT2BSV7eTD7","colab_type":"text"},"source":["　以下のコードでは、`n_estimators=10`に設定しています。"]},{"cell_type":"code","metadata":{"id":"9wO3ohtheSmy","colab_type":"code","colab":{}},"source":["# 使用するデータ\n","# スケーリング前のデータ\n","x_train, y_train = x_train_raw, y_train_le     # トレーニングデータ\n","x_test, y_test  = x_test_raw, y_test_le      # テストデータ\n","# スケーリング後のデータ\n","#x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","#x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","from sklearn.ensemble import RandomForestClassifier\n","# ============== 編集エリア(start) =============\n","# モデルの作成\n","model2 = RandomForestClassifier(n_estimators=10, criterion=\"gini\", max_depth=2, random_state=0)\n","#model2 = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=2, random_state=0)\n","#model2 = RandomForestClassifier(n_estimators=1000, criterion=\"gini\", max_depth=2, random_state=0)\n","#model2 = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=2, random_state=0, bootstrap=False)\n","\n","# ============== 編集エリア(end) ==============\n","# モデルの学習\n","model2.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model2, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# 各説明変数の重要度\n","print('Importances:', model2.feature_importances_ )\n","\n","# 評価（正解率）\n","print(\"training data: \", model2.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model2.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zRyJ8gGThGHV","colab_type":"text"},"source":["## ニューラルネットワーク Neural network\n","\n","　**ニューラルネットワーク（Neural network）**は、身の回りにある多くのIoT機器（インターネットに接続された電子機器）やWebサービスの内部でよく使われている機械学習手法です（例をあげると、Google翻訳やデジタルカメラやスマホカメラの顔認識機能、など）。\n","\n","　**神経細胞が信号を伝達する仕組みを模した機械学習アルゴリズム**が使われています。この予測モデルからは、各分類クラスに含まれる確率が出力されます。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/neural_network.png?raw=true\" alt=\"neural_network\" height=\"250px\">\n","\n","　なお、ニューラルネットワークには多くの種類があります（参考: [The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/) / THE ASIMOV INSTITUTE）。\n"]},{"cell_type":"markdown","metadata":{"id":"1MEfY5fbgJbx","colab_type":"text"},"source":["### ニューラルネットワークのユニット"]},{"cell_type":"markdown","metadata":{"id":"4VAmGOexiUz7","colab_type":"text"},"source":["　ニューラルネットワークは、入力値（Input）である数値データが、入力層、中間層、出力層を経て、出力値（Output）に変換される機械学習手法です。各層は、1個以上の**ユニット**（上述の図では、$x_1$、$h_{11}$、$y$など、円で描かれているもの）で構成されています。\n","\n","　以下では、各層のユニットがどのような値を受け取るかを、どのような値を出力するかを説明しています。\n","\n","<small>*※ 各層には、バイアス（誤差）を表すユニットもありますが、ここではバイアスユニットを省略しています。*</small>\n","\n","\n","\n","#### ◆ 入力層（Input layer）\n","　この層のユニットの数は、説明変数の数と同じです。各ユニットは、説明変数の数値をそのまま受け取ります。入力層の各ユニットが受け取った数値は、次の層（隠れ層 第1層）の入力データになります。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_input_layer.png?raw=true\" alt=\"NN_input_layer\" height=\"300px\">\n","\n","\n","\n","#### ◆ 隠れ層（Hidden layers）\n","　隠れ層 第1層の各ユニットは、入力層のすべてのユニットからデータを受け取ります。その際、入力層の各ユニットから出力される数値にいくらかの**重み（weight）**をかけた値を受け取ります。さらに、受け取った数値の合計値を **活性化関数（Activation function）** と言われる関数で変換して、次の層への出力値を発生させます。\n","\n","\\begin{align}\n","例)　t_{11} &= w_{x_{1}\\to h_{11}} x_{1} + w_{x_{2}\\to h_{11}} x_{2} \\\\\n","h_{11} &= activation(t_{11})\n","\\end{align}\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_hidden_layer.png?raw=true\" alt=\"NN_hidden_layer\" height=\"300px\">\n","\n","よく使われる活性化関数:\n","- ReLU関数\n","- シグモイド関数（ロジスティック関数）\n","- tanh関数\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_activation_function.png?raw=true\" alt=\"NN_activation_function\" height=\"200px\">\n","\n","　「活性化関数」は、非線形（直線ではない）関数を基本的には使います。この理由については、下記の「活性化関数に非線形関数を使うのはなぜか？」を参照してください。\n","\n","　隠れ層　第2層以降についても基本的には同じです。前層のすべてのユニットから出力される数値に重みをかけた値を受け取ります。そして、活性化関数によって、数値の変換をおこなった後、次の層に向けて出力します。\n","\n","\\begin{align}\n","例)　t_{21} &= w_{h_{11}\\to h_{21}} h_{11} + w_{h_{12}\\to h_{21}} h_{12} + w_{h_{13}\\to h_{21}} h_{13} \\\\\n","h_{21} &= activation(t_{21})\n","\\end{align}\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_hidden_layer2.png?raw=true\" alt=\"NN_hidden_layer2\" height=\"300px\">\n","\n","#### ◆ 出力層（output layer）\n","　\n","　出力層も、数値の受け取りに関しては、隠れ層と同じです: 隠れ層の最後の層にある各ユニットから出力される数値に重みをかけた値を受け取ります。受け取った数値の合計値を**ソフトマックス関数（Softmax関数）**で変換します。ソフトマックス関数についての詳細は省きますが、変換後の値を各クラスに所属する「確率」とみなして予測値を得ます。\n","\n","\\begin{align}\n","例)　t_{y} &= w_{h_{31}\\to y} h_{31} + w_{h_{32}\\to y} h_{32} + w_{h_{33}\\to y} h_{33} \\\\\n","y &= softmax(t_{y})\n","\\end{align}\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_output_layer.png?raw=true\" alt=\"NN_output_layer\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"92eNbSuDyvVN","colab_type":"text"},"source":["### ニューラルネットワークの重みの更新"]},{"cell_type":"markdown","metadata":{"id":"FVJ1TjqFUQ1L","colab_type":"text"},"source":["　各ユニット間の重みは、モデルの学習で随時更新されます。非線形な活性化関数が間に入っているため、解析的に（計算でピンポイントに）それぞれの重み$w$の最適値を求めることはできません。そこで、確率的勾配降下法などの反復法を使って、重みを徐々に更新し、最適値を探します。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_update_w.png?raw=true\" alt=\"NN_update_w\" height=\"450px\">"]},{"cell_type":"markdown","metadata":{"id":"K0bRHgtx6g-K","colab_type":"text"},"source":["### 活性化関数に非線形関数を使うのはなぜか？\n","<small>*※ 読み飛ばしてOKです。*</small>"]},{"cell_type":"markdown","metadata":{"id":"3KAkPb2m7E_R","colab_type":"text"},"source":["　この答えは、活性化関数に線形関数（直線になる関数）を使うと、わざわざ隠れ層を作る意味がなくなるからです。\n","\n","　簡単な例をあげると、次のような3層のネットワークがあったとします（各層のユニット数は1個）。\n","- 第1層: $t_{1} = a_{1}x$\n","- 第1層の活性化関数: $y_{1} = b_{1}t_{1} $\n","- 第2層: $t_{2} = a_{2}y_{1}$\n","- 第2層の活性化関数: $y_{2} = b_{2}t_{2} $\n","- 第3層: $t_{3} = a_{3}y_{2}$\n","- 第3層の活性化関数: $y_{3} = b_{3}t_{3} $\n","\n","　この6つの式を1行で書くと、\n","$$y_3 = b_{3}(a_{3}(b_{2}(a_{2}(b_{1}(a_{1}x))))) = a_{1}a_{2}a_{3}b_{1}b_{2}b_{3}x$$\n","\n","$c = a_{1}a_{2}a_{3}b_{1}b_{2}b_{3}$とすると、\n","\n","$$ y_3 = c x $$\n","\n","となり、単層の式で表すことが可能になります。各層のユニット数が増えても同様です。\n","\n","　活性化関数に線形関数を使った場合、多層のニューラルネットワークを構築しても計算量が増えるだけで、まったくメリットはありません。そのため、活性化関数には非線形関数を使います。\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rfOF1-dRrF0g","colab_type":"text"},"source":["### 実装\n","\n","　scikit-learnを使ったニューラルネットワークの実装方法は次のとおりです。\n","\n","```python\n","# 書き方\n","from sklearn.neural_network import MLPClassifier\n","model_nn = MLPClassifier(ハイパーパラメータ)\n","model_nn.fit(トレーニングデータ)\n","```\n","\n","以下で使っているハイパーパラメータ（詳しくはscikit-learn [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)参照）:\n","\n","- `hidden_layer_sizes=(3,3,3,)`: （上図のように）3層各3モジュールを「隠れ層」として設置する\n","- `solver=\"sgd\"`: 重みの更新方法に「確率的勾配降下法」を使用する\n","- `learning_rate_init=0.1`: 重みの更新率を0.1に設定する\n","- `max_iter=1000`: 学習回数を1000回に設定する\n","\n","<small>*※ ニューラルネットワーク用のライブラリは、他にも多数あります（TensorFlow、Keras、Pytorch、Caffeなど）。より高度なニューラルネットワークを構築するには、それらのライブラリを使う方が良いでしょう。*</small>\n"]},{"cell_type":"code","metadata":{"id":"dP9aoNr2W_NI","colab_type":"code","colab":{}},"source":["# 使用するデータ\n","x_train, y_train = x_train_ss, y_train_le  # トレーニングデータ\n","x_test, y_test  = x_test_ss, y_test_le   # テストデータ\n","\n","# モデルを作成\n","from sklearn.neural_network import MLPClassifier\n","model_nn = MLPClassifier(hidden_layer_sizes=(3,3,3,), solver=\"sgd\", \n","                        learning_rate_init=0.1, max_iter=1000, random_state=0)\n","# モデルをトレーニング\n","model_nn.fit(x_train, y_train)\n","\n","# グラフ\n","from classification import draw_decision_boundary\n","draw_decision_boundary(x1_train=x_train[:,0], x2_train=x_train[:,1], \n","                       x1_test=x_test[:,0], x2_test=x_test[:,1], \n","                       y_train=y_train, y_test=y_test, \n","                       labels=le.inverse_transform(y_train), \n","                       model=model_nn, xlabel=\"x1\", ylabel=\"x2\")\n","\n","# モジュール間の各コネクションの重み\n","print(\"Coefficients=\", model_nn.coefs_)\n","\n","# 評価（正解率）\n","print(\"training data: \", model_nn.score(x_train, y_train)) # トレーニングデータ\n","print(\"test data: \",    model_nn.score(x_test, y_test))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9wg2BXIcmSUC"},"source":["## モデルの評価\n","\n","　二値分類（A / not A）の場合、実測値（実ラベル）とモデルから得られた予測値（予測ラベル）の一致/不一致を調べると、次の4つのタイプの結果が得られます。\n","\n","||予測ラベル=A|予測ラベル=not A|\n","|:---|:---|:---|\n","|**実ラベル=A**|真陽性<br>True positive: TP|偽陰性<br>False negative: FN|\n","|**実ラベル=not A**|偽陽性<br>False positive: FP|真陰性<br>True negative: TN|\n","\n","　よく使われる評価指標は、実ラベルと予測ラベルがどれだけ一致していたかを表す指標、**正解率（Accuracy）**です。\n","\n","$$Accuracy = \\frac{TP+TN}{TP + FN + FP + TN}$$\n","\n","　今回の実習では、各モデルの精度を正解率で評価していきました。再度、それぞれの正解率を確認しておきます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pnys3JLpmX-B","colab":{}},"source":["# ロジスティック回帰モデル\n","print(\"=== Logistica regression ===\")\n","print(\"training data: \", model_lr.score(x_train_ss, y_train_le)) # トレーニングデータ\n","print(\"test data: \",    model_lr.score(x_test_ss, y_test_le))  # テストデータ\n","\n","# サポートベクトルマシンモデル\n","print(\"=== Support vector machine ===\")\n","print(\"training data: \", model_svm.score(x_train_ss, y_train_le)) # トレーニングデータ\n","print(\"test data: \",    model_svm.score(x_test_ss, y_test_le))  # テストデータ\n","\n","# 決定木モデル\n","print(\"=== Decision tree ===\")\n","print(\"training data: \", model_dt.score(x_train_raw, y_train_le)) # トレーニングデータ\n","print(\"test data: \",    model_dt.score(x_test_raw, y_test_le))  # テストデータ\n","#print(\"training data: \", model_dt.score(x_train_ss, y_train_le)) # トレーニングデータ（スケーリング後）\n","#print(\"test data: \",    model_dt.score(x_test_ss, y_test_le))  # テストデータ（スケーリング後）\n","\n","# ランダムフォレストモデル\n","print(\"=== Decision tree ===\")\n","print(\"training data: \", model_rf.score(x_train_raw, y_train_le)) # トレーニングデータ\n","print(\"test data: \",    model_rf.score(x_test_raw, y_test_le))  # テストデータ\n","#print(\"training data: \", model_rf.score(x_train_ss, y_train_le)) # トレーニングデータ（スケーリング後）\n","#print(\"test data: \",    model_rf.score(x_test_ss, y_test_le))  # テストデータ（スケーリング後）\n","\n","# ニューラルネットワークモデル\n","print(\"=== Neural network ===\")\n","print(\"training data: \", model_nn.score(x_train_ss, y_train_le)) # トレーニングデータ\n","print(\"test data: \",    model_nn.score(x_test_ss, y_test_le))  # テストデータ"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q90BpMEsCfm7","colab_type":"text"},"source":["### その他の評価指標"]},{"cell_type":"markdown","metadata":{"id":"LRO7klaxCnAs","colab_type":"text"},"source":["　先ほどの表を下記に再度載せています。この行列は、**混同行列（Confusion matrix）**と呼ばれています。\n","\n","||予測ラベル=A<br>（ポジティブラベル）|予測ラベル=not A<br>（ネガティブラベル）|\n","|:---|:---|:---|\n","|**実ラベル=A<br>（ポジティブラベル）**|真陽性<br>True positive: TP|偽陰性<br>False negative: FN|\n","|**実ラベル=not A<br>（ネガティブラベル）**|偽陽性<br>False positive: FP|真陰性<br>True negative: TN|\n","\n","　正解率のほかにも、評価指標はあります。ここでは、おもなものリストアップします。\n","\n","- 精度 Precision  \n","ポジティブラベルと予測されたもののうち、実際にポジティブラベルであるサンプルの割合\n","$$Precision = \\frac{TP}{TP + FP}$$\n","\n","- False discovery rate (FDR)\n","ポジティブラベルと予測されたもののうち、誤って予測されたサンプルの割合。なお、ゲノム解析のひとつ、遺伝子発現解析でよく使われる指標です。\n","$$FDR = \\frac{FP}{TP + FP} = 1 - Precision$$ \n","\n","- 検出率 Recall / 真陽性率 True positive rate / 感度 Sensitivity  \n","ポジティブラベルのサンプルのうち、ポジティブラベルと予測された割合\n","$$Recall = \\frac{TP}{TP + FN}$$\n","\n","- 偽陰性率 False negative rate (FNR)  \n","ポジティブラベルのサンプルのうち、ネガティブラベルと予測された割合\n","$$FNR = \\frac{FP}{FP + TN} = 1 - Recall$$\n","\n","- 偽陽性率 False positive rate (FPR)  \n","ネガティブラベルのサンプルのうち、ポジティブラベルと予測された割合\n","$$FPR = \\frac{FP}{FP + TN}$$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cWkirafnOqK-","colab_type":"text"},"source":["　以下では、混同行列（Confusion matrix）を作るコードを紹介しています。また、精度（Precision）をscikit-learnを使って調べる方法も紹介しています。\n","\n","```python\n","# 混同行列\n","from sklearn.metrics import confusion_matrix\n","confusion_matrix(実ラベル、予測ラベル)\n","\n","# 精度\n","from sklearn.metrics import precision_score\n","precision_score(実ラベル、予測ラベル)\n","```\n","\n","　例として、ロジスティック回帰モデルからの予測ラベルを使っています。\n"]},{"cell_type":"code","metadata":{"id":"ZJYl4kHBCmV2","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix, precision_score\n","\n","# 実測値\n","y_train = y_train_le\n","y_test = y_test_le\n","\n","# ロジスティック回帰モデルからの予測値\n","y_train_pred = model_lr.predict(x_train_ss)\n","y_test_pred = model_lr.predict(x_test_ss)\n","\n","# トレーニングデータの混同行列\n","cm = confusion_matrix(y_train, y_train_pred)\n","pre = precision_score(y_train, y_train_pred, average=None)\n","print(\"=== training data ===\")\n","print(cm)\n","print(\"precision=\", pre)\n","\n","# テストデータの混同行列\n","cm = confusion_matrix(y_test, y_test_pred)\n","pre = precision_score(y_test, y_test_pred, average=None)\n","print(\"=== test data ===\")\n","print(cm)\n","print(\"precision=\", pre)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F3aTxJmKTtjq","colab_type":"text"},"source":["---\n","\n","## まとめ\n","\n","　今回、前処理、5つの分類アルゴリズムの概要とその実装方法、モデルの評価方法を勉強しました。\n","\n","　まず、前処理では、**ラベル（目的変数）の数値変換**をおこないました。\n","\n","　次に、5つの分類アルゴリズムを勉強しました。それぞれのアルゴリズムは、3行のコードで実装できます。\n","- **ロジスティック回帰**: あるクラスに分類される確率を求める手法\n","```python\n","from sklearn.ensemble import RandomForestClassifier\n","model_dt = RandomForestClassifier(ハイパーパラメータ)\n","model_dt.fit(トレーニングデータ)\n","```\n","\n","- **サポートベクトルマシン**: 最も近いデータまでの距離（マージン margin）が最大となる境界を作り、データを二分する方法\n","```python\n","from sklearn.svm import SVC\n","model_lr = SVC(ハイパーパラメータ)\n","model_lr.fit(トレーニングデータ)\n","```\n","\n","- **決定木**: 説明変数に対する問い（True or False）を繰り返して、木構造の分類モデルを作る方法\n","```python\n","from sklearn.tree import DecisionTreeClassifier\n","model_dt = DecisionTreeClassifier(ハイパーパラメータ)\n","model_dt.fit(トレーニングデータ)\n","```\n","\n","- **ランダムフォレスト**: 各決定木の予測結果を多数決して、最終的な予測値を決める手法\n","```python\n","from sklearn.ensemble import RandomForestClassifier\n","model_dt = RandomForestClassifier(ハイパーパラメータ)\n","model_dt.fit(トレーニングデータ)\n","```\n","\n","- **ニューラルネットワーク**: 神経細胞が信号を伝達する仕組みを模した機械学習アルゴリズム\n","```python\n","from sklearn.neural_network import MLPClassifier\n","model_nn = MLPClassifier(ハイパーパラメータ)\n","model_nn.fit(トレーニングデータ)\n","```\n","　最後に、ラベルの**正解率**でそれぞれのモデルの評価をおこないました。ほかにも評価指標はあり、実際の解析では、モデルを構築する目的に合わせて、評価指標を選ぶ必要があります。"]}]}