{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L08_machine_learning_1.ipynb","provenance":[],"collapsed_sections":["csp0UnLmtIzE","zsm5OPWyuXi-","NS1uQZ7psDWI","KgO8H5lOLm2n","iRLPUdXtUenk","itwAzqgET4O0","c_ajWotnP61f","d-k6VE5eAOVe","CnW9CaqPnmFY","R47c_LFtQ9xD","gw4sP5gIn-ZA","5Sb7dh-mIClg","vQfKAorvzCDr","-bAnZL1-Cb09","x-jpAVs1M7jP","-7WlxTCX8M6g"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e9fGSNj8QRzB","colab_type":"text"},"source":["<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/lec_title.png?raw=true\" alt=\"2019年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"1rqb6I1jO3Q7","colab_type":"text"},"source":["# 機械学習 - 線形回帰 -\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Rmsy1dStPHH1","colab_type":"text"},"source":["## 今回の実習内容\n","\n","1. 機械学習とは？\n","1. 教師あり学習 Supervised learning\n","1. 線形回帰 Linear regression（1変数）\n","1. 線形回帰 Linear regression（2変数以上）"]},{"cell_type":"markdown","metadata":{"id":"YpznGfs_aceP","colab_type":"text"},"source":["## 1. 機械学習とは？\n","\n","　**機械学習（Machine Learning）**は、過去のデータから特徴やパターンを発見し、数理モデルを構築することです。その目的は、**新たなデータに対して精確な予測をおこなうこと**にあります。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/machine_learning.png?raw=true\" alt=\"Machine Learning\" height=\"180px\">"]},{"cell_type":"markdown","metadata":{"id":"csp0UnLmtIzE","colab_type":"text"},"source":["### 例えば..."]},{"cell_type":"markdown","metadata":{"id":"EXDt3DAwtbjC","colab_type":"text"},"source":["#### 分類 Classification \n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/classification.png?raw=true\" alt=\"classification\" height=\"300px\">\n","\n","#### 回帰 Regression\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/regression.png?raw=true\" alt=\"regression\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"zsm5OPWyuXi-","colab_type":"text"},"source":["### 機械学習とデータマイニング"]},{"cell_type":"markdown","metadata":{"id":"V_K_n-1bv7zn","colab_type":"text"},"source":["　機械学習によく似た手法を用いるものとして、**データマイニング**（あるいは、統計モデリング）があります。データマイニングは、機械学習とは異なり、**データから特徴やパターンを発見すること**を目的に、データを解析します。\n","\n","　解析目的は違っているものの、機械学習とデータマイニングは、同じ手法を使うことが多いです。機械学習の勉強をすれば、データマイニングの勉強にもなります。このテキストでは、機械学習とデータマイニングをあまり区別せずに、「機械学習」として扱っています。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/data_mining.png?raw=true\" alt=\"data mininng\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"OwsbMnaPmHT_","colab_type":"text"},"source":["### 機械学習の種類\n","　大きく分けて3つあります。\n","\n","- **教師あり学習 Supervised learning**  \n","ラベル付きのデータを学習して、新しいデータに対する予測モデルを立てる。\n","\n","- **教師なし学習 Unsupervised learning**  \n","ラベルのないデータを学習して、クラスタリングや次元削減をおこなう。\n","\n","- **強化学習 Reinforcement learning**  \n","試行とその結果のフィードバックを繰り返して、性能を良くしていく。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/machine_learning_types.png?raw=true\" alt=\"Machine Learning 3-Types\" height=\"250px\">\n","\n","　このゲノム情報解析入門では、「教師あり学習」と「教師なし学習」を勉強します。「強化学習」については取り上げません。\n"]},{"cell_type":"markdown","metadata":{"id":"hFVvJVJKFGdr","colab_type":"text"},"source":["## 2. 教師あり学習 Supervised learning\n","　**ラベル（出力値）**によって、大きく **分類**と**回帰** に分かれます。使われる技法もそれぞれ異なります。\n","\n","- 分類 Classification  \n","ラベルが離散値データ（種Aか種Bか、スパムメールかそうでないか、など）の場合の学習。  \n","  - ロジスティック回帰\n","  - サポートベクトルマシン\n","  - 決定木\n","  - ランダムフォレスト\n","  - ニューラルネットワーク　など\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/classification2.png?raw=true\" alt=\"classification\" height=\"160px\">\n","\n","- 回帰 Regression  \n","ラベルが連続値データ（収穫量や草丈など）の場合の学習。\n","  - 線形回帰\n","  - サポートベクトル回帰\n","  - 回帰木\n","  - ランダムフォレスト回帰\n","  - ニューラルネットワーク　など\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/regression2.png?raw=true\" alt=\"regression\" height=\"180px\">\n","\n","　ここでは、「回帰」の技法の基礎とそのプログラミングをおこなっていきます。\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"3qVOouQseCpk","colab_type":"text"},"source":["### 「教師あり学習」の手順\n","\n","1. 前処理: **トレーニングデータ（training data）** と **テストデータ（test data）** を準備する。\n","2. 選択: 使用するモデルや設定値（**ハイパーパラメータ**）を選択する。\n","3. 学習: トレーニングデータを使って、モデルを学習させて、予測モデルを構築する。\n","4. 評価: テストデータを使って、学習済みの予測モデルを評価する。\n","  - 結果が良くなければ、2へ戻る。\n","  - 結果が良ければ、5へ進む。\n","5. 予測・解釈: 予測モデルを新しいデータに適用する。または、予測モデルを解釈する。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/supervised_learning_process.png?raw=true\" alt=\"supervised_learning_process\" height=\"60px\">\n","\n","　今回の実習では、この機械学習の流れを学んでいきます。"]},{"cell_type":"markdown","metadata":{"id":"NS1uQZ7psDWI","colab_type":"text"},"source":["### 機械学習ライブラリ scikit-learn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CLEnCaXRvLcz","colab_type":"text"},"source":["\n","　Pythonには、機械学習のためのライブラリ [**scikit-learn**](https://scikit-learn.org/stable/)（sklearn）があります。このライブラリのおかげで、機械学習内部のアルゴリズムをコーディングせずに、各種の機械学習をおこなえます。後々、次のような3行のコードを頻繁に使うことになります。\n","\n","```python\n","from sklearn.機能 import 関数\n","変数 = 関数（）\n","変数.fit()\n","```"]},{"cell_type":"markdown","metadata":{"id":"aOaYdFcfbFMX","colab_type":"text"},"source":["### 実習で使用するデータセット\n","\n","　次のコードセルを実行して、データファイル（[gene_expression.csv](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/gene_expression.csv)）をダウンロードしてください。\n","\n","ファイルの詳細:\n","- ファイル名: gene_expression.csv\n","- カンマ区切りテキストファイル\n","- 100行（100サンプル） x 51列（表現型値+50個の遺伝子発現量）\n"]},{"cell_type":"code","metadata":{"id":"xKU2jUPHbXDV","colab_type":"code","colab":{}},"source":["### このコードセルは実行のみ ###\n","# サンプルデータのダウンロード\n","!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/gene_expression.csv\n","\n","# pandasで読み込み\n","import pandas as pd\n","df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n","df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9WLjff4LZkp","colab_type":"text"},"source":["## 3. 線形回帰 Linear regression（1変数）\n","\n","　ここでおこなう解析の目標は次のとおりです。\n","> 遺伝子発現量から表現型値を予測する線形回帰モデルを作る\n","\n","　「線形回帰モデル」とは何か？　遺伝子発現量（$x$）と表現型値（$y$）の関係を次の方程式で表したモデルのことです。\n","\n","\n","用語説明: \n","- **目的変数（objective variable）**: 予測される変数$y$（今回の場合、表現型値）。\n","- **説明変数（explanatory variable）**: 予測に使う変数$x$（各遺伝子発現量）。\n","- **偏回帰係数（coefficient）**: 各説明変数の重み。目的変数の予測にその変数がどれぐらい影響するかを示す指標。\n","- **誤差（intercept; 切片）**: 説明変数以外の影響を示す項。\n","\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/09_statistics/data/regression_base.png?raw=true\" alt=\"regression\" height=\"130px\">\n","\n","　"]},{"cell_type":"markdown","metadata":{"id":"RzhpxlGdo6YG","colab_type":"text"},"source":["### 3-1. 前処理\n","\n","　機械学習で最初におこなうことは、**トレーニングデータ**と**テストデータ**の準備です。データセットを分割して、これらを準備します。\n","\n","<small>_※ 例では、説明変数「gene_11の遺伝子発現量 `gene_11`」のみを使って、目的変数「表現型値 `phenotype`」を予測する線形回帰モデルを作っていきます。_</small>\n"]},{"cell_type":"code","metadata":{"id":"oVyIioYKpqhJ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# 使用する変数\n","x = np.array(df[\"gene_11\"])    # 説明変数（ここでは1つのみ）\n","y = np.array(df[\"phenotype\"]) # 目的変数\n","\n","# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n","# test_sizeで分割比率を制御する。\n","# random_stateの値を設定すると、分割データの選ばれ方が固定される。\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n","\n","# 各データのサイズ\n","print(\"training: \", x_train.shape, y_train.shape)\n","print(\"test: \", x_test.shape, y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5bs0kRCexZ2_","colab_type":"text"},"source":["　散布図を書いて、データを眺めてみましょう。"]},{"cell_type":"code","metadata":{"id":"Reg98YRRxlHb","colab_type":"code","colab":{}},"source":["# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(x_train, y_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(x_test, y_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgO8H5lOLm2n","colab_type":"text"},"source":["### データセットの分割の比率"]},{"cell_type":"markdown","metadata":{"id":"fLWo3L3bLtbk","colab_type":"text"},"source":["　分割の比率をどのように設定するかについては、厳密に決まっていません。一概には言えませんが、トレーニングデータが多くすれば、より多くの新しいデータに対応できるモデルを構築できるようになります。一方で、予測モデルの評価用のテストデータが少なくなり、評価精度が落ちます。\n","\n","　サンプル数100~10,000程度のデータセットでは、トレーニングデータ : テストデータを「70% : 30%」や、「75% : 25%」、「80% : 20%」の比率で分割するのが一般的です。サンプル数100,000では「90% : 10%」、1,000,000では「99% : 1%」といった比率が一般的なようです。ただし、説明変数の個数なども考慮する必要があるので注意が必要です。"]},{"cell_type":"markdown","metadata":{"id":"LW-g1fEoFemm","colab_type":"text"},"source":["### 実習1\n","\n","<small>_※ コード例では説明変数として「gene_11の遺伝子発現量」を使っていますが、実習問題では「gene_29の遺伝子発現量」を使います。同じようなコードを書くため、変数が上書きされないように注意してください。_</small>\n","\n","　100サンプル分のデータを分割して、トレーニングデータとテストデータを作成してください。分割比率は自由に設定してください。\n","- 説明変数$x$: gene_29の遺伝子発現量 `gene_29`\n","- 目的変数$y$: 表現型値 `phenotype`\n"]},{"cell_type":"code","metadata":{"id":"l93zFzawWh0E","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# 使用する変数\n","xx = np.array(df[\"gene_29\"])    # 説明変数（ここでは1つのみ）\n","yy = np.array(df[\"phenotype\"])   # 目的変数\n","\n","# ============== 編集エリア(start) =============\n","# データ分割\n","from sklearn.model_selection import train_test_split\n","xx_train, xx_test, yy_train, yy_test = \n","\n","# ============== 編集エリア(end) ==============\n","\n","# 各データのサイズ\n","print(\"training: \", xx_train.shape, yy_train.shape)\n","print(\"test: \", xx_test.shape, yy_test.shape)\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(xx_test, yy_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRLPUdXtUenk","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"Jmd7uvU0UYWZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# 使用する変数\n","xx = np.array(df[\"gene_29\"])    # 説明変数（ここでは1つのみ）\n","yy = np.array(df[\"phenotype\"])   # 目的変数\n","\n","# ============== 編集エリア(start) =============\n","# データ分割\n","from sklearn.model_selection import train_test_split\n","xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.2, random_state=0)\n","\n","# ============== 編集エリア(end) ==============\n","\n","# 各データのサイズ\n","print(\"training: \", xx_train.shape, yy_train.shape)\n","print(\"test: \", xx_test.shape, yy_test.shape)\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(xx_test, yy_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOtHFnMVJWFg","colab_type":"text"},"source":["### 3-2. モデルの選択\n","\n","　モデルの選択肢は、「線形回帰」以外にもいくつかあります。ここでは、この節のタイトル通りに「線形回帰」を使いましょう。\n","\n","　次の「モデルの学習」に進む前に、線形回帰予測モデルの作り方を漠然と理解しておきましょう。予測モデルを作るのに使うのは、トレーニングデータ（散布図の青色の点）のみです。テストデータ（オレンジ色の点）はまだ使いません。\n","\n","　x-y座標上には、直線を無数に作成できます。これから作る予測モデルは、それら無数の直線のうち、トレーニングデータ（青色の点）の説明変数$x$ と目的変数$y$ の関係をよく表した直線です。次の一次方程式の係数（傾き）$b$ と誤差（切片）$e$ の最適な値を見つけることと同じ意味合いです。\n","\n","  $$ y = bx + e $$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SFNqonlG9bTI","colab_type":"text"},"source":["### 3-3. モデルの学習\n","\n","　それでは、トレーニングデータを使って学習し、予測モデルを作ってみましょう。ここでおこなう手順は、次のとおりです。\n","\n","1. 線形回帰モデルを使う場合、scikit-learnの`linear_model.LinearRegression`を呼び出します。下の例では、変数`model`に学習前のモデルを用意しています。その後、`model.やりたいこと` といったコードを書いて、モデルの学習や情報の抽出、予測モデルの評価、新しいデータの予測をおこなっていきます。\n","```python\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","```\n","\n","2. トレーニングデータを使って学習（モデルのフィッティング）をおこない、 $y = bx + e$ の傾き$b$ と 切片$e$を求めます。\n","```python\n","model.fit(x_train, y_train)\n","```\n","\n","3. （必要に応じて、）学習の結果、傾き$b$ と 切片$e$を確認します。"]},{"cell_type":"code","metadata":{"id":"q0AbO-WKMn0T","colab_type":"code","colab":{}},"source":["# モデルを選択 => 線形モデルを使う\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","\n","# モデルを学習 => トレーニングデータを使って、係数と誤差の最適値を見つける\n","x_train = x_train.reshape(-1, 1)  # 整形（一次元配列の場合に必要）\n","model.fit(x_train, y_train)\n","\n","# 傾きb、切片e\n","b = model.coef_[0]\n","e = model.intercept_\n","\n","# 表示\n","print(\"Coefficient=\", b) # 傾きb\n","print(\"Intercept=\"  , e) # 切片e"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7rDcCzTfOQU","colab_type":"text"},"source":["　散布図に予測モデルの直線 $y=bx+e$ を追加しましょう。"]},{"cell_type":"code","metadata":{"id":"AJOsfbgc-sPy","colab_type":"code","colab":{}},"source":["# トレーニングデータの各xについて、予測値（直線上のyの値）を得る\n","y_pred = model.predict(x_train)\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(x_train, y_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 予測モデルの直線\n","plt.plot(x_train, y_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itwAzqgET4O0","colab_type":"text"},"source":["### 線形回帰で得られる直線\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OJYeev8hT7Dl","colab_type":"text"},"source":["　線形回帰をおこなうと、すべての点について、予測値（直線）と実測値（点）のずれ「yの値の差分」を調べて、その差分の二乗の合計値、**残差平方和 residual sum of squares** が最も小さくなる直線が得られます。\n","\n","$$ 予測モデル: \\hat{y}_i = bx_i + e $$\n","\n","$$ 残差平方和: \\frac{1}{2m}\\sum_{i=1}^{m} (\\hat{y}_{i} - y_{i})^2 = \\frac{1}{2m}\\sum_{i=1}^{m} (bx_i + e - y_i)^2 $$ \n","\n","$$ サンプル: i=1,2,3,...,m $$\n","\n","この式の $\\hat{y}_{i}$ は予測モデル（予測されたyの値）、$x_i$や$y_i$ は実測値です。\n","\n","　上の式のように、最小化を目指すような関数のことを、**目的関数 objective function** や **コスト関数 cost function** と呼ぶことが多いです。\n","\n","$$ 目的関数: J(θ) = \\frac{1}{2m}\\sum_{i=1}^{m} (\\hat{y}_{i} - y_{i})^2 = \\frac{1}{2m}\\sum_{i=1}^{m} (bx_i + e - y_i)^2 $$ \n","\n","$$ サンプル: i=1,2,3,...,m $$\n","\n","　この予測モデルの直線は、同じ母集団から得られた新しいデータに対して、そのxの値からyの値を予測するために使えます。例えば、下の図の予測モデルの場合、新しいデータがx=4であった場合、予測モデルの式に代入して、y=約5.8の予測値を得ることができます。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/fitting_line.png?raw=true\" alt=\"fitting_line\" height=\"200px\">\n","\n","_新しいデータの予測値を得る実習は、「3-5. 予測」でおこないます。_\n"]},{"cell_type":"markdown","metadata":{"id":"c_ajWotnP61f","colab_type":"text"},"source":["### 傾き$a$ と 切片$b$ の値をどうやって求めているか？"]},{"cell_type":"markdown","metadata":{"id":"YCv6TYh-UBNj","colab_type":"text"},"source":["　scikit-learnの `LinearRegression()` は、**最小二乗法 least squares method**と呼ばれる方法を使って、残差平方和の最小値（極小値）を調べて、予測モデルの係数$b$ と誤差$e$ を求めています。最小二乗法の概要は次のとおりです。\n","\n","1. 目的関数を最小化したい。\n","1. 目的関数の$\\sum$の部分を展開すると、\n","$$ \\sum_{i=1}^{m} (bx_i + e - y_i)^2 = b^{2}\\sum_{i=1}^{m}{x_{i}^{2}} + me^2 + \\sum_{i=1}^{m}{y_{i}^{2}} - 2b\\sum_{i=1}^{m}{x_{i}y_{i}} + 2be\\sum_{i=1}^{m}{x_{i}} -2e\\sum_{i=1}^{m}{y_{i}} $$\n","1. 上の展開式を$b$の関数としてみると、下に凸な関数であることがわかります。$e$の関数としてみても同様に下に凸な関数で、最小値=極小値となりそうなのがわかります。\n","1. そこで、$b$や$e$についての偏微分関数が0になる値を調べます。\n"," $$ (bについて偏微分)=0 $$\n"," $$ (eについて偏微分)=0 $$\n","1. 上の2つの方程式を解くと、目的関数が最小値（極小値）となる$b$や$e$の値が求まります\n","\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/least_squares_method.png?raw=true\" alt=\"least squares method\" height=\"180px\">\n"]},{"cell_type":"markdown","metadata":{"id":"d-k6VE5eAOVe","colab_type":"text"},"source":["### scikit-learnでよくおこなう3行\n"]},{"cell_type":"markdown","metadata":{"id":"jjiQltD1CTaV","colab_type":"text"},"source":["　scikit-learnの基本構文は、この3行です。\n","\n","```python\n","from sklearn.機能 import 関数 \n","モデル変数 = 関数(オプション)\n","モデル変数.fit(x, y)\n","```\n","\n","　上でおこなった「線形回帰」だけでなく、他の技法（「決定木」や「ランダムフォレスト」、「サポートベクトルマシン」を使った回帰、など）も3行でおこなえます。\n","```python\n","# 線形回帰\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","model.fit(x, y)\n","\n","# 決定木を使った回帰（回帰木）\n","from sklearn.tree import DecisionTreeRegressor\n","model = DecisionTreeRegressor()\n","model.fit(x, y)\n","\n","# ランダムフォレストを使った回帰\n","from sklearn.tree import DecisionTreeRegressor\n","model = DecisionTreeRegressor()\n","model.fit(x, y)\n","\n","# サポートベクトルマシンを使った回帰（サポートベクトル回帰）\n","from sklearn.svm import SVR\n","model = SVR()\n","model.fit(x, y)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"dS3vxjdp28Zn","colab_type":"text"},"source":["### 実習2\n","<small>_※ 実習1の続きです。_</small>\n","\n","<small>_※ コード例では説明変数として「gene_11の遺伝子発現量」を使っていますが、実習問題では「gene_29の遺伝子発現量」を使います。同じようなコードを書くため、変数が上書きされないように注意してください。_</small>\n","\n","　実習1で得られたトレーニングデータを使って、説明変数$x$から目的変数$y$を予測する線形回帰モデルを作ってください。\n","- 説明変数$x$: gene_29の遺伝子発現量 `gene_29`　=> `xx_train`や`xx_test`に代入されています\n","- 目的変数$y$: 表現型値 `phenotype`　=> `yy_train`や`yy_test`に代入されています"]},{"cell_type":"code","metadata":{"id":"nAQVqmdNnnVa","colab_type":"code","colab":{}},"source":["# 整形（一次元配列の場合に必要）\n","xx_train = xx_train.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","from sklearn.linear_model import LinearRegression\n","\n","# 学習前のモデルを準備する\n","model2 = \n","\n","# トレーニングデータでモデルを学習させる（モデルフィッティング）\n","\n","\n","# ============== 編集エリア(end) ==============\n","\n","# 傾きbと切片eを表示\n","print(\"Coefficient=\", model2.coef_[0])   # 傾きb\n","print(\"Intercept=\"  , model2.intercept_) # 切片e\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 予測モデルの直線\n","yy_pred = model2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CnW9CaqPnmFY","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"PXrKLx_V-Gx_","colab_type":"code","colab":{}},"source":["# 整形（一次元配列の場合に必要）\n","xx_train = xx_train.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","from sklearn.linear_model import LinearRegression\n","\n","# 学習前のモデルを準備する\n","model2 = LinearRegression()\n","\n","# トレーニングデータでモデルを学習させる（モデルフィッティング）\n","model2.fit(xx_train, yy_train)\n","\n","# ============== 編集エリア(end) ==============\n","\n","# 傾きbと切片eを表示\n","print(\"Coefficient=\", model2.coef_[0])   # 傾きb\n","print(\"Intercept=\"  , model2.intercept_) # 切片e\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 予測モデルの直線\n","yy_pred = model2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend()                # 凡例"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qvn3oSE5Ht2t","colab_type":"text"},"source":["### 3-3. モデルの評価\n","\n","　次に、トレーニングデータの学習で得られたモデルの予測精度を評価する方法についてです。\n","\n","　回帰モデルの評価はいくつかあります。ここでは、**決定係数 $R^2$ (R-squared, coefficient of determination)** と呼ばれる評価指標を用いて、学習で得られたモデルを評価してみましょう。\n","\n","$R^{2}$について:\n","- 予測モデルの当てはまりの良さを示す指標。\n","- 0に近づくほど当てはまりが悪く、1に近づくほど当てはまりが良い。予測精度があまりに悪いと、マイナスの値をとることもある。\n","- $R^2$の式は下記のとおり。\n","  - $y_{i}$: $i$番目のサンプルの実測値\n","  - $\\bar{y}_{i}$: 実測値の平均\n","  - $\\hat{y}_{i}$: $i$番目のサンプルの予測値\n","  - $m$: サンプル数\n","\n","  $$ R^{2} = 1 - \\frac{\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})^2}}{\\sum_{i=1}^{m}{(\\bar{y} - y_{i})^2}} $$\n","\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"tU4wx9bjVhhM","colab_type":"text"},"source":["　トレーニングデータとテストデータ両方の$R^2$を調べてみましょう。\n","\n","```python\n","# トレーニングデータのR2\n","model.score(x_train, y_train)\n","# テストデータのR2\n","model.score(x_test, y_test)\n","```"]},{"cell_type":"code","metadata":{"id":"4GW7Z5UWVfhc","colab_type":"code","colab":{}},"source":["# 整形: テストデータのx\n","x_test = x_test.reshape(-1, 1)\n","\n","# 決定係数R2\n","r2_train = model.score(x_train, y_train)\n","r2_test = model.score(x_test, y_test)\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cO3Jh_vBXT9_","colab_type":"text"},"source":["　トレーニングデータ、テストデータともに決定係数が低く、この予測モデルの当てはまりは悪いことがわかります。"]},{"cell_type":"markdown","metadata":{"id":"R47c_LFtQ9xD","colab_type":"text"},"source":["### 決定係数$R^{2}$のもう少し詳しい説明"]},{"cell_type":"markdown","metadata":{"id":"-bcfWZU-RDpr","colab_type":"text"},"source":["  $$決定係数 R^{2} = 1 - \\frac{\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})^2}}{\\sum_{i=1}^{m}{(\\bar{y} - y_{i})^2}} $$\n","\n","　$R^{2}$の分数の部分の説明です。分母は「サンプルの分散（ばらつき）」を表しています。分子は「残差平方和（実測値と予測値がどれだけ離れているかの指標）」です。\n","\n","　分数の値が1に近いと（$R^{2}$が0に近いと）、サンプルのばらつきと残差平方和はほぼ同じで、各点は予測モデルから大きく離れていることになります。一方で、分数の値が0に近いと（$R^{2}$が1に近いと）、各点は予測モデルによくフィットしていることになります。"]},{"cell_type":"markdown","metadata":{"id":"gw4sP5gIn-ZA","colab_type":"text"},"source":["### 決定係数$R^{2}$は高ければ良いのか？"]},{"cell_type":"markdown","metadata":{"id":"f989FhcOfWnF","colab_type":"text"},"source":["　決定係数$R^{2}$は、データの「当てはまりの良さ」を表す指標です。予測モデル構築の目的が「新しいデータのyの値を予測したい」場合、トレーニングデータとテストデータの両方で決定係数が高いことが重要です。\n","\n","　例えば、下図のように、シンプルなモデル（赤線）と複雑なモデル（青線）があったとします。複雑なモデルの方は、トレーニングデータに対する決定係数は非常に高いですが、新しいデータに対する決定係数は低く、予測モデルとして役立ちません。こういう状態を **過学習（オーバーフィッティング overfitting）** と呼びます。一方、シンプルなモデルは、トレーニングデータやテストデータに対する決定係数はそこそこですが、複雑なモデルよりは現実的な予測モデルとして使えるでしょう。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/10_statistics/data/overfit.png?raw=true\" alt=\"overfit\" height=\"300px\">\n","\n","　また、データマイニング的な目的「yの値に影響を持っている変数xを調べたい」であれば、決定係数といった指標ですべて評価することは必ずしも適切ではありません。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9EALrfzhHxAk","colab_type":"text"},"source":["### 実習3\n","\n","<small>_※ 実習2の続きです。_</small>\n","\n","<small>_※ コード例では説明変数として「gene_11の遺伝子発現量」を使っていますが、実習問題では「gene_29の遺伝子発現量」を使います。同じようなコードを書くため、変数が上書きされないように注意してください。_</small>\n","\n","　実習2で得られた予測モデルを決定係数$R^2$で評価してみましょう。トレーニングデータとテストデータの両方の$R^2$を求めてください。\n","- 予測モデル: `model2`に学習済みのモデルが入っています\n","- トレーニングデータ: `xx_train`や`yy_train`に代入されています\n","- テストデータ: `xx_test`や`yy_test`に代入されています"]},{"cell_type":"code","metadata":{"id":"QV2qAcWkHumU","colab_type":"code","colab":{}},"source":["# ============== 編集エリア(start) =============\n","# 整形: テストデータのx\n","xx_test = xx_test.reshape(-1, 1)\n","\n","# 決定係数R2\n","r2_train = \n","r2_test = \n","# ============== 編集エリア(end) ==============\n","\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Sb7dh-mIClg","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"pIvRBcDbIKWx","colab_type":"code","colab":{}},"source":["# ============== 編集エリア(start) =============\n","# 整形: テストデータのx\n","xx_test = xx_test.reshape(-1, 1)\n","\n","# 決定係数R2\n","r2_train = model2.score(xx_train, yy_train)\n","r2_test = model2.score(xx_test, yy_test)\n","# ============== 編集エリア(end) ==============\n","\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3oyGyC-GHbf4","colab_type":"text"},"source":["### 3-4. 予測\n","\n","　新しいデータの予測値を調べる前に、説明変数を増やして、モデルを改善させましょう。次の「4. 線形回帰 Linear regression（2変数以上）」に進んでください。"]},{"cell_type":"markdown","metadata":{"id":"vOXFDY5zlYCs","colab_type":"text"},"source":["..."]},{"cell_type":"markdown","metadata":{"id":"jEg9g2e0UYch","colab_type":"text"},"source":["## 4. 線形回帰 Linear regression（2変数以上）\n","\n","　ここまでは、1つの説明変数xから目的変数yを予測する線形回帰モデルを作ってきました。さらに説明変数を増やして、予測モデルを改善してみましょう。ここからは、説明変数が4つの線形回帰をおこないます。\n","\n","$$ f(x) = w_1x_1 + w_2x_2 + w_3x_3 + b $$\n","\n","<small>_※ ここで使う変数には、予測モデルが改善するものを選んでます。_</small>"]},{"cell_type":"markdown","metadata":{"id":"KwH2Nub0sN5O","colab_type":"text"},"source":["### 4-1. 前処理\n","\n","　データ分割方法は、説明変数が増えても、同じです。"]},{"cell_type":"code","metadata":{"id":"O-BQtibXe6NZ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# 使用する変数\n","x = np.array(df.loc[:,[\"gene_7\", \"gene_11\", \"gene_29\"]]) # 説明変数3つ\n","y = np.array(df[\"phenotype\"])                              # 目的変数\n","\n","# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n","\n","print(\"training: \", x_train.shape, y_train.shape)\n","print(\"test: \", x_test.shape, y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7aHHC-XrAKG","colab_type":"text"},"source":["　様々な説明変数（身長や体重など）をモデルに組み込もうとすると、変数の単位が揃わないことがしばしばあります。そのまま、モデルの学習をおこなうと、学習効率や予測モデルの悪影響を及ぼします。そこで、モデル学習の前に、**スケーリング（scaling）**をおこない、各説明変数の尺度を揃える必要があります。\n","\n","　スケーリングの際、まず、トレーニングデータでスケーリングをおこないます。次に、同じ基準を使って、テストデータをスケーリングします。\n","\n","スケーリングの種類: \n","- **正規化 normalization**: 各変数の値を0 ~ 1の範囲に変換する  \n","```python\n","from sklearn.preprocessing import MinMaxScaler\n","mms = MinMaxScaler()  # 変数名「mms」の部分は任意\n","x_train_mms = mms.fit_transform(x_train)\n","x_test_mms = mms.transform(x_test)\n","```\n","\n","- **標準化 standardization**: 各変数の値を平均値0、標準偏差1になるように変換する\n","```python\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler() # 変数名「ss」の部分は任意\n","x_train_ss = ss.fit_transform(x_train)\n","x_test_ss = ss.transform(x_test)\n","```\n","\n","　ここでは、標準化の方のスケーリングをおこなってみましょう。"]},{"cell_type":"code","metadata":{"id":"CLPw_MejxlKJ","colab_type":"code","colab":{}},"source":["# 標準化\n","from sklearn.preprocessing import StandardScaler\n","ss = StandardScaler()\n","x_train_ss = ss.fit_transform(x_train)\n","x_test_ss = ss.transform(x_test)\n","\n","# 確認\n","#x_train_ss\n","#x_test_ss"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MlBn1mAEyIFs","colab_type":"text"},"source":["### 実習4\n","\n","　上の例と同じデータセット（`x_train`、`x_test`）を正規化をしてください。"]},{"cell_type":"code","metadata":{"id":"llK6nyvrylWf","colab_type":"code","colab":{}},"source":["# ============== 編集エリア(start) =============\n","# 正規化\n","from sklearn.preprocessing import MinMaxScaler\n","mms = \n","x_train_mms = \n","x_train_mms = \n","# ============== 編集エリア(end) ==============\n","\n","# 確認\n","x_train_mms\n","#x_test_mms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vQfKAorvzCDr","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"bQ5nkW8RzDxV","colab_type":"code","colab":{}},"source":["# ============== 編集エリア(start) =============\n","# 正規化\n","from sklearn.preprocessing import MinMaxScaler\n","mms = MinMaxScaler()\n","x_train_mms = mms.fit_transform(x_train)\n","x_test_mms = mms.transform(x_test)\n","# ============== 編集エリア(end) ==============\n","\n","# 確認\n","x_train_mms\n","#x_test_mms"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BewlyUXtviu-","colab_type":"text"},"source":["### 4-2 ~ 4-3. モデルの選択・学習\n","\n","　ここでも「線形回帰」をおこないます。スケーリング後のトレーニングデータを学習に使います。"]},{"cell_type":"code","metadata":{"id":"1ICaAnSBi6N9","colab_type":"code","colab":{}},"source":["# scikit-learnの線形モデルを準備\n","from sklearn.linear_model import LinearRegression\n","# モデル選択＆学習\n","model_ss = LinearRegression()\n","model_ss.fit(x_train_ss, y_train)\n","\n","# 傾きw、切片b\n","w = model_ss.coef_\n","b = model_ss.intercept_\n","print(\"Coefficient=\", w)\n","print(\"Intercept=\"  , b)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-bAnZL1-Cb09","colab_type":"text"},"source":["### 各項の係数 Coefficient について"]},{"cell_type":"markdown","metadata":{"id":"_rR-WxRnCrQ8","colab_type":"text"},"source":["　各項の係数 Coefficient は、それぞれの説明変数$w_1, w_2...w_m$が目的変数$y$にどの程度影響を持っているかを表しています。影響の大きさを知りたいのであれば、係数の絶対値をみます。影響の方向性（プラスかマイナスか）を知りたいのであれば、係数の符号をみます。\n","\n","\n","　詳細は省きますが、前処理でおこなったスケーリングはこの係数に影響してきます。モデルの各係数の値を調べたい場合（例えば、各遺伝子の発現が表現型値にどの程度関わっているか調べたい場合）、スケーリングをせずに解析すると、誤った結果を得ることになります。"]},{"cell_type":"markdown","metadata":{"id":"ns7-YTWL8A7Z","colab_type":"text"},"source":["### 4-4. モデルの評価\n","\n","　決定係数$R^2$を調べると、説明変数が1つの場合よりも$R^2$が高くなることがわかります。\n","\n","\n","```\n","# 参考: 説明変数が1つ（gene_11）のみを使った場合のR2\n","training:  0.28006904968191343\n","test:  0.3765405659481891\n","```"]},{"cell_type":"code","metadata":{"id":"pKoQUHTq4QZc","colab_type":"code","colab":{}},"source":["# 決定係数R2: 4つの説明変数を使ったとき\n","r2_train_ss = model_ss.score(x_train_ss, y_train)  # トレーニングデータ\n","r2_test_ss = model_ss.score(x_test_ss, y_test)    # テストデータ\n","print(\"training: \", r2_train_ss)\n","print(\"test: \"    , r2_test_ss)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-jpAVs1M7jP","colab_type":"text"},"source":["### モデル比較"]},{"cell_type":"markdown","metadata":{"id":"w-opZ32NM8aw","colab_type":"text"},"source":["　ここでは、モデル間の比較で重要な考え方を紹介します。\n","\n","```\n","精度が高いモデルが良い\n","複雑なモデルより、単純なモデルが良い\n","```\n","\n","　機械学習では、新しいデータに対して上手く予測できるモデル、すなわち「精度が高い」モデルが好まれます（実習で使った決定係数$R^2$はモデルの精度を測る指標です）。また、「単純なモデル」は、理解しやすいので好まれます。\n","\n","　一般的に、モデルの精度を高くしようとすると、複雑になる傾向にあります。しかし、単純さを優先すると、精度が低いモデルになります。\n","\n","　精度と単純さのバランスを評価する指標として、次のような指標があります。これらは、モデル間の比較に使われます。詳しく知りたい方は、統計モデリングの本を参照してください。\n","- AIC(赤池情報量基準): 予測能力が最良のモデルを良いとする指標\n","- BIC(ベイズ情報量規準): 真のモデルである確率が最も大きいモデルを良いとする指標\n","\n","\n","「統計モデリング」の勉強におすすめの本: \n","- [データ解析のための統計モデリング入門](https://www.iwanami.co.jp/book/b257893.html)\n","- [Bayesian Analysis with Python](https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition)（原書）/ [Pythonによるベイズ統計モデリング](https://www.kyoritsu-pub.co.jp/bookdetail/9784320113374)（日本語訳）"]},{"cell_type":"markdown","metadata":{"id":"N2FGjVcn8D4f","colab_type":"text"},"source":["### 4-5. 予測\n","\n","　（改善の余地はありますが）そこそこ良いモデルが得られたので、新しい入力データ（$x$の値）をこの予測モデルに入れて、予測値（$y$の値）を得てみましょう。\n","\n","　ここでは、新しく3サンプルの遺伝子発現データが得られたとして、その表現型値を予測します。\n","\n","$$\n","(gene_{7}, gene_{11}, gene_{29}) = (x_1, x_2, x_3) = (10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)\n","$$\n","\n","予測の手順:  \n","1. 新しいデータのスケーリングをします。トレーニングデータと同じ基準で、スケーリングする必要があります（テストデータのスケーリングと同じ操作です）。\n","```python\n","スケーリング名.transform(新しいデータ)\n","```\n","\n","1. `predict`で新しいデータの予測値を得ます。\n","```python\n","モデル名.predict(スケーリング後のデータ)\n","```"]},{"cell_type":"code","metadata":{"id":"o4fPHC7p8Gqn","colab_type":"code","colab":{}},"source":["# 新しいデータ\n","new_data = np.array([(10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)])\n","\n","# スケーリング（標準化 standardization）\n","new_data = ss.transform(new_data)\n","\n","# 予測\n","model_ss.predict(new_data)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bgSSd6c28J9i","colab_type":"text"},"source":["### 実習4\n","\n","　次のコードセルの1行目の`new_data`には、5つの新しいデータが入っています。これまでの実習で作ってきたモデル`model_ss`（標準化トレーニングデータで学習した線形回帰モデル）を使って、それらの予測値を得てください。"]},{"cell_type":"code","metadata":{"id":"sA9CTnCfUEw_","colab_type":"code","colab":{}},"source":["new_data = np.array([(9.4, 7.4, 10.3), (10.0, 9.5, 10.2),\n","                     (8.6, 6.8, 9.1), (9.4, 6.4, 13.0), (9.9, 7.7, 10.0)])\n","\n","# ============== 編集エリア(start) =============\n","# スケーリング（標準化 standardization）\n","new_data = ss.\n","\n","# 予測\n","\n","\n","# ============== 編集エリア(end) =============="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-7WlxTCX8M6g","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"1-EK7zGz8MLu","colab_type":"code","colab":{}},"source":["new_data = np.array([(9.4, 7.4, 10.3), (10.0, 9.5, 10.2),\n","                     (8.6, 6.8, 9.1), (8.4, 10.4, 13.0), (7.9, 7.7, 10.0)])\n","\n","# ============== 編集エリア(start) =============\n","# スケーリング（標準化 standardization）\n","new_data = ss.transform(new_data)\n","\n","# 予測\n","model_ss.predict(new_data)\n","\n","# ============== 編集エリア(end) =============="],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-YywDR48RdG","colab_type":"text"},"source":["---\n","\n","## まとめ\n","\n","　今回の実習では、機械学習の概要を学びました。また、機械学習手法のひとつである **線形回帰** を通して、機械学習の流れを学びました。\n","\n","　線形回帰の内部で使われている計算（係数や誤差の最適値を求める計算）を自身でコーディングしようとすると、手間がかかります。**scikit-learn**ライブラリを使うことで、少量のコーディングで線形回帰モデルを構築することが可能です。有名な手法であれば、線形回帰以外の手法も scikit-learn で同じように実装可能です。\n","\n","　機械学習の流れ（**前処理 → 選択 → 学習 → 評価 → 予測**）は、基本的にどの手法であっても同じです。  \n","　「前処理」では、**トレーニングデータ**と**テストデータ**の分割や説明変数の**スケーリング**をおこないました。他にも、カテゴリカルデータや欠損値など処理もこの工程でおこないます。  \n","　「選択」では、使うモデルの選択をおこないました（線形回帰のみでしたが...）。通常、ハイパーパラメータと呼ばれる学習前に設定しなければならないパラメータを決めるのもこの工程でおこないます。  \n","　「学習」では、トレーニングデータを使ってモデルの学習をおこないました。最適値を得るアルゴリズムや計算方法は、機械学習の手法によって異なっています。  \n","　「評価」では、今回は**決定係数$R^2$**と呼ばれる評価指標で、モデルの良し悪しをみました。解析目的によって、何を評価指標にするか変わってくるので、実際の解析では重要な工程です。  \n","　「予測」では、得られた予測モデルに新しいデータを入力して、その予測値を得ました。\n","\n","　次回は、回帰問題を通して、**目的関数（コスト関数）** の最適値を探すアルゴリズム **勾配法 Gradient method** を学びます。これは、ニューラルネットワーク（Neural network）と呼ばれる高度な機械学習モデルにも使われている手法です。"]},{"cell_type":"markdown","metadata":{"id":"z9GvmFeOiC6i","colab_type":"text"},"source":["##### 「機械学習」の勉強におすすめの本\n","- 理論\n","  - [The Hundred-Page Machine Learning Book](http://themlbook.com/)\n","  - [わけがわかる機械学習](https://gihyo.jp/book/2019/978-4-297-10740-6)\n","- Pythonプログラミング\n","  - [Python Machine Learning](https://www.packtpub.com/data/python-machine-learning-third-edition)（原書）/ [Python機械学習プログラミング](https://book.impress.co.jp/books/1117101099)（日本語訳）\n","  - [機械学習のための「前処理」入門](https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%80%8C%E5%89%8D%E5%87%A6%E7%90%86%E3%80%8D%E5%85%A5%E9%96%80-%E8%B6%B3%E7%AB%8B-%E6%82%A0/dp/4865941967)\n","  - [Pythonと実データで遊んで学ぶ データ分析講座](http://www.c-r.com/book/detail/1322)"]},{"cell_type":"markdown","metadata":{"id":"hh43xglcF6l1","colab_type":"text"},"source":["##### 今回実装した機械学習のプログラム（おもなコードのみ）"]},{"cell_type":"code","metadata":{"id":"DRGcErOSq7Ta","colab_type":"code","outputId":"d8fbc63a-cb84-4819-b641-8347f1f3e18a","executionInfo":{"status":"ok","timestamp":1574577591843,"user_tz":-540,"elapsed":2410,"user":{"displayName":"Atsushi Ohta","photoUrl":"","userId":"10102012717593146998"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# ライブラリ\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","\n","# サンプルデータの読み込み\n","!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/gene_expression.csv\n","df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n","x = np.array(df.loc[:,[\"gene_7\", \"gene_11\", \"gene_29\"]]) # 説明変数3つ\n","y = np.array(df[\"phenotype\"])                              # 目的変数\n","\n","# === 前処理 ===\n","# データ分割\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n","# 標準化\n","ss = StandardScaler()\n","x_train_ss = ss.fit_transform(x_train)\n","x_test_ss = ss.transform(x_test)\n","\n","# === モデル選択＆学習 ===\n","model_ss = LinearRegression()  # 線形回帰\n","model_ss.fit(x_train_ss, y_train)    # 学習\n","\n","# === 評価 ===\n","r2_train_ss = model_ss.score(x_train_ss, y_train)  # トレーニングデータ\n","r2_test_ss = model_ss.score(x_test_ss, y_test)    # テストデータ\n","print(\"training: \", r2_train_ss)\n","print(\"test: \"    , r2_test_ss)\n","\n","# === 予測 ===\n","# 新しいデータ\n","new_data = np.array([(10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)])\n","# スケーリング（標準化 standardization）\n","new_data = ss.transform(new_data)\n","# 予測値\n","model_ss.predict(new_data)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["training:  0.6316303143591651\n","test:  0.7031019471801097\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([ 99.99125475, 118.58182973, 105.17854055])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"RUQ9fUNMIQ4c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}