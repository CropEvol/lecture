{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L09_ML_gradient_method.ipynb","provenance":[],"collapsed_sections":["qU2dtMxyz_ID","ao-qiItunmR8","pBnkaPCsNWGy","EbtzqFi5IOY2","AQ7K2-c1lvgD","_kiw8D3UT2F9","GA5Him55i3fG","w8qk7p4C-qhW","ekpC2AAWj7NS"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gBb8VqFVQVfk","colab_type":"text"},"source":["<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/lec_title.png?raw=true\" alt=\"2019年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"KIfbawIwEw5H","colab_type":"text"},"source":["# 機械学習 - 最適値探索 -"]},{"cell_type":"code","metadata":{"id":"7jHa_iFsX2nu","colab_type":"code","colab":{}},"source":["# /// 実習前にこのセルを実行してください ///\n","# データの読み込み\n","!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/gene_expression.csv\n","# 勾配法用モジュール\n","!wget -q -O gradient_method.py https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/modules/gradient_method.py\n","# 動画再生用モジュール\n","!apt-get -q install ffmpeg"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0EP9GpPE0Xa","colab_type":"text"},"source":["## 今回の実習内容\n","\n","　今回は、最適値の探索方法について学びます。まず、線形回帰を例に、「勾配法」と呼ばれるパラメータ（係数や誤差）の最適値を探索する方法を学びます。また、「グリッドサーチ」と呼ばれる適切なハイパーパラメータ（初期設定値）を見つける方法についても学びます。\n","\n","1. 勾配法とは？\n","1. 確率的勾配降下法 Stochastic Gradient Descent\n","1. 最急降下法 Gradient Descent\n","1. グリッドサーチ Grid search\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"nCdARPQSFWXj","colab_type":"text"},"source":["## 1. 勾配法とは？\n","　線形回帰モデルは次のような式で表されます。\n","$$y=\\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_m x_m + e $$\n","\n","　前回は、scikit-learnの`LinearRegression`を用いて、説明変数が数個（1,2個）の場合の線形回帰をおこないました。`LinearRegression`は、「最小二乗法」を用いて、残差平方和 $\\sum(\\hat{y}_i - y_{i})^{2}$ が最小値をとるときの係数$\\beta$と誤差$e$ を求めることも学びました。\n","\n","　最小二乗法は、説明変数が少ない場合、一瞬のうちに解を求めることができます。しかし、説明変数が多くなると、計算量がとてつもなく多くなり、短時間で解が得られなくなります。そのような場合に有効な方法が**勾配法（Gradient method）**です。\n","\n","　最小二乗法と勾配法はどう違うのか？　最小二乗法は、計算により最小値をピンポイントに見つける方法です。勾配法は、ある値から出発して、係数$\\beta$や誤差$e$の値を徐々に更新し、最小値に近づいていく方法です。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method.png?raw=true\" alt=\"gradient_method\" height=\"250px\">"]},{"cell_type":"markdown","metadata":{"id":"qU2dtMxyz_ID","colab_type":"text"},"source":["### 最小二乗法の計算コスト\n","_※ 数学（行列）の話になるので、読み飛ばしてOKです。_"]},{"cell_type":"markdown","metadata":{"id":"LuE4cefrYlkF","colab_type":"text"},"source":["　$i$行$j$列のトレーニングデータセットの説明変数を行列$X$で表すと、次のようになります。1行は1サンプル分のデータで、各列は説明変数の値です。また、目的変数$y$も$i$行1列の行列で表すことができ、係数$\\beta$も$j$行1列の行列で表すことができます。\n","\n","$$\n","X = \\begin{pmatrix}\n","x_{11} & x_{12} & x_{12} & ... & x_{1j} \\\\\n","x_{21} & x_{22} & x_{22} & ... & x_{2j} \\\\\n","x_{31} & x_{32} & x_{32} & ... & x_{3j} \\\\\n"," : & : & : & ... & : \\\\\n"," x_{i1} & x_{i2} & x_{i2} & ... & x_{ij} \\\\\n","\\end{pmatrix}, \n","y = \\begin{pmatrix}\n","y_{1}  \\\\\n","y_{2} \\\\\n","y_{3} \\\\\n"," :  \\\\\n","y_{i} \\\\\n","\\end{pmatrix},  \n","\\beta = \\begin{pmatrix}\n","\\beta_{1}  \\\\\n","\\beta_{2} \\\\\n","\\beta_{3} \\\\\n"," :  \\\\\n","\\beta_{j} \\\\\n","\\end{pmatrix}\n","$$\n","\n","　最小二乗法で、残差平方和を最小化する$\\hat{\\beta}$を求めるとき、実際には次のような行列式を解いています。\n","$$ \\hat{\\beta} = (X^{T} X)^{-1} X^{T} y $$\n","\n","　説明変数$j$が多くなると、上述の式の$(X^{T} X)^{-1}$（逆行列）の計算が大規模計算機を用いても困難になってきます（時間がかかりすぎるか、メモリ上に数値を保持できなくなります）。\n","\n","　もう少し具体的に言うと、\n","- $X^{T}$（$j\\times i$行列）と$X$（$i\\times j$行列）の積$X^{T} X$により、$j\\times j$行列ができます。この$j\\times j$行列の逆行列を求めることになります。\n","- LU分解を利用して $j\\times j$行列の逆行列を得ようとすると、$j^3$の計算量が必要になります（「[プログラミングのための線形代数](https://www.ohmsha.co.jp/book/9784274065781/)」より）。\n"]},{"cell_type":"markdown","metadata":{"id":"ao-qiItunmR8","colab_type":"text"},"source":["### 勾配法のアルゴリズム"]},{"cell_type":"markdown","metadata":{"id":"qXLykc5qULf2","colab_type":"text"},"source":["　勾配法は、上述したとおり、「ある値から出発して、係数𝛽や誤差𝑒の値を徐々に更新し、最小値に近づいていく方法」です。ここではその方法について、もう少し具体的な話をしましょう。\n","\n","　次の図のように、下に凸なグラフがあり、現在の$\\beta$の値が$\\beta_0$だったとします。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method_example.png?raw=true\" alt=\"gradient_method_example\" height=\"250px\">\n","\n","- [Question] 目的関数$Cost(\\beta)$が最小値となる$\\beta_{optimum}$に近づくためには、現在の$\\beta$からどのように値を変化させればいいでしょうか？  \n","  - [Answer] $\\beta$が右側に移るような値（$\\beta_0$に正の値）を追加すれば、$\\beta_{optimum}$側に近づきます。\n","- [Question] どの程度の正の値を追加すれば良いでしょうか？\n","  - [Answer] 大きすぎない値を追加すれば、$\\beta_{optimum}$に近づきます。\n","\n","　勾配法では、上のQ&Aのような処理を、次のようにおこなっています。\n","1. まず、$\\beta_0$の「接線の傾き」を調べます。すなわち、$Cost(\\beta)$を$\\beta$について微分したときの値を調べます。\n","  $$ \\frac{\\partial Cost(\\beta)}{\\partial  \\beta} $$\n","\n","1. その値が負のとき、右方向に移るように$\\beta$の値を更新します。その時の更新幅を、傾きの大きさと**学習率$\\eta$（イータ; eta）**の積で決定します。\n","  $$ \\beta_{1} := \\beta_{0} - \\eta \\frac{\\partial Cost(\\beta)}{\\partial  \\beta} $$\n","\n","1. この1と2のステップを$\\beta$の値が変化しなくなるまで（もしくは、指定の繰り返し数に達するまで）、ひたすら繰り返します。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method_algorithm.png?raw=true\" alt=\"gradient_method_algorithm\" height=\"250px\">\n"]},{"cell_type":"markdown","metadata":{"id":"iiJyfvDZJkRk","colab_type":"text"},"source":["### おもな勾配法\n","\n","　おもな勾配法には下記の3つがあります。これらの違いは、勾配の計算をおこなうときに使うサンプルセットが異なっている点です。そのほかのアルゴリズムはほとんど同じです。\n","- **確率的勾配降下法 Stochastic Gradient Descent**\n","  - ランダムに選んだ1サンプルのデータを使って勾配の計算をおこない、パラメータを更新する\n","- **最急降下法（バッチ勾配降下法） Gradient Descent**\n","  - 全サンプルのデータセットを使って勾配の計算をおこない、パラメータを更新する\n","- **ミニバッチ勾配降下法 Mini batch Gradient Descent**\n","  - ランダムに選んだ複数サンプルのデータセットを使って勾配の計算をおこない、パラメータを更新する\n","\n","　このテキストでは、「確率的勾配降下法」と「最急降下法」の概要と実装方法をみていきます。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fEEV-hVOYPlQ","colab_type":"text"},"source":["### 今回の実習のサンプルデータ\n","\n","　扱うデータは、前回と同じく、遺伝子発現量と表現型のデータセットです。勾配法の仕組みを学ぶために、説明変数1個と目的変数1個のみを使います。\n","\n","　次のコードセルを実行して、データを読み込み、説明変数xと目的変数yの値を得てください。実行すると、自動的に散布図まで書かれるようになっています。\n","\n","<small>_※ 勾配法の仕組みを勉強するだけなので、「トレーニングデータとテストデータの分割」や「モデルの評価」、「新しいデータの予測」は今回おこないません。_</small>\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"V-hiw5rp9raA","colab_type":"code","colab":{}},"source":["# 説明変数xとして使うデータ\n","# gene_1からgene_50であれば、どれでもOK（初期値: gene_29）\n","use_var = \"gene_29\"  \n","\n","#===== 以下は変更しないでください =====\n","# ライブラリ\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler \n","# データの読み込み\n","df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n","# 変数x, y　（＊重要＊ 説明変数xは正規化をおこなっている）\n","mms = MinMaxScaler()\n","x = mms.fit_transform(np.array(df[use_var]).reshape(-1,1))\n","y = np.array(df[\"phenotype\"])\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.figure(figsize=[8, 4])\n","plt.scatter(x, y, color=\"blue\", label=\"training data\") # データ\n","plt.xlabel(\"normalized x\")  # x軸ラベル\n","plt.ylabel(\"y\")  # y軸ラベル\n","plt.legend()         # 凡例\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pBnkaPCsNWGy","colab_type":"text"},"source":["### 参考: 最小二乗法で線形回帰モデルを推定した場合..."]},{"cell_type":"markdown","metadata":{"id":"Vp_hM25XSbnO","colab_type":"text"},"source":["　この例を「最小二乗法」を使って線形回帰モデルを推定すると、次の係数$\\beta$と誤差$e$が得られます。"]},{"cell_type":"code","metadata":{"id":"kIaKpaTOTx1-","colab_type":"code","colab":{}},"source":["# =================== 線形回帰 ===================\n","from sklearn.linear_model import LinearRegression\n","\n","# モデル作成・学習\n","model_lr = LinearRegression()\n","model_lr.fit(x, y)\n","\n","# 学習後の係数と誤差を確認\n","print(\"b=\", model_lr.coef_)\n","print(\"e=\", model_lr.intercept_)\n","\n","# 決定係数\n","print(\"R2=\", model_lr.score(x, y))\n","\n","# =================== グラフ ===================\n","import matplotlib.pyplot as plt\n","\n","# 直線を書くためのデータ\n","x_line = np.linspace(np.min(x), np.max(x), num=2)\n","y_line = model_lr.predict(x_line.reshape(-1, 1))\n","\n","# グラフ\n","plt.figure(figsize=[8, 4])\n","plt.scatter(x, y, color=\"blue\", label=\"training data\") # データ\n","plt.plot(x_line, y_line, color=\"orange\")\n","plt.xlabel(\"normalized x\")  # x軸ラベル\n","plt.ylabel(\"y\")  # y軸ラベル\n","plt.legend()         # 凡例\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yHGXQtmQlByJ","colab_type":"text"},"source":["## 2. 確率的勾配降下法 Stochastic Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"cKSg4BLslD4A","colab_type":"text"},"source":["### 概要\n","\n","　**確率的勾配降下法（Stochastic Gradient Descent: SGD）**は、ランダムに（確率的に）選んだ1サンプルを使って勾配計算をおこない、係数$\\beta$や誤差$e$などの**パラメータ（Parameter）**を少しずつ更新する方法です。\n","\n","　このアルゴリズムは、次のとおりです。\n","1. トレーニング回数（イテレーション数）やパラメータの更新幅（**学習率**）を設定します。\n","1. パラメータ（係数や誤差）の初期値を設定する。多くの場合、「0」または「ごく小さいランダムな数値（小数点数）」を設定します。\n","1. 上で設定したトレーニング回数の分だけ、以下の操作を繰り返します。  \n","  次の(1)-(3)を全データに対しておこないます。  \n","  (1) データセットからランダムに1サンプル選びます。  \n","  (2) そのデータを使って、勾配計算をおこないます。  \n","  (3) 勾配と学習率をかけた値を、パラメータから引き、パラメータ更新をおこないます。\n","    $$ 更新後パラメータ := 更新前パラメータ - 学習率 \\times 勾配$$\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"vWDZU6TIlJzE","colab_type":"text"},"source":["### 実装\n","\n","　scikit-learnで「確率的勾配降下法」をおこなうことが可能です。\n","\n","```python\n","# 実装方法\n","from sklearn.linear_model import SGDRegressor\n","model = SGDRegressor(max_iter=100, eta0=0.1)\n","model.fit(x, y)\n","```"]},{"cell_type":"code","metadata":{"id":"EUeWDFuYlqUV","colab_type":"code","colab":{}},"source":["# =================== 勾配法 ===================\n","from sklearn.linear_model import SGDRegressor\n","\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = 0.01   # 学習率\n","\n","# モデル作成・学習\n","model_gd = SGDRegressor(max_iter=n_iter, eta0=eta, random_state=1, learning_rate=\"constant\")\n","model_gd.fit(x, y)\n","print(\"b=\", model_gd.coef_)\n","print(\"e=\", model_gd.intercept_)\n","\n","# 決定係数\n","r2 = model_gd.score(x, y)\n","print(\"R2=\", r2)\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","\n","x_line = np.linspace(np.min(x), np.max(x), num=2)\n","y_line = model_gd.predict(x_line.reshape(-1, 1))\n","\n","fig = plt.figure(figsize=[8, 4])\n","plt.scatter(x, y, color=\"blue\") # データ\n","plt.plot(x_line, y_line, color=\"orange\")\n","plt.xlabel(\"normalized x\")  # x軸ラベル\n","plt.ylabel(\"y\")  # y軸ラベル\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFd99685A8ID","colab_type":"text"},"source":["### 実習1\n","\n","　次のデータセット（説明変数`X`、目的変数`Y`）について、「確率的勾配降下法」で回帰モデルを作成してください。その際、以下のオプションを設定してください。\n","\n","- トレーニング回数`max_iter`: 自由に設定してください\n","- 学習率`eta0`: 自由に設定してください\n"]},{"cell_type":"code","metadata":{"id":"XKW4tLO6BhTk","colab_type":"code","colab":{}},"source":["# /// このセルは実行のみ ///\n","import numpy as np\n","\n","# データ(y = -5x + 3 + 誤差)\n","N = 100\n","X = 2 * np.random.rand(N, 1)\n","Y = (-5*X + 3 + np.random.randn(N, 1)).flatten()\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(X, Y, color=\"blue\", label=\"training data\") # データ\n","plt.xlabel(\"X\")  # x軸ラベル\n","plt.ylabel(\"Y\", rotation=0)  # y軸ラベル\n","plt.legend()         # 凡例\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnzNJxIvIVp-","colab_type":"code","colab":{}},"source":["# /// このセルに追記してください ///\n","from sklearn.linear_model import SGDRegressor\n","\n","# --------------- 編集箇所: start ---------------\n","# 学習前のモデルを作成\n","model_sgd = \n","# データをセットしてモデルを学習\n","model_sgd\n","\n","# --------------- 編集箇所: end -----------------\n","\n","# 係数と誤差、決定係数\n","print(\"b=\", model_sgd.coef_)\n","print(\"e=\", model_sgd.intercept_)\n","print(\"R2=\", model_sgd.score(X, Y))\n","\n","# 直線を書くためのデータ\n","X_line = np.linspace(np.min(X), np.max(X), num=2)\n","Y_line = model_sgd.predict(X_line.reshape(-1, 1))\n","\n","# 描画\n","import matplotlib.pyplot as plt\n","plt.scatter(X, Y, color=\"blue\", label=\"training data\") # データ\n","plt.plot(X_line, Y_line, color=\"orange\", label=\"model\") # 直線\n","plt.xlabel(\"X\")  # x軸ラベル\n","plt.ylabel(\"Y\", rotation=0)  # y軸ラベル\n","plt.legend()         # 凡例\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EbtzqFi5IOY2","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"5KLizjo-IQN0","colab_type":"code","colab":{}},"source":["# /// このセルに追記してください ///\n","from sklearn.linear_model import SGDRegressor\n","\n","# --------------- 編集箇所: start ---------------\n","# 学習前のモデルを作成\n","model_sgd = SGDRegressor(max_iter=100, eta0=0.1)\n","# データをセットしてモデルを学習\n","model_sgd.fit(X, Y)\n","\n","# --------------- 編集箇所: end -----------------\n","\n","# 係数と誤差、決定係数\n","print(\"b=\", model_sgd.coef_)\n","print(\"e=\", model_sgd.intercept_)\n","print(\"R2=\", model_sgd.score(X, Y))\n","\n","# 直線を書くためのデータ\n","X_line = np.linspace(np.min(X), np.max(X), num=2)\n","Y_line = model_sgd.predict(X_line.reshape(-1, 1))\n","\n","# 描画\n","import matplotlib.pyplot as plt\n","plt.scatter(X, Y, color=\"blue\", label=\"training data\") # データ\n","plt.plot(X_line, Y_line, color=\"orange\", label=\"model\") # 直線\n","plt.xlabel(\"X\")  # x軸ラベル\n","plt.ylabel(\"Y\", rotation=0)  # y軸ラベル\n","plt.legend()         # 凡例\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQ7K2-c1lvgD","colab_type":"text"},"source":["### 学習経過を記録する"]},{"cell_type":"markdown","metadata":{"id":"QflqMkZIpPBL","colab_type":"text"},"source":["　上で使用した`fit`は、全工程を一通りおこなうものです。途中経過を記録したい場合、`partial_fit`を使って、次のようなコードを書きます。\n","\n","1. 学習前のモデルを作る\n","2. ループ構文で、以下を任意回数繰り返す\n","  1. サンプル順序をシャッフルする（サンプルをランダムに並び替える）\n","  1. ループ構文で、1サンプルずつ取り出す。\n","  1. 取り出した1サンプルデータを `partial_fit`に渡して、モデルの学習をおこなう\n","  1. 全サンプルの学習を終えたら、パラメータなどを記録する\n","\n"]},{"cell_type":"code","metadata":{"id":"RilY-10dlSTb","colab_type":"code","colab":{}},"source":["# =================== 勾配法 ===================\n","from copy import deepcopy\n","from sklearn.linear_model import SGDRegressor\n","\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = 0.01   # 学習率\n","\n","# 記録用リスト\n","log_coef = []      # 係数（傾き）\n","log_intercept = [] # 誤差(切片)\n","log_cost = []      # 残差二乗和\n","\n","# モデル作成\n","model_gd = SGDRegressor(eta0=eta, random_state=1, learning_rate=\"constant\")\n","\n","# 設定数の学習を繰り返す\n","for iteration in range(n_iter):\n","  # トレーニングデータをランダムに並べ替える\n","  r = np.random.permutation(len(y))\n","\n","  # 1サンプルずつ学習する\n","  for i in range(0, len(y), 1):\n","    idx = r[i:i+1]  # サンプルのインデックスを取得\n","    x_i = x[idx]       # サンプルのデータを取得\n","    y_i = y[idx]\n","    # 学習\n","    model_gd.partial_fit(x_i, y_i)\n","  \n","  # 記録\n","  log_coef.append(deepcopy(model_gd.coef_))\n","  log_intercept.append(model_gd.intercept_)\n","  cost = ((model_gd.predict(x) - y)**2).sum() / 2.0 / len(y)  # コスト（残差二乗和）の計算\n","  log_cost.append(cost)\n","\n","# 学習後の係数と誤差を確認\n","print(\"b=\", model_gd.coef_)\n","print(\"e=\", model_gd.intercept_)\n","# 決定係数\n","print(\"R2=\", model_gd.score(x, y))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6OWGk6pmTFz","colab_type":"text"},"source":["　以下のコードセルを実行すると、モデルが学習されていく過程を動画でみれます。\n","\n","<small>*※ 学習過程のデータを動画用データに変換するコードは、こちらに記述しています: [gradient_method.py](https://github.com/CropEvol/lecture/blob/master/textbook_2019/modules/gradient_method.py)の`plot_reg`関数*</small>"]},{"cell_type":"code","metadata":{"id":"ucSpvrxemOB2","colab_type":"code","colab":{}},"source":["# =================== グラフ ===================\n","import matplotlib.pyplot as plt\n","from matplotlib import animation, rc, rcParams\n","from IPython.display import HTML\n","from gradient_method import plot_reg\n","rcParams['animation.embed_limit'] = 2**128\n","\n","# パラメータのログを取得\n","b_ = log_coef\n","e_ = log_intercept\n","c_ = log_cost\n","\n","# 動画実行\n","fig = plt.figure(figsize=[16, 4])\n","plt.close()\n","frames = plot_reg(fig, x, y, b_, e_, c_, n_frames=100)\n","ani = animation.ArtistAnimation(fig, frames, interval=100, blit=True)\n","# ani.save('gradient_descent.mp4', writer=\"ffmpeg\")\n","rc('animation', html='jshtml')\n","ani"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q2Gc5geqFY2B","colab_type":"text"},"source":["## 3. 最急降下法 Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"Jxhl5Qr4j0-t","colab_type":"text"},"source":["### 概要\n","\n","　「最急降下法」は、全サンプルで勾配計算をおこない、その平均値を使って、係数$\\beta$や誤差$e$などの**パラメータ（Parameter）**を更新する方法です。\n","\n","　このアルゴリズムは、次のとおりです。\n","1. トレーニング回数（イテレーション数）やパラメータの更新幅（**学習率**）を設定します。\n","1. パラメータ（係数や誤差）の初期値を設定する。多くの場合、「0」または「ごく小さいランダムな数値（小数点数）」を設定します。\n","1. 上で設定したトレーニング回数の分だけ、以下の操作を繰り返します。   \n","  (1) 各サンプルのデータを使って勾配を計算し、その平均値を求めます。  \n","  (2) 勾配の平均値と学習率をかけた値を、パラメータから引き、パラメータ更新をおこないます。\n","    $$ 更新後パラメータ := 更新前パラメータ - 学習率 \\times \\frac{1}{N}\\sum 勾配$$\n"]},{"cell_type":"markdown","metadata":{"id":"phSTwSG_cy-Q","colab_type":"text"},"source":["### 実装\n","　scikit-learnライブラリに「最急降下法」用の関数はありません。自身でコーディングする必要があります（例: [gradient_method.py](https://github.com/CropEvol/lecture/blob/master/textbook_2019/modules/gradient_method.py)の`GradientDescent`）。\n","\n","　ここでは、「確率的勾配降下法」用の`SGDRegressor`の`partial_fit`を利用して、「最急降下法」をおこないます。\n","\n","1. 学習前のモデルを作成する `SGDRegressor`\n","1. データセット全体に対して、`partial_fit`を使い、モデルの学習をおこなう\n","1. 2を繰り返す\n","\n","\n","```python\n","# 実装方法\n","from sklearn.linear_model import SGDRegressor\n","model = SGDRegressor()\n","for i in range(トレーニング回数):\n","  model.partial_fit(x, y)\n","```\n","\n","<small>*※ この`SGDRegressor`の`partial_fit`を使った方法は、実際のところ1サンプルごとにパラメータ更新をおこなっており、真の「最急降下法」（全データにおける勾配の平均値でパラメータ更新）ではありません。しかし、「最急降下法」とほとんど変わりない学習過程を観察できるため、ここではこの方法を使っています。*</small>\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"_nz3fVcSO7JD","colab_type":"code","colab":{}},"source":["# =================== 勾配法 ===================\n","from copy import deepcopy\n","from sklearn.linear_model import SGDRegressor\n","\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = 0.01   # 学習率\n","\n","# 記録用リスト\n","log_coef = []      # 係数（傾き）\n","log_intercept = [] # 誤差(切片)\n","log_cost = []      # 残差二乗和\n","\n","# モデル作成\n","model_gd = SGDRegressor(eta0=eta, random_state=1, learning_rate=\"constant\")\n","\n","# 設定数の学習を繰り返す\n","for iteration in range(n_iter):\n","  #　学習\n","  model_gd.partial_fit(x, y)\n","  # 記録\n","  log_coef.append(deepcopy(model_gd.coef_))\n","  log_intercept.append(model_gd.intercept_)\n","  cost = ((model_gd.predict(x) - y)**2).sum() / 2.0 / len(y)  # コストの計算\n","  log_cost.append(cost)\n","\n","# 学習後の係数と誤差を確認\n","print(\"b=\", model_gd.coef_)\n","print(\"e=\", model_gd.intercept_)\n","# 決定係数\n","print(\"R2=\", model_gd.score(x, y))\n","\n","# =================== グラフ ===================\n","import matplotlib.pyplot as plt\n","from matplotlib import animation, rc, rcParams\n","from IPython.display import HTML\n","from gradient_method import plot_reg\n","rcParams['animation.embed_limit'] = 2**128\n","\n","# パラメータのログを取得\n","b_ = log_coef\n","e_ = log_intercept\n","c_ = log_cost\n","\n","# 動画実行\n","fig = plt.figure(figsize=[16, 4])\n","plt.close()\n","frames = plot_reg(fig, x, y, b_, e_, c_, n_frames=100)\n","ani = animation.ArtistAnimation(fig, frames, interval=100, blit=True)\n","# ani.save('gradient_descent.mp4', writer=\"ffmpeg\")\n","rc('animation', html='jshtml')\n","ani"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_kiw8D3UT2F9","colab_type":"text"},"source":["### 「確率的勾配降下法」と「最急降下法」の比較"]},{"cell_type":"markdown","metadata":{"id":"eNHblcu2T458","colab_type":"text"},"source":["|| 確率的勾配降下法 | 最急降下法 |\n","|---:|---:|---:|\n","| パラメータ更新 | 1サンプル毎 | データセット毎 |\n","| 計算量 | 一定 | サンプル数に依存して多くなる |\n","| 解周辺に辿り着く早さ | 早い | サンプル数に依存して遅くなる |\n","| 1サンプル（外れ値）の影響 | 影響を受けやすい | 平均値を使うので影響が緩和される |\n","| パラメータ更新の方向<br>（学習率が低い場合） | 総じて目標の解の方向に更新されるが、<br>ときどき反対方向にも更新される | 一方向に更新される |\n","| 局所解 | 抜け出せる可能性がある | 抜け出せない |\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/local_optimum.png?raw=true\" alt=\"local_optimum\" height=\"200px\">"]},{"cell_type":"markdown","metadata":{"id":"GA5Him55i3fG","colab_type":"text"},"source":["### ミニバッチ勾配降下法 Mini batch gradient descent"]},{"cell_type":"markdown","metadata":{"id":"o9uLqorijFMn","colab_type":"text"},"source":["　「最急降下法」は、データセット全体をパラメータ更新単位（**バッチ**）としてみなしており、「確率的勾配降下法」は、ランダムに選んだ1サンプルをパラメータ更新単位としてみなす方法です。\n","\n","　**ミニバッチ勾配降下法（Mini batch gradient descent）**は、上述二つの方法の中間的な方法です。この方法では、ランダムに選んだ複数サンプルを一つのバッチとしてみなして、パラメータの更新をおこないます。\n","\n","　トレーニングデータのサイズが大きい場合、「最急降下法」ではサンプル全体の計算に時間がかかってしまいますが、「ミニバッチ勾配降下法」では、サブセットを1ユニットとして計算をおこなうため、**解析スピードを損わずにパラメータの更新**をおこなえます。また、「確率的勾配降下法」では1サンプルの影響（外れ値サンプルの影響）を受けやすいですが、「ミニバッチ勾配降下法」は、サブセットの平均値を使うため、**1サンプルの影響を受けにくい**です。加えて、ランダム性も持ち合わせているため、**局所解に陥りにくい**方法です。"]},{"cell_type":"markdown","metadata":{"id":"suM7kwswEW8d","colab_type":"text"},"source":["## 4. グリッドサーチ Grid search"]},{"cell_type":"markdown","metadata":{"id":"PEZIhL2GEcy0","colab_type":"text"},"source":["　機械学習では、**パラメータ（Parameter）**といわれるものがあります。線形回帰の例では、係数$\\beta$や誤差$e$がこれに相当します。それらパラメータは、学習によって変動する値であり、学習過程でほぼ最適解（もしくは局所解）が得られるので、機械学習をおこなう者が設定する必要のない値です。\n","$$y=\\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_m x_m + e $$\n","\n","　一方で、**ハイパーパラメータ（Hyperparameter）**と呼ばれる特別なパラメータもあります。このハイパーパラメータは、機械学習をおこなう前に設定し、トレーニング時は更新されない値です。機械学習をおこなう者が任意の値を設定する必要があります。\n","\n","<small>*※ 今回の実習で登場した「学習率」や「トレーニング回数」はハイパーパラメータです。*</small>\n","\n","　ハイパーパラメータは、トレーニング結果に大きく影響を与えます。適切な値を設定しなければ、全く意味のない予測モデルができてしまいます。実際の機械学習では、この値の探索（**チューニング Tuning**）が非常に大切です。\n","\n","　ここでは、良いハイパーパラメータをどうやって見つけるか勉強します。"]},{"cell_type":"markdown","metadata":{"id":"-IIx9wOXjzkc","colab_type":"text"},"source":["### グリッドサーチ Grid search\n","\n","　学習率などの「ハイパーパラメータ」は、学習過程で最適値が求められることはありません。どうやって、ハイパーパラメータを見つけるか？ この答えは、手探りで見つけるしかありません。\n","\n","　設定可能な値をすべて調べれば、最適なハイパーパラメータを見つけることはできますが、現実的には不可能です。そこで、一定間隔な値のみを調べて、ハイパーパラメータの絞り込みをおこなう、**グリッドサーチ（Grid search）**と呼ばれる方法を用います。\n","\n","　グリッドサーチには、次の二点が重要です。\n","- 調べるハイパーパラメータの間隔をどう設定するか\n","- 何を指標に評価するか\n","\n","　調べるハイパーパラメータの間隔は、何も情報がない場合、対数的に等間隔な値を使うのが一般的です。例えば、「0.0001, 0.001, 0.01, 0.1, 1.0」といった値です。\n","\n","　評価指標は、機械学習をおこなう目的によって変わってくるでしょう。良い予測精度をもったモデルを作りたい場合、決定係数$R^2$やラベルの正解率、などを指標に使うことになるでしょう。\n","\n","　ここでは、「最急降下法」の学習率について、以下の設定値を調べて、最も決定係数$R^2$が高くなる値を探してみましょう。\n","- 学習回数: 100に固定\n","- 調べる学習率: 0.0001, 0.001, 0.01, 0.1, 1.0\n","- 評価指標: 決定係数$R^2$"]},{"cell_type":"code","metadata":{"id":"_q7cb8NcEXW3","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import SGDRegressor\n","\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = [0.0001, 0.001, 0.01, 0.1, 1.0]   # 学習率\n","\n","# グリッドサーチ\n","R2 = []  # 調べた決定係数R2を保存するリスト\n","for r in eta:\n","  \n","  # 「最急降下法」を使った回帰モデルの作成と学習\n","  model_gd = SGDRegressor(eta0=r, random_state=1, learning_rate=\"constant\")\n","  for iteration in range(n_iter):\n","    model_gd.partial_fit(x, y)\n","\n","  # 学習後、決定係数R2を調べる\n","  r2 = model_gd.score(x, y)\n","  R2.append(r2)\n","  print(\"eta=\", r, \" R2=\", r2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3CNeL0l4Shh","colab_type":"text"},"source":[" 学習率と決定係数の関係をグラフにすると、次のようになります。\n"]},{"cell_type":"code","metadata":{"id":"riqt8F6z4Yjo","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.plot(np.log10(eta), R2) # 直線\n","plt.xlabel(\"learning rate: log10(eta)\")  # x軸ラベル\n","plt.ylabel(\"R2\", rotation=0)  # y軸ラベル\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOtgHgD37M-8","colab_type":"text"},"source":["### 実習2\n","\n","　例の結果をみると、学習率0.01辺りが最も良さそうです。今度は、その周辺をグリッドサーチして、最適な値を絞り込んでください。\n","\n","- 学習回数: 100に固定\n","- 調べる学習率: 0.001から0.1までの値をいくつか設定（例えば、0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.01）\n","- 評価指標: 決定係数$R^2$"]},{"cell_type":"code","metadata":{"id":"A5lB0voR-1Ih","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import SGDRegressor\n","\n","# --------------- 編集箇所: start ---------------\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = []     # 学習率\n","\n","# --------------- 編集箇所: end -----------------\n","\n","\n","# グリッドサーチ\n","R2 = []  # 調べた決定係数R2を保存するリスト\n","for r in eta:\n","  \n","  # 「最急降下法」を使った回帰モデルの作成と学習\n","  model_gd = SGDRegressor(eta0=r, random_state=1, learning_rate=\"constant\")\n","  for iteration in range(n_iter):\n","    model_gd.partial_fit(x, y)\n","\n","  # 学習後、決定係数R2を調べる\n","  r2 = model_gd.score(x, y)\n","  R2.append(r2)\n","  print(\"eta=\", r, \" R2=\", r2)\n","\n","# グラフ\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.plot(np.log10(eta), R2) # 直線\n","plt.xlabel(\"learning rate: log10(eta)\")  # x軸ラベル\n","plt.ylabel(\"R2\", rotation=0)  # y軸ラベル\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w8qk7p4C-qhW","colab_type":"text"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"qXU0ZK8S72la","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import SGDRegressor\n","\n","# --------------- 編集箇所: start ---------------\n","# 設定値\n","n_iter = 100  # 学習回数\n","eta   = [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1]   # 学習率\n","\n","# --------------- 編集箇所: end -----------------\n","\n","# グリッドサーチ\n","R2 = []  # 調べた決定係数R2を保存するリスト\n","for r in eta:\n","  \n","  # 「最急降下法」を使った回帰モデルの作成と学習\n","  model_gd = SGDRegressor(eta0=r, random_state=1, learning_rate=\"constant\")\n","  for iteration in range(n_iter):\n","    model_gd.partial_fit(x, y)\n","\n","  # 学習後、決定係数R2を調べる\n","  r2 = model_gd.score(x, y)\n","  R2.append(r2)\n","  print(\"eta=\", r, \" R2=\", r2)\n","\n","# グラフ\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.plot(np.log10(eta), R2) # 直線\n","plt.xlabel(\"learning rate: log10(eta)\")  # x軸ラベル\n","plt.ylabel(\"R2\", rotation=0)  # y軸ラベル\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekpC2AAWj7NS","colab_type":"text"},"source":["### 検証データ Validation data"]},{"cell_type":"markdown","metadata":{"id":"TwZt27I1kAJR","colab_type":"text"},"source":["　今回は、「トレーニングデータ」と「テストデータ」を分けずに実習してきましたが、機械学習では、通常、データセットを分割して解析します。トレーニングデータは、学習のためのデータセットで、テストデータは、学習結果を評価するためのデータセットです。\n","\n","　ハイパーパラメータのチューニングやモデル選択をおこないたい場合、**検証データ（Validation data）**というデータも用意するのが一般的です。データセットを「トレーニングデータ」と「検証データ」、「テストデータ」の3つに分割し、「トレーニングデータ」と「検証データ」を使って、ハイパーパラメータのチューニングやモデル選択をおこないます。そして、最も良いパフォーマンスが得られそうなモデルに対して、「テストデータ」を使って、最終的な評価をします。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/holdout.png?raw=true\" alt=\"holdout\" height=\"280px\">"]},{"cell_type":"markdown","metadata":{"id":"kzVyjWKuFg0I","colab_type":"text"},"source":["---\n","## まとめ\n","\n","　今回、**勾配法**と呼ばれる最適値探索法を学びました。「勾配法」には、おもに3つの方法、**確率的勾配降下法**、**最急降下法**、**ミニバッチ勾配降下法**があり、そのうち、「確率的勾配降下法」と「最急降下法」の概要と実装方法を学びました。  \n","\n","　「確率的勾配降下法」は、ランダムに選んだ1サンプルのデータを使ってパラメータを更新していく方法です。「最急降下法」は、データセットのすべてのサンプルに対して、勾配計算をおこない、その平均値でパラメータを更新する方法です。どちらの勾配法にも利点や欠点があります。\n","\n","　勾配法で有用なモデルを作るためには、**ハイパーパラメータ**のひとつ、**学習率**を適切な値に設定する必要があります。この適切な値を探索する（**チューニング**）方法として、**グリッドサーチ**を学びました。\n","\n","\n","\n","\n","\n","\n"]}]}