{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L12_ML_regression_1.ipynb","provenance":[],"collapsed_sections":["csp0UnLmtIzE","zsm5OPWyuXi-","28Yhuao-qhFY","pQQgC6d9rHDp","APHpyc9WrttL","NS1uQZ7psDWI","HhiREQJGpwnr","KgO8H5lOLm2n","iRLPUdXtUenk","CnW9CaqPnmFY","itwAzqgET4O0","c_ajWotnP61f","d-k6VE5eAOVe","5Sb7dh-mIClg","R47c_LFtQ9xD","gw4sP5gIn-ZA","NBOA7BR9d3fB"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e9fGSNj8QRzB"},"source":["<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3fFHZrzKpHGWl0vYz7Sr8FX8QqLQ_tc8XHBSwqQnM4hgsIOjtjaOde1M9oHSAfe1Fs2SwVORlapit4-JOz0mjP8Tnz6HetkLZDZb8CifSd0uoSp1Nj3wG_wh1sEQlKXXzvEA9Y9HnQqu2Ecv2igmInb=w1097-h235-no?authuser=0\" alt=\"2020年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">"]},{"cell_type":"markdown","metadata":{"id":"AtPzXrSAtBm8"},"source":["# 機械学習 - 線形回帰（1）-\n"]},{"cell_type":"markdown","metadata":{"id":"YpznGfs_aceP"},"source":["## 機械学習とは？\n","\n","　**機械学習（Machine Learning）**は、データから特徴やパターンを発見し、数理モデルを構築する技術です。**新たなデータに対して精確な予測をおこなうこと**が機械学習のおもな目的です。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/machine_learning.png?raw=true\" alt=\"Machine Learning\" height=\"180px\">\n"]},{"cell_type":"markdown","metadata":{"id":"csp0UnLmtIzE"},"source":["### 例えば・・・"]},{"cell_type":"markdown","metadata":{"id":"EXDt3DAwtbjC"},"source":["**分類 Classification**\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/classification.png?raw=true\" alt=\"classification\" height=\"300px\">\n","\n","**回帰 Regression**\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/regression.png?raw=true\" alt=\"regression\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"zsm5OPWyuXi-"},"source":["### 機械学習とデータマイニング"]},{"cell_type":"markdown","metadata":{"id":"V_K_n-1bv7zn"},"source":["　機械学習によく似た手法を用いるものとして、**データマイニング**（あるいは、統計モデリング）があります。データマイニングは、機械学習とは異なり、**データから特徴やパターンを発見すること**を目的に、データを解析します。\n","\n","　解析目的は違っているものの、機械学習とデータマイニングは、同じ手法を使うことが多いです。機械学習の勉強をすれば、データマイニングの勉強にもなります。このテキストでは、機械学習とデータマイニングを区別せずに「機械学習」として扱っています。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/data_mining.png?raw=true\" alt=\"data mininng\" height=\"300px\">"]},{"cell_type":"markdown","metadata":{"id":"28Yhuao-qhFY"},"source":["### 機械学習の種類"]},{"cell_type":"markdown","metadata":{"id":"OwsbMnaPmHT_"},"source":["　大きく分けて3つあります。\n","\n","- **教師あり学習 Supervised learning**  \n","ラベル付きのデータを学習して、新しいデータに対する予測モデルを立てる。\n","\n","- **教師なし学習 Unsupervised learning**  \n","ラベルのないデータを学習して、クラスタリングや次元削減をおこなう。\n","\n","- **強化学習 Reinforcement learning**  \n","試行とその結果のフィードバックを繰り返して、性能を良くしていく。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/machine_learning_types.png?raw=true\" alt=\"Machine Learning 3-Types\" height=\"250px\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pQQgC6d9rHDp"},"source":["### 教師あり学習 Supervised learning"]},{"cell_type":"markdown","metadata":{"id":"hFVvJVJKFGdr"},"source":["\n","　**ラベル（出力値）**によって、大きく **分類**と**回帰** に分かれます。使われる技法もそれぞれ異なります。\n","\n","- 分類 Classification  \n","ラベルが離散値データ（種Aか種Bか、スパムメールかそうでないか、など）の場合の学習。  \n","  - ロジスティック回帰\n","  - サポートベクトルマシン\n","  - 決定木\n","  - ランダムフォレスト\n","  - ニューラルネットワーク　など\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/classification2.png?raw=true\" alt=\"classification\" height=\"160px\">\n","\n","- 回帰 Regression  \n","ラベルが連続値データ（収穫量や草丈など）の場合の学習。\n","  - 線形回帰\n","  - サポートベクトル回帰\n","  - 回帰木\n","  - ランダムフォレスト回帰\n","  - ニューラルネットワーク　など\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/regression2.png?raw=true\" alt=\"regression\" height=\"180px\">\n","\n","　今回、「線形回帰」の基礎とそのプログラミングをおこなっていきます。"]},{"cell_type":"markdown","metadata":{"id":"APHpyc9WrttL"},"source":["### 「教師あり学習」の手順"]},{"cell_type":"markdown","metadata":{"id":"3qVOouQseCpk"},"source":["\n","\n","1. 前処理: **トレーニングデータ（training data）** と **テストデータ（test data）** を準備する。\n","2. 選択: 使用するモデルや設定値（**ハイパーパラメータ**）を選択する。\n","3. 学習: トレーニングデータを使って、モデルを学習させて、予測モデルを構築する。\n","4. 評価: テストデータを使って、学習済みの予測モデルを評価する。\n","  - 結果が良くなければ、2へ戻る。\n","  - 結果が良ければ、5へ進む。\n","5. 予測・解釈: 予測モデルを新しいデータに適用する。または、予測モデルを解釈する。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/supervised_learning_process.png?raw=true\" alt=\"supervised_learning_process\" height=\"60px\">\n","\n","　今回の実習では、この機械学習の流れを学んでいきます。"]},{"cell_type":"markdown","metadata":{"id":"NS1uQZ7psDWI"},"source":["### 機械学習ライブラリ scikit-learn\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CLEnCaXRvLcz"},"source":["\n","　Pythonには、機械学習のためのライブラリ [**scikit-learn**](https://scikit-learn.org/stable/)（sklearn）があります。このライブラリのおかげで、機械学習内部のアルゴリズムをコーディングせずに、各種の機械学習をおこなえます。\n","\n","　次のような3行のコードをこれから何度か使います。\n","\n","```python\n","from sklearn.機能 import 関数\n","モデル変数 = 関数（）\n","モデル変数.fit()\n","```"]},{"cell_type":"markdown","metadata":{"id":"cUijXJo0q5in"},"source":["---\n","\n","## 今回の実習内容\n","\n","1. 線形回帰 Linear regression（1変数）"]},{"cell_type":"markdown","metadata":{"id":"HhiREQJGpwnr"},"source":["### 線形回帰モデル\n"]},{"cell_type":"markdown","metadata":{"id":"czigWGYAp1uO"},"source":["　ここでおこなう解析の目標は次のとおりです。\n","> 遺伝子発現量から表現型値を予測する線形回帰モデルを作る\n","\n","　「線形回帰モデル」とは何か？　遺伝子発現量（$x$）と表現型値（$y$）の関係を次の方程式で表したモデルのことです。\n","\n","線形回帰モデルの方程式: \n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/09_statistics/data/regression_base.png?raw=true\" alt=\"regression\" height=\"130px\">\n","\n","- **目的変数（objective variable）**: 予測される変数$y$。今回の場合、表現型値。\n","- **説明変数（explanatory variable）**: 予測に使う変数$x$。今回の場合、各遺伝子発現量。\n","- **偏回帰係数（coefficient）**: 各説明変数の重み。目的変数の予測にその変数がどれぐらい影響するかを示す指標。\n","- **誤差（intercept; 切片）**: 説明変数以外の影響を示す項。\n","\n","　偏回帰係数や誤差の値は、データから推定します。「線形回帰モデルを作る」とは、それらの推定値を得ることに相当します。"]},{"cell_type":"markdown","metadata":{"id":"aOaYdFcfbFMX"},"source":["### 実習で使用するデータセット\n","\n","　次のコードセルを実行して、データファイル（[gene_expression.csv](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/gene_expression.csv)）をダウンロードしてください。\n","\n","ファイルの詳細:\n","- ファイル名: gene_expression.csv\n","- カンマ区切りテキストファイル\n","- 100行（100サンプル） x 51列（表現型値 + 50個の遺伝子発現量）\n","\n","今回の実習では、3つの列データのみ使います。\n","- 表現型値 `phenotype`\n","- 遺伝子11の発現量 `gene_11` （例題で使用する）\n","- 遺伝子29の発現量 `gene_29` （実習問題で使用）"]},{"cell_type":"code","metadata":{"id":"xKU2jUPHbXDV"},"source":["### このコードセルは実行のみ ###\n","# サンプルデータのダウンロード\n","!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/data/gene_expression.csv\n","\n","# pandasで読み込み\n","import pandas as pd\n","df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D9WLjff4LZkp"},"source":["## 1. 線形回帰 Linear regression（1変数）\n","\n","　最初に、線形回帰の全体像を学ぶために、説明変数をひとつだけ使って線形回帰をおこないます。使用する説明変数は「gene_11の遺伝子発現量」です。\n","\n","> gene_11の遺伝子発現量から表現型値（phenotype）を予測する線形回帰モデル（線形回帰直線）を作る\n","  $$ y = \\beta_{gene\\_11} x_{gene\\_11} + e $$\n","\n","　次の5つのステップで、線形回帰モデルを使った機械学習をひととおり勉強しましょう。\n","- 1-1. 前処理\n","- 1-2. モデルの選択\n","- 1-3. モデルの学習\n","- 1-4. モデルの評価\n","- 1-5. 予測\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/supervised_learning_process.png?raw=true\" alt=\"supervised_learning_process\" height=\"60px\">"]},{"cell_type":"markdown","metadata":{"id":"RzhpxlGdo6YG"},"source":["### 1-1. 前処理\n","\n","- 説明変数、目的変数の準備\n","  - 説明変数: gene_11の遺伝子発現量 `gene_11`\n","  - 目的変数: 表現型値 `phenotype`\n","- データセットの分割\n","  - **トレーニングデータ**: 偏回帰係数や誤差を推定するためのデータ\n","  - **テストデータ**: 出来上がったモデルを評価するためのデータ\n","\n","```python\n","# データセットの分割\n","from sklearn.model_selection import train_test_split\n","train_test_split(説明変数, 目的変数, test_size=分割比率)\n","```\n","\n"]},{"cell_type":"code","metadata":{"id":"fFHmBvnQz455"},"source":["# 説明変数、目的変数の準備\n","import numpy as np\n","x = np.array(df[\"gene_11\"])    # 説明変数\n","y = np.array(df[\"phenotype\"])  # 目的変数\n","\n","# 確認\n","print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVyIioYKpqhJ"},"source":["# データセットの分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n","# test_sizeで分割比率を制御する。\n","\n","from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n","\n","# 各データのサイズ\n","print(\"training: \", x_train.shape, y_train.shape)\n","print(\"test: \", x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WibUnNfo1EZO"},"source":["それぞれの変数には以下の情報が代入されます。\n","- `x_train`: トレーニングデータの説明変数\n","- `x_test`: テストデータの説明変数\n","- `y_train`: トレーニングデータの目的変数\n","- `y_test`: テストデータの目的変数"]},{"cell_type":"markdown","metadata":{"id":"5bs0kRCexZ2_"},"source":["　散布図を書いて、データを眺めてみましょう。"]},{"cell_type":"code","metadata":{"id":"Reg98YRRxlHb"},"source":["# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(x_train, y_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(x_test, y_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"expression of gene11\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KgO8H5lOLm2n"},"source":["### データセットの分割の比率"]},{"cell_type":"markdown","metadata":{"id":"fLWo3L3bLtbk"},"source":["　分割の比率をどのように設定するかについては、厳密に決まっていません。一概には言えませんが、トレーニングデータが多くすれば、より多くの新しいデータに対応できるモデルを構築できるようになります。一方で、予測モデルの評価用のテストデータが少なくなり、評価精度が落ちます。\n","\n","　サンプル数100~10,000程度のデータセットでは、トレーニングデータ : テストデータを「70% : 30%」や、「75% : 25%」、「80% : 20%」の比率で分割するのが一般的です。サンプル数100,000では「90% : 10%」、1,000,000では「99% : 1%」といった比率が一般的なようです。ただし、説明変数の個数なども考慮する必要があるので注意が必要です。"]},{"cell_type":"markdown","metadata":{"id":"LW-g1fEoFemm"},"source":["### 実習1-1\n","\n","　`train_test_split`関数を使用して、100サンプルのデータを分割して、トレーニングデータ 80サンプル、テストデータ 20サンプルを準備してください。\n","\n","```python\n","from sklearn.model_selection import train_test_split\n","train_test_split(説明変数, 目的変数, test_size=分割比率)\n","```\n","\n","　なお、この実習問題では「gene_29の遺伝子発現量」を使います。\n","- 説明変数: gene_29の遺伝子発現量 `gene_29`\n","- 目的変数: 表現型値 `phenotype`\n","\n","  $$ y = \\beta_{gene\\_29} x_{gene\\_29} + e $$\n","\n"]},{"cell_type":"code","metadata":{"id":"l93zFzawWh0E"},"source":["# 説明変数、目的変数の準備\n","import numpy as np\n","xx = np.array(df[\"gene_29\"])    # 説明変数\n","yy = np.array(df[\"phenotype\"])  # 目的変数\n","\n","# ============== 編集エリア(start) =============\n","# データセットの分割\n","from sklearn.model_selection import train_test_split\n","xx_train, xx_test, yy_train, yy_test = \n","# ============== 編集エリア(end) ==============\n","\n","# 各データのサイズ\n","print(\"training: \", xx_train.shape, yy_train.shape)\n","print(\"test: \", xx_test.shape, yy_test.shape)\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(xx_test, yy_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"expression of gene29\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iRLPUdXtUenk"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"Jmd7uvU0UYWZ"},"source":["# 説明変数、目的変数の準備\n","import numpy as np\n","xx = np.array(df[\"gene_29\"])    # 説明変数\n","yy = np.array(df[\"phenotype\"])  # 目的変数\n","\n","# ============== 編集エリア(start) =============\n","# データ分割\n","from sklearn.model_selection import train_test_split\n","xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.2)\n","# ============== 編集エリア(end) ==============\n","\n","# 各データのサイズ\n","print(\"training: \", xx_train.shape, yy_train.shape)\n","print(\"test: \", xx_test.shape, yy_test.shape)\n","\n","# グラフ\n","import matplotlib.pyplot as plt\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.scatter(xx_test, yy_test, color=\"orange\", label=\"test\") # テストデータ\n","plt.xlabel(\"expression of gene29\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOtHFnMVJWFg"},"source":["### 1-2. モデルの選択\n","\n","　モデルの選択肢は、「線形回帰」以外にもあります。ここでは、この節のタイトル通りに「線形回帰」を使いましょう。\n","\n","```python\n","# 線形回帰モデルを準備する\n","from sklearn.linear_model import LinearRegression\n","モデル変数 = LinearRegression()\n","```\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Igxlgw9TIBSq"},"source":["# 線形回帰モデルの準備\n","from sklearn.linear_model import LinearRegression\n","lr = LinearRegression()\n","print(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bgr4p2Q9IE-B"},"source":["　次の「1-3. モデルの学習」に進む前に、モデルの学習で何をするかを確認しておきましょう。\n","\n","　x-y座標上には、直線（線形回帰モデル $ y = \\beta x + e $）を無数に作成できます。良いモデルは、説明変数$x$ と目的変数$y$ の関係を\"うまく\"表現している直線です。そのモデルは、係数（傾き）$\\beta$ と誤差（切片）$e$ がどんな値のときでしょうか？ \n","\n","<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3dgasEpHVokz9CEyraRAKjhCezObcJrcZqN4OhhSb8LsA_SMx33ryqb8ZQGT7fjo1rcFckxW8FFxUajOqRAlS15kQUR_3YLeBn06K9O6R_Cb-LWsen9QoNVrFQkCRGoNljS98Tl-jbvWUfknY31U9cc=w1658-h552-no?authuser=0\" alt=\"which lines\" height=\"200px\">\n","\n","　このあと、トレーニングデータ（散布図の青色の点）を使ってモデルの学習をおこない、$\\beta_{gene\\_11}$と$e$の最適値を求めます。テストデータ（オレンジ色の点）はまだ使いません。\n"]},{"cell_type":"markdown","metadata":{"id":"SFNqonlG9bTI"},"source":["### 1-3. モデルの学習\n","\n","　トレーニングデータを使ってモデルの学習をおこない、その学習結果を確認してみましょう。\n","\n","1. トレーニングデータを使って学習（モデルのフィッティング）をおこない、 係数$\\beta_{gene\\_11}$（傾き） と 誤差$e$（切片）の推定値を求めます。\n","```python\n","# モデルの学習\n","モデル変数.fit(トレーニングデータの説明変数, トレーニングデータの目的変数)\n","```\n","\n","1. 学習の結果、係数$\\beta_{gene\\_11}$ と 誤差$e$を確認します。\n","```python\n","# 学習済みの線形回帰モデルから係数と誤差を取り出す\n","モデル変数.coef_      # 係数\n","モデル変数.intercept_  # 誤差\n","```"]},{"cell_type":"code","metadata":{"id":"vgepe_awHrEf"},"source":["# モデルを学習\n","x_train = x_train.reshape(-1, 1)  # 整形（説明変数がひとつの場合に必要）\n","lr.fit(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oou4TSt0KTlA"},"source":["# 学習済みの線形回帰モデルから係数と誤差を取り出す\n","b = lr.coef_\n","e = lr.intercept_\n","\n","# 表示\n","print(\"Coefficient=\", b) # 係数b　（傾き）\n","print(\"Intercept=\"  , e) # 誤差e （切片）"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y7rDcCzTfOQU"},"source":["　散布図に学習済み線形回帰モデルの直線 $y=\\beta_{gene\\_11} x+e$ を追加してみましょう。\n","\n","<small>※ このコードの説明は省略いたしません。</small>"]},{"cell_type":"code","metadata":{"id":"AJOsfbgc-sPy"},"source":["# トレーニングデータの各xについて、予測値（直線上のyの値）を得る\n","y_pred = lr.predict(x_train)\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(x_train, y_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"expression of gene11\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","plt.plot(x_train, y_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend() # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dS3vxjdp28Zn"},"source":["### 実習1-2 & 1-3\n","\n","　実習1-1で準備したトレーニングデータを使って、説明変数（gene_29の遺伝子発現量）から目的変数（表現型値）を予測する線形回帰モデルを作ってください。\n","- トレーニングデータの説明変数: `xx_train`に代入されています\n","- トレーニングデータの目的変数: `yy_train`に代入されています\n","\n","```python\n","# 線形回帰モデルの準備\n","from sklearn.linear_model import LinearRegression\n","モデル変数 = LinearRegression()\n","\n","# モデルの学習\n","モデル変数.fit(トレーニングデータの説明変数, トレーニングデータの目的変数)\n"]},{"cell_type":"code","metadata":{"id":"nAQVqmdNnnVa"},"source":["# 整形（説明変数がひとつの場合に必要）\n","xx_train = xx_train.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 線形回帰モデルの準備\n","from sklearn.linear_model import LinearRegression\n","lr2 = \n","\n","# モデルの学習\n","lr2\n","# ============== 編集エリア(end) ==============\n","\n","# 学習済み線形回帰モデルの係数と誤差を表示\n","print(\"Coefficient=\", lr2.coef_)   # 係数b （傾き）\n","print(\"Intercept=\"  , lr2.intercept_) # 誤差e (切片)\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"expression of gene29\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","yy_pred = lr2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CnW9CaqPnmFY"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"PXrKLx_V-Gx_"},"source":["# 整形（一次元配列の場合に必要）\n","xx_train = xx_train.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 線形回帰モデルの準備\n","from sklearn.linear_model import LinearRegression\n","lr2 = LinearRegression()\n","\n","# モデルの学習\n","lr2.fit(xx_train, yy_train)\n","# ============== 編集エリア(end) ==============\n","\n","# 学習済み線形回帰モデルの係数と誤差を表示\n","print(\"Coefficient=\", lr2.coef_)   # 係数b （傾き）\n","print(\"Intercept=\"  , lr2.intercept_) # 誤差e (切片)\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"expression of gene29\") # x軸ラベル\n","plt.ylabel(\"phenotype value\")      # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","yy_pred = lr2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"itwAzqgET4O0"},"source":["### 線形回帰で得られる直線\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OJYeev8hT7Dl"},"source":["　線形回帰モデルの学習で得られる直線はどのような直線か？　\n","\n","　各トレーニングデータについて、実測値と予測値のずれ「yの値の差分」（残差）が小さければ説明変数と目的変数の関係を\"うまく\"表現している直線と言えるでしょう。\n","\n","　今回おこなった線形回帰モデルの学習では、残差の二乗の合計値、**残差平方和 (residual sum of squares)** が最も小さくなる係数 $\\beta$ と誤差 $e$ の直線を求めています。\n","\n","<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3eIFmh8PDRx64eFArwdgxO2CGt3PEi272ny1dyqAMue0un_yL_GMgZ0CsyvBnX4lEC9BfOEdfTNGsiEG-R4xZDPM9zMHwHcINcnQFxcdTmSgsF7LotLsBpwzs0S49fZtN1fQrbHY7JrB9m2kwuDGb9r=w815-h560-no?authuser=0\" alt=\"least squares\" height=\"180px\">\n","\n","$$ 残差平方和: \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 = \\sum_{i=1}^{N} (\\beta x_i + e - y_i)^2 $$\n","\n","  - $x_{i}$: i番目のトレーニングデータの説明変数の値\n","  - $y_{i}$: i番目のトレーニングデータの目的変数の値（実測値）\n","  - $\\hat{y}_{i}$: i番目のトレーニングデータの目的変数の値（予測値）\n","  - $N=1,2,3,...,n$: データの個数\n","\n","　残差平方和のように、最小化を目指す関数のことを、**目的関数 objective function** や **コスト関数 cost function** と呼びます。  \n","　機械学習（教師あり学習）の各アルゴリズムは、何らかの目的関数を持っています。トレーニングデータによる学習で、目的関数の最小化（ときには最大化）をおこないます。\n","\n","<small>※ 線形回帰モデルでパラメータ（係数や誤差）の最適値を求める方法は、「係数$\\beta$ と 誤差$e$ の最適値をどうやって求めているか？」を参照してください。</small>\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c_ajWotnP61f"},"source":["### 係数$\\beta$ と 誤差$e$ の最適値をどうやって求めているか？"]},{"cell_type":"markdown","metadata":{"id":"YCv6TYh-UBNj"},"source":["　scikit-learnの `LinearRegression()` は、**最小二乗法 least squares method**と呼ばれる方法を使って、残差平方和が最小値（極小値）になる係数$\\beta$ と誤差$e$ を求めています。最小二乗法の概要は次のとおりです。\n","\n","1. 目的関数（残差平方和）\n","$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 = \\sum_{i=1}^{N} (\\beta x_i + e - y_i)^2 $$\n","  - $x_{i}$: i番目のトレーニングデータの説明変数の値\n","  - $y_{i}$: i番目のトレーニングデータの目的変数の値（実測値）\n","  - $\\hat{y}_{i}$: i番目のトレーニングデータの目的変数の値（予測値）\n","  - $N=1,2,3,...,n$: データの個数\n","  \n","1. 目的関数の$\\sum$の部分を展開すると、\n","$$ \\sum_{i=1}^{N} (\\beta x_i + e - y_i)^2 = \\beta^{2}\\sum_{i=1}^{N}{x_{i}^{2}} + Ne^2 + \\sum_{i=1}^{N}{y_{i}^{2}} - 2\\beta\\sum_{i=1}^{N}{x_{i}y_{i}} + 2\\beta e\\sum_{i=1}^{N}{x_{i}} -2e\\sum_{i=1}^{N}{y_{i}} $$\n","\n","1. 上の展開式を$\\beta$の関数として考えると、下に凸な二次関数であることがわかります。\n","$$ \\sum_{i=1}^{N} (\\beta x_i + e - y_i)^2 = \\Bigl(\\sum_{i=1}^{N}{x_{i}^{2}} \\Bigr) \\beta^{2} - \\Bigl( 2\\sum_{i=1}^{N}{x_{i}y_{i}} + 2 e\\sum_{i=1}^{N}{x_{i}} \\Bigr) \\beta + \\Bigl( Ne^2 + \\sum_{i=1}^{N}{y_{i}^{2}}　-2e\\sum_{i=1}^{N}{y_{i}} \\Bigr) $$\n","同様に、$e$の関数として考えた場合も下に凸な二次関数となります。したがって、残差平方和が最小となる$\\beta$と$e$は、両者の関数が同時に極小値となるときの値と一致しそうです。\n","\n","1. そこで、$\\beta$や$e$についての偏微分関数が0になる値（極小値）を調べます。\n"," $$ (\\beta について偏微分)=0 $$\n"," $$ (e について偏微分)=0 $$\n","1. 上の2つの方程式を解くと、目的関数が極小値となる$\\beta$や$e$の値が求まります\n","\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/least_squares_method.png?raw=true\" alt=\"least squares method\" height=\"180px\">\n"]},{"cell_type":"markdown","metadata":{"id":"d-k6VE5eAOVe"},"source":["### scikit-learnでよくおこなう3行\n"]},{"cell_type":"markdown","metadata":{"id":"jjiQltD1CTaV"},"source":["　scikit-learnの基本構文は、この3行です。\n","\n","```python\n","from sklearn.機能 import 関数 \n","モデル変数 = 関数(オプション)\n","モデル変数.fit(x, y)\n","```\n","\n","　上でおこなった「線形回帰」だけでなく、他の技法（「決定木」や「ランダムフォレスト」、「サポートベクトルマシン」を使った回帰、など）も3行でおこなえます。\n","```python\n","# 線形回帰\n","from sklearn.linear_model import LinearRegression\n","model = LinearRegression()\n","model.fit(x, y)\n","\n","# 決定木を使った回帰（回帰木）\n","from sklearn.tree import DecisionTreeRegressor\n","model = DecisionTreeRegressor()\n","model.fit(x, y)\n","\n","# ランダムフォレストを使った回帰\n","from sklearn.ensemble import RandomForestRegressor\n","model = RandomForestRegressor()\n","model.fit(x, y)\n","\n","# サポートベクトルマシンを使った回帰（サポートベクトル回帰）\n","from sklearn.svm import SVR\n","model = SVR()\n","model.fit(x, y)\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"qvn3oSE5Ht2t"},"source":["### 1-4. モデルの評価\n","\n","　次に、トレーニングデータの学習で得られたモデルの予測精度を評価する方法についてです。\n","\n","　回帰モデルの評価はいくつかあります。ここでは、**決定係数 $R^2$ (R-squared; coefficient of determination)** と呼ばれる評価指標を用いて、学習で得られたモデルを評価してみましょう。\n","\n","$R^{2}$について:\n","- 予測モデルの当てはまりの良さを示す指標。\n","- 0に近づくほど当てはまりが悪く、1に近づくほど当てはまりが良い。予測精度があまりに悪いと、マイナスの値をとることもある。\n","- $R^2$の式は下記のとおり。\n","  - $y_{i}$: $i$番目のサンプルの実測値\n","  - $\\bar{y}$: 実測値の平均値\n","  - $\\hat{y}_{i}$: $i$番目のサンプルの予測値\n","  - $m$: サンプル数\n","\n","  $$ R^{2} = 1 - \\frac{\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})^2}}{\\sum_{i=1}^{m}{(\\bar{y} - y_{i})^2}} $$\n","\n","  \n"]},{"cell_type":"markdown","metadata":{"id":"tU4wx9bjVhhM"},"source":["　トレーニングデータとテストデータ両方の$R^2$を調べてみましょう。\n","\n","```python\n","# トレーニングデータの決定係数R2\n","モデル変数.score(トレーニングデータの説明変数, トレーニングデータの目的変数)\n","# テストデータの決定係数R2\n","モデル変数.score(テストデータの説明変数, テストデータの目的変数)\n","```"]},{"cell_type":"code","metadata":{"id":"4GW7Z5UWVfhc"},"source":["# 整形: テストデータのx\n","x_test = x_test.reshape(-1, 1)\n","\n","# 決定係数R2\n","r2_train = lr.score(x_train, y_train)\n","r2_test = lr.score(x_test, y_test)\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cO3Jh_vBXT9_"},"source":["　トレーニングデータ、テストデータともに決定係数が低く、この予測モデルの当てはまりは悪いことがわかります。"]},{"cell_type":"markdown","metadata":{"id":"9EALrfzhHxAk"},"source":["### 実習1-4\n","\n","　実習1-3で得られた線形回帰モデルを「決定係数$R^2$」で評価してみましょう。トレーニングデータとテストデータの両方の$R^2$を求めてください。\n","- 予測モデル: `lr2`に学習済みのモデルが入っています\n","- トレーニングデータ: `xx_train`や`yy_train`に代入されています\n","- テストデータ: `xx_test`や`yy_test`に代入されています\n","\n","```python\n","# トレーニングデータの決定係数R2\n","モデル変数.score(トレーニングデータの説明変数, トレーニングデータの目的変数)\n","# テストデータの決定係数R2\n","モデル変数.score(テストデータの説明変数, テストデータの目的変数)\n","```"]},{"cell_type":"code","metadata":{"id":"QV2qAcWkHumU"},"source":["# 整形（説明変数がひとつの場合に必要）\n","xx_test = xx_test.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 決定係数R2\n","r2_train = \n","r2_test = \n","# ============== 編集エリア(end) ==============\n","\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Sb7dh-mIClg"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"pIvRBcDbIKWx"},"source":["# 整形（説明変数がひとつの場合に必要）\n","xx_test = xx_test.reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 決定係数R2\n","r2_train = lr2.score(xx_train, yy_train)\n","r2_test = lr2.score(xx_test, yy_test)\n","# ============== 編集エリア(end) ==============\n","\n","print(\"training: \", r2_train)\n","print(\"test: \"    , r2_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R47c_LFtQ9xD"},"source":["### 決定係数$R^{2}$のもう少し詳しい説明"]},{"cell_type":"markdown","metadata":{"id":"-bcfWZU-RDpr"},"source":["  $$決定係数 R^{2} = 1 - \\frac{\\sum_{i=1}^{m}{(\\hat{y}_{i} - y_{i})^2}}{\\sum_{i=1}^{m}{(\\bar{y} - y_{i})^2}} $$\n","\n","　$R^{2}$の分数の部分の説明です。分母は「サンプルの分散（ばらつき）」を表しています。分子は「残差平方和（実測値と予測値がどれだけ離れているかの指標）」です。\n","\n","　分数の値が1に近いと（$R^{2}$が0に近いと）、サンプルのばらつきと残差平方和はほぼ同じで、各点は予測モデルから大きく離れていることになります。一方で、分数の値が0に近いと（$R^{2}$が1に近いと）、各点は予測モデルによくフィットしていることになります。"]},{"cell_type":"markdown","metadata":{"id":"gw4sP5gIn-ZA"},"source":["### 決定係数$R^{2}$は高ければ良いのか？"]},{"cell_type":"markdown","metadata":{"id":"f989FhcOfWnF"},"source":["　決定係数$R^{2}$は、データの「当てはまりの良さ」を表す指標です。予測モデル構築の目的が「新しいデータのyの値を予測したい」場合、トレーニングデータとテストデータの両方で決定係数が高いことが重要です。\n","\n","　例えば、下図のように、シンプルなモデル（赤線）と複雑なモデル（青線）があったとします。複雑なモデルの方は、トレーニングデータに対する決定係数は非常に高いですが、新しいデータに対する決定係数は低く、予測モデルとして役立ちません。こういう状態を **過学習（オーバーフィッティング overfitting）** と呼びます。一方、シンプルなモデルは、トレーニングデータやテストデータに対する決定係数はそこそこですが、複雑なモデルよりは現実的な予測モデルとして使えるでしょう。\n","\n","<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/10_statistics/data/overfit.png?raw=true\" alt=\"overfit\" height=\"300px\">\n","\n","　また、データマイニング的な目的「yの値に影響を持っている変数xを調べたい」であれば、決定係数といった指標ですべて評価することは必ずしも適切ではありません。\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3oyGyC-GHbf4"},"source":["### 1-5. 予測\n","\n","　あまり良いモデルではありませんが、得られた予測モデルを使って、新しいデータ（目的変数yの値が未知のデータ）の予測値を調べてみましょう。\n","\n","```python\n","# 新しいデータの予測値を得る\n","モデル変数.predict(新しいデータ)\n","```\n","\n","　ここでは、3個の新しいデータの予測値を調べます。\n","\n","<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3dF-8_d2r30aDjXFZ-C5kkHaLLff6GQCgYzDQoX_SPbF59M1K_3UAatY18kUDw_AXRx4OlLCUXfDoKiaTbFt2Ag8WNOso8m88T1R6R1uPadQwEhCC67QBV2XefcMLe7zgoLp9r3FwaEVJLUQMxOp02w=w909-h641-no?authuser=0\" alt=\"predict\" height=\"180px\">"]},{"cell_type":"code","metadata":{"id":"WDPreSm_Zrsc"},"source":["# 新しいデータ + 整形（説明変数がひとつの場合に必要）\n","x_new = np.array([5.0, 7.5, 9.0])\n","x_new = x_new.reshape(-1, 1)\n","\n","# 予測\n","y_new = lr.predict(x_new)\n","print(y_new)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gjZsS_HBa-YQ"},"source":["# 散布図（トレーニングデータのみ）\n","plt.scatter(x_train, y_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","y_pred = lr.predict(x_train)  # 直線上のyの値を求める\n","plt.plot(x_train, y_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","# 予測値\n","plt.scatter(x_new, y_new, color=\"red\", label=\"new\")\n","\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VZmW7vkacUy6"},"source":["### 実習1-5\n","\n","　実習1-3で得られた線形回帰モデルを使って、新しいデータの予測値を調べてください。\n","\n","- 予測モデル: `lr2`に学習済みのモデルが入っています\n","\n","```python\n","# 新しいデータの予測値を得る\n","モデル変数.predict(新しいデータ)\n","```"]},{"cell_type":"code","metadata":{"id":"Mr6-cEGacwSc"},"source":["# 新しいデータ + 整形（説明変数がひとつの場合に必要）\n","xx_new = np.array([9.0, 10.5, 12.8]).reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 予測\n","yy_new = \n","print(yy_new)\n","# ============== 編集エリア(start) =============\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","yy_pred = lr2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","# 予測値\n","plt.scatter(xx_new, yy_new, color=\"red\", label=\"new\")\n","\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NBOA7BR9d3fB"},"source":["#### 解答例"]},{"cell_type":"code","metadata":{"id":"ssmwgp21d5Kh"},"source":["# 新しいデータ + 整形（説明変数がひとつの場合に必要）\n","xx_new = np.array([9.0, 10.5, 12.8]).reshape(-1, 1)\n","\n","# ============== 編集エリア(start) =============\n","# 予測\n","yy_new = lr2.predict(xx_new)\n","print(yy_new)\n","# ============== 編集エリア(start) =============\n","\n","# 散布図（トレーニングデータのみ）\n","plt.scatter(xx_train, yy_train, color=\"blue\", label=\"train\") # トレーニングデータ\n","plt.xlabel(\"x\")              # x軸ラベル\n","plt.ylabel(\"y\", rotation=0)  # y軸ラベル\n","\n","# 学習済み線形回帰モデルの直線\n","yy_pred = lr2.predict(xx_train)  # 直線上のyの値を求める\n","plt.plot(xx_train, yy_pred, color=\"blue\", alpha=0.5, label=\"model\")\n","\n","# 予測値\n","plt.scatter(xx_new, yy_new, color=\"red\", label=\"new\")\n","\n","plt.legend()                # 凡例\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L-YywDR48RdG"},"source":["---\n","\n","## まとめ\n","\n","　今回の実習では、機械学習の概要を学びました。また、機械学習手法のひとつである **線形回帰** の基礎と機械学習の流れを勉強しました。\n","\n","　線形回帰の内部で使われている計算（最小二乗法の計算: 係数や誤差の最適値を求める計算）を自身でコーディングしようとすると、非常に手間がかかります。**scikit-learn**ライブラリを使うことで、少量のコーディングで線形回帰モデルを構築できます。\n","\n","　今回おこなった5ステップ（**前処理 → 選択 → 学習 → 評価 → 予測**）は、機械学習の基本的な手順です。\n","```python\n","# 前処理\n","from sklearn.model_selection import train_test_split\n","train_test_split(説明変数, 目的変数, test_size=分割比率)\n","\n","# モデルの選択\n","from sklearn.linear_model import LinearRegression\n","モデル変数 = LinearRegression()\n","\n","# モデルの学習\n","モデル変数.fit(トレーニングデータ説明変数, トレーニングデータ目的変数)\n","\n","# モデルの評価: 決定係数R2\n","モデル変数.score(トレーニングデータ説明変数, トレーニングデータ目的変数)\n","モデル変数.score(テストデータ説明変数, テストデータ目的変数)\n","\n","# 予測\n","モデル変数.predict(新しいデータ（説明変数の値）)\n","```\n","\n","　今回のテキストでは、説明変数をひとつのみ使い、予測モデルを作りました。決定係数の値が低く、あまり良くないモデルでした。次のテキスト「[機械学習 - 線形回帰（2）-](https://colab.research.google.com/github/CropEvol/lecture/blob/master/textbook_2020/L12_ML_regression_2.ipynb)」で、使用する説明変数の個数を増やして、モデルの改善を試みてみましょう。"]},{"cell_type":"markdown","metadata":{"id":"z9GvmFeOiC6i"},"source":["##### 「機械学習」の勉強におすすめの本\n","- 理論\n","  - [The Hundred-Page Machine Learning Book](http://themlbook.com/)\n","  - [わけがわかる機械学習](https://gihyo.jp/book/2019/978-4-297-10740-6)\n","- Pythonプログラミング\n","  - [Python Machine Learning](https://www.packtpub.com/data/python-machine-learning-third-edition)（原書）/ [Python機械学習プログラミング](https://book.impress.co.jp/books/1117101099)（日本語訳）\n","  - [機械学習のための「前処理」入門](https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%80%8C%E5%89%8D%E5%87%A6%E7%90%86%E3%80%8D%E5%85%A5%E9%96%80-%E8%B6%B3%E7%AB%8B-%E6%82%A0/dp/4865941967)\n","  - [Pythonと実データで遊んで学ぶ データ分析講座](http://www.c-r.com/book/detail/1322)"]}]}