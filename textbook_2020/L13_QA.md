# ご質問・ご意見への回答
---

[ご質問またはご意見]

勾配法のプログラミングについて、学習率が高いほど更新幅が大きくなるという解釈でよろしいでしょうか。トレーニング回数と学習率を大きくすると決定係数の値が減少したのですが、それは何故でしょうか。

[回答]

その解釈で合っています。「学習率」で更新幅を制御しており、学習率を小さくすれば、更新幅が小さくなります。反対に、学習率を大きくすれば、更新幅が大きくなります。

「学習率」（更新幅）があまりに大きいと、更新後のパラメータが極値（optimum; コスト関数の谷の部分）を飛び越えます。場合によっては、極値を目指すような学習がおこなわれずに、パラメータが発散してしまいます（右下図）。

<img src="https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_descent_eta.png?raw=true" alt="eta" height="300px" align="middle">


---

[ご質問またはご意見]

機械学習は人間の脳の学習過程を模倣して作られたと聞きますが、今回ご紹介された過学習という現象は人間の脳にも起こるのでしょうか。

[回答]

 人間の脳で、実際に「過学習」といえる事例があるのかどうか私は知りません。

 人間の脳には、入力データ中のパラメータを上手く取捨選択する機構が備わっているように思います。無関係な情報を多く含むデータ（多数のパラメータをもったデータ）を入力データにして、学習を続けたとしても、最終的には、必要な情報（パラメータ）のみを使ったモデルを上手く構築できているように見受けられます。  
　ただし、入力データ数が少なすぎたり、偏った入力データを使っているために、一般的な評価で "誤ったモデル" を構築することはあると思います。

 「過学習」と言えるかどうかわかりませんが、熟練のスポーツ選手などで起こりうる「イップス」（これまで普通におこなえてきた動作を、あるときから思うようにできなくなる症状）は、過学習に近い事象のように思います。  
　ある失敗を過剰に学習することで、今まで低い影響力しかもたなかったパラメータに大きな影響力を付与してしまい、その結果、イップスが引き起こされているのかもしれません。
　
