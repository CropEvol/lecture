{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L10_ML_gradient_method.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qU2dtMxyz_ID",
        "sRVTfqHq8_BB",
        "ao-qiItunmR8",
        "pBnkaPCsNWGy",
        "EbtzqFi5IOY2",
        "AQ7K2-c1lvgD",
        "JXCiUm9meFfV"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBb8VqFVQVfk"
      },
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3fFHZrzKpHGWl0vYz7Sr8FX8QqLQ_tc8XHBSwqQnM4hgsIOjtjaOde1M9oHSAfe1Fs2SwVORlapit4-JOz0mjP8Tnz6HetkLZDZb8CifSd0uoSp1Nj3wG_wh1sEQlKXXzvEA9Y9HnQqu2Ecv2igmInb=w1097-h235-no?authuser=0\" alt=\"2020年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIfbawIwEw5H"
      },
      "source": [
        "# 機械学習 - 勾配法 -\n",
        "\n",
        "　前回は、scikit-learnの`LinearRegression()`で線形回帰モデルを構築しました。線形回帰モデルの学習では、残差の二乗の合計値、**目的関数**である**残差平方和 (residual sum of squares)** が最も小さくなる係数 $\\beta$ と誤差 $e$ の直線を求めていました。\n",
        "\n",
        "$$ 残差平方和: \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 $$\n",
        "\n",
        "その際、**最小二乗法**と呼ばれる「解析的」な方法でパラメータ（係数 $\\beta$ や誤差 $e$ ）の最適値を求めていました。\n",
        "\n",
        "<small>※ 「解析的」とは、連立方程式や微分等の数式の変形により、厳密解を求めること。</small>\n",
        "\n",
        "　しかし、選んだモデルや手法、目的関数によっては簡単に解の求まる連立方程式などの数式に落とし込めない場合があります。また、数式に落とし込めても計算時間がかかりすぎてしまうこともあります。そうした場合には、「数値的」な方法でパラメータ推定をおこなうことが機械学習では良くあります。\n",
        "\n",
        "<small>※ 「数値的」とは、コンピュータを使って近似解を求めること。</small>\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/bibun.png?raw=true\" alt=\"gradient_method\" height=\"300px\">\n",
        "\n",
        "　今回、**勾配法（Gradient method）**と呼ばれる数値的にパラメータ推定をおこなう方法を学びます。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU2dtMxyz_ID"
      },
      "source": [
        "### 最小二乗法の計算コスト\n",
        "<small>※ 数学（行列）を使ったやや込み入った説明をしています。読み飛ばしてOKです。</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuE4cefrYlkF"
      },
      "source": [
        "　$i$行$j$列のトレーニングデータセットの説明変数を行列$X$で表すと、次のようになります。1行は1サンプル分のデータで、各列は説明変数の値です。また、目的変数$y$も$i$行1列の行列で表すことができ、係数$\\beta$も$j$行1列の行列で表すことができます。\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix}\n",
        "x_{11} & x_{12} & x_{12} & ... & x_{1j} \\\\\n",
        "x_{21} & x_{22} & x_{22} & ... & x_{2j} \\\\\n",
        "x_{31} & x_{32} & x_{32} & ... & x_{3j} \\\\\n",
        " : & : & : & ... & : \\\\\n",
        " x_{i1} & x_{i2} & x_{i2} & ... & x_{ij} \\\\\n",
        "\\end{pmatrix}, \n",
        "y = \\begin{pmatrix}\n",
        "y_{1}  \\\\\n",
        "y_{2} \\\\\n",
        "y_{3} \\\\\n",
        " :  \\\\\n",
        "y_{i} \\\\\n",
        "\\end{pmatrix},  \n",
        "\\beta = \\begin{pmatrix}\n",
        "\\beta_{1}  \\\\\n",
        "\\beta_{2} \\\\\n",
        "\\beta_{3} \\\\\n",
        " :  \\\\\n",
        "\\beta_{j} \\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "　最小二乗法で、残差平方和を最小化する$\\hat{\\beta}$を求めるとき、実際には次のような行列式を解いています。\n",
        "$$ \\hat{\\beta} = (X^{T} X)^{-1} X^{T} y $$\n",
        "\n",
        "　説明変数$j$が多くなると、上述の式の$(X^{T} X)^{-1}$（逆行列）の計算が大規模計算機を用いても困難になってきたり、そもそも求まらなかったりします（時間がかかりすぎるか、メモリ上に数値を保持できなくなります）。\n",
        "\n",
        "　もう少し具体的に言うと、\n",
        "- $X^{T}$（$j\\times i$行列）と$X$（$i\\times j$行列）の積$X^{T} X$により、$j\\times j$行列ができます。この$j\\times j$行列の逆行列を求めることになります。\n",
        "- LU分解を利用して $j\\times j$行列の逆行列を得ようとすると、$j^3$の計算量が必要になります（「[プログラミングのための線形代数](https://www.ohmsha.co.jp/book/9784274065781/)」より）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRVTfqHq8_BB"
      },
      "source": [
        "### 勾配法とは？"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCdARPQSFWXj"
      },
      "source": [
        "　線形回帰モデルは次のような式で表されます。\n",
        "$$y=\\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_m x_m + e $$\n",
        "\n",
        "　最小二乗法と勾配法では、パラメータの求め方がどう違うのか？\n",
        "\n",
        "　最小二乗法は、計算により最小値をピンポイントに見つける方法です。一方で、勾配法は、ある値から出発して、係数$\\beta$や誤差$e$の値を徐々に更新し、最小値に近づいていく方法です。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method.png?raw=true\" alt=\"gradient_method\" height=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao-qiItunmR8"
      },
      "source": [
        "### 勾配法のアルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXLykc5qULf2"
      },
      "source": [
        "　勾配法は、上述したとおり、**ある値から出発して、係数𝛽や誤差𝑒の値を徐々に更新し、最小値に近づいていく方法** です。その方法について、もう少し具体的な話をしましょう。\n",
        "\n",
        "　次の図のように、下に凸なグラフがあり、現在の$\\beta$の値が$\\beta_0$とします。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method_example.png?raw=true\" alt=\"gradient_method_example\" height=\"250px\">\n",
        "\n",
        "　目的関数$Cost(\\beta)$が最小値となる$\\beta_{optimum}$に近づくためには、現在の$\\beta$からどのように値を変化させればいいでしょうか？  \n",
        "> $\\beta$が右側に移るような値を$\\beta_0$に追加する\n",
        "\n",
        "どの程度の値を追加すれば良いでしょうか？\n",
        "> あまり大きすぎない値を追加する。\n",
        "\n",
        "　勾配法では、次のような処理をおこない、パラメータの最適値に近づきます。\n",
        "1. まず、$\\beta_0$の「接線の傾き」を調べます。すなわち、$Cost(\\beta)$を$\\beta$について微分したときの値を調べます。\n",
        "  $$ \\frac{\\partial Cost(\\beta)}{\\partial  \\beta} $$\n",
        "\n",
        "1. その値が負のとき、右方向に移るように$\\beta$の値を更新します。その時の更新幅を、傾きの大きさと**学習率$\\eta$（イータ; eta）**の積で決定します。\n",
        "  $$ \\beta_{1} := \\beta_{0} - \\eta \\frac{\\partial Cost(\\beta)}{\\partial  \\beta} $$\n",
        "\n",
        "1. この1と2のステップを$\\beta$の値が変化しなくなるまで（もしくは、指定の繰り返し数に達するまで）、ひたすら繰り返します。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/gradient_method_algorithm.png?raw=true\" alt=\"gradient_method_algorithm\" height=\"250px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wErzJw62wA9h"
      },
      "source": [
        "### 実習で使用するデータセット\n",
        "\n",
        "　次のコードセルを実行して、データファイル（[gene_expression.csv](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/gene_expression.csv)）をダウンロードしてください。\n",
        "\n",
        "ファイルの詳細:\n",
        "- ファイル名: gene_expression.csv\n",
        "- カンマ区切りテキストファイル\n",
        "- 100行（100サンプル） x 51列（表現型値 + 50個の遺伝子発現量）\n",
        "\n",
        "今回の実習では、一部の列データのみ使います。\n",
        "- 表現型値 `phenotype`\n",
        "- 遺伝子11の発現量 `gene_11` (例題)\n",
        "- 遺伝子7, 11, 29の発現量 `gene_7`,`gene_11`,`gene_29`  (実習問題)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jHa_iFsX2nu"
      },
      "source": [
        "# /// 実習前にこのセルを実行してください ///\n",
        "# データの読み込み\n",
        "!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/data/gene_expression.csv\n",
        "# 動画再生用モジュール\n",
        "!wget -q -O gradient_method.py https://raw.githubusercontent.com/CropEvol/lecture/master/modules/gradient_method.py\n",
        "!apt-get -q install ffmpeg\n",
        "\n",
        "# pandasで読み込み\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0EP9GpPE0Xa"
      },
      "source": [
        "---\n",
        "\n",
        "## 今回の実習内容\n",
        "\n",
        "1. 前処理\n",
        "1. 確率的勾配降下法 Stochastic Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEEV-hVOYPlQ"
      },
      "source": [
        "### 1. 前処理\n",
        "\n",
        "　前回と同じく、勾配法の仕組みを学ぶために、説明変数1個と目的変数1個のみを使います。\n",
        "\n",
        "- 説明変数: 遺伝子11の発現量 `gene_11`（他の遺伝子でも構いません）\n",
        "- 目的変数: 表現型値 `phenotype`\n",
        "\n",
        "> gene_11の遺伝子発現量から表現型値（phenotype）を予測する線形回帰モデルを作る\n",
        "  $$ y = \\beta_{gene\\_11} x_{gene\\_11} + e $$\n",
        "\n",
        "　次のコードセルを実行して、以下の前処理をおこないましょう。\n",
        "1. 説明変数xと目的変数yの準備\n",
        "2. 説明変数のスケーリング\n",
        "3. データのグラフ描画\n",
        "\n",
        "<small>※ 勾配法の仕組みを勉強するだけなので、「トレーニングデータとテストデータの分割」や「モデルの評価」、「新しいデータの予測」は今回おこないません。</small>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-hiw5rp9raA"
      },
      "source": [
        "# 説明変数xとして使うデータ\n",
        "# gene_1からgene_50であれば、どれでもOK（初期値: gene_11）\n",
        "use_col = \"gene_11\"  \n",
        "\n",
        "#===== 以下は変更しないでください =====\n",
        "# 変数x, y\n",
        "import numpy as np\n",
        "x = np.array(df[use_col]).reshape(-1,1)\n",
        "y = np.array(df[\"phenotype\"])\n",
        "\n",
        "# 説明変数xの正規化\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "mms = MinMaxScaler()\n",
        "x = mms.fit_transform(x)\n",
        "\n",
        "# グラフ\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x, y, color=\"blue\", label=\"training data\") # データ\n",
        "plt.xlabel(\"gene expression (normalized)\")  # x軸ラベル\n",
        "plt.ylabel(\"phenotype value\")               # y軸ラベル\n",
        "plt.legend()  # 凡例\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBnkaPCsNWGy"
      },
      "source": [
        "### 参考: 最小二乗法による線形回帰モデルを作った場合…"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp_hM25XSbnO"
      },
      "source": [
        "　scikit-learnの`LinearRegression()`（最小二乗法）で線形回帰モデルを構築すると、次のコードセルの実行結果に表示される係数$\\beta$と誤差$e$が得られます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIaKpaTOTx1-"
      },
      "source": [
        "# =================== 線形回帰 ===================\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# モデル作成・学習\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(x, y)\n",
        "\n",
        "# 学習後の係数と誤差を確認\n",
        "print(\"b=\", model_lr.coef_)\n",
        "print(\"e=\", model_lr.intercept_)\n",
        "\n",
        "# 決定係数\n",
        "print(\"R2=\", model_lr.score(x, y))\n",
        "\n",
        "# =================== グラフ ===================\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 直線を書くためのデータ\n",
        "x_line = np.linspace(np.min(x), np.max(x), num=2)\n",
        "y_line = model_lr.predict(x_line.reshape(-1, 1))\n",
        "\n",
        "# グラフ\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x, y, color=\"blue\", label=\"data\") # データ\n",
        "plt.plot(x_line, y_line, color=\"orange\", label=\"predict\") # 予測モデル\n",
        "plt.xlabel(\"gene expression (normalized)\")  # x軸ラベル\n",
        "plt.ylabel(\"phenotype value\")               # y軸ラベル\n",
        "plt.legend()  # 凡例\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHGXQtmQlByJ"
      },
      "source": [
        "### 2. 確率的勾配降下法 Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKSg4BLslD4A"
      },
      "source": [
        "　**確率的勾配降下法（Stochastic Gradient Descent: SGD）**は、ランダムに選んだ1サンプルを使って勾配計算をおこない、係数$\\beta$や誤差$e$などの**パラメータ（Parameter）**を少しずつ更新する方法です。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWDZU6TIlJzE"
      },
      "source": [
        "1. パラメータ（係数や誤差）の初期値を設定する。通常、「0」または「ごく小さいランダムな数値（小数点数）」が初期値として設定される。\n",
        "1. 以下(1)〜(3)の操作を、指定のトレーニング回数繰り返す。  \n",
        "  (1) データセットからランダムに1サンプル選び、そのデータを使って、勾配計算をおこなう。  \n",
        "  (2) 勾配と学習率をかけた値を、パラメータから引き算し、パラメータ更新をおこなう。\n",
        "  $$ 更新後パラメータ := 更新前パラメータ - 学習率 \\times 勾配$$\n",
        "  (3) (1)と(2)を全サンプルに対しておこなう。\n",
        "\n",
        "\n",
        "　「確率的勾配降下法」を使った線形回帰モデルの構築は、scikit-learnで簡単に実装できます。\n",
        "\n",
        "```python\n",
        "# 確率的勾配降下法による線形回帰モデル\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "モデル変数 = SGDRegressor(max_iter=トレーニング回数, eta0=学習率)\n",
        "モデル変数.fit(説明変数, 目的変数)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUeWDFuYlqUV"
      },
      "source": [
        "# ===== 確率的勾配降下法 =====\n",
        "# モデル作成・学習\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "model_sgd = SGDRegressor(max_iter=100, eta0=0.01, learning_rate=\"constant\")\n",
        "model_sgd.fit(x, y)\n",
        "print(\"b=\", model_sgd.coef_)\n",
        "print(\"e=\", model_sgd.intercept_)\n",
        "\n",
        "# 決定係数\n",
        "r2 = model_sgd.score(x, y)\n",
        "print(\"R2=\", r2)\n",
        "\n",
        "# グラフ\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_line = np.linspace(np.min(x), np.max(x), num=2)\n",
        "y_line = model_sgd.predict(x_line.reshape(-1, 1))\n",
        "\n",
        "fig = plt.figure(figsize=[8, 4])\n",
        "plt.scatter(x, y, color=\"blue\") # データ\n",
        "plt.plot(x_line, y_line, color=\"orange\")\n",
        "plt.xlabel(\"normalized x\")  # x軸ラベル\n",
        "plt.ylabel(\"y\")  # y軸ラベル\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFd99685A8ID"
      },
      "source": [
        "### 実習1\n",
        "\n",
        "　実習問題では、3つの説明変数を使って、次の線形回帰モデルを作ります。\n",
        "> gene_7, 11, 28の遺伝子発現量から表現型値（phenotype）を予測する線形回帰モデルを作る\n",
        "  $$ y = \\beta_{gene\\_7} x_{gene\\_7} + \\beta_{gene\\_11} x_{gene\\_11} + \\beta_{gene\\_28} x_{gene\\_28} + e $$\n",
        "\n",
        "\n",
        "　「確率的勾配降下法」で線形回帰モデルのパラメータ推定（係数 $\\beta$ と 誤差 $e$の値の推定）をおこなってください。その際、トレーニング回数や学習率に適当な値を設定し、決定係数 $R^2>0.5$ を目指してください。\n",
        "\n",
        "```python\n",
        "# 確率的勾配降下法による線形回帰モデル\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "モデル変数 = SGDRegressor(max_iter=トレーニング回数, eta0=学習率)\n",
        "モデル変数.fit(説明変数, 目的変数)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnzNJxIvIVp-"
      },
      "source": [
        "# 使用するデータの準備\n",
        "import numpy as np\n",
        "xx = np.array(df[[\"gene_7\", \"gene_11\", \"gene_28\"]]) # 説明変数\n",
        "yy = np.array(df[\"phenotype\"]) # 目的変数\n",
        "\n",
        "# 説明変数の正規化\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "mms = MinMaxScaler()\n",
        "xx = mms.fit_transform(xx)\n",
        "\n",
        "# --------------- 編集箇所: start ---------------\n",
        "# 学習前のモデルを作成\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "model_sgd2 = SGDRegressor(max_iter=1, eta0=1, learning_rate=\"constant\")\n",
        "# データをセットしてモデルを学習\n",
        "model_sgd2\n",
        "# --------------- 編集箇所: end -----------------\n",
        "\n",
        "# 係数と誤差、決定係数\n",
        "print(\"b=\", model_sgd2.coef_)\n",
        "print(\"e=\", model_sgd2.intercept_)\n",
        "print(\"R2=\", model_sgd2.score(xx, yy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbtzqFi5IOY2"
      },
      "source": [
        "#### 解答例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KLizjo-IQN0"
      },
      "source": [
        "# 変数x, y\n",
        "import numpy as np\n",
        "xx = np.array(df[[\"gene_7\", \"gene_11\", \"gene_28\"]])\n",
        "yy = np.array(df[\"phenotype\"])\n",
        "\n",
        "# 説明変数xの正規化\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "mms = MinMaxScaler()\n",
        "xx = mms.fit_transform(xx)\n",
        "\n",
        "# --------------- 編集箇所: start ---------------\n",
        "# 学習前のモデルを作成\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "model_sgd2 = SGDRegressor(max_iter=1000, eta0=0.05, learning_rate=\"constant\")\n",
        "# データをセットしてモデルを学習\n",
        "model_sgd2.fit(xx, yy)\n",
        "# --------------- 編集箇所: end -----------------\n",
        "\n",
        "# 係数と誤差、決定係数\n",
        "print(\"b=\", model_sgd2.coef_)\n",
        "print(\"e=\", model_sgd2.intercept_)\n",
        "print(\"R2=\", model_sgd2.score(xx, yy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQ7K2-c1lvgD"
      },
      "source": [
        "### 学習過程の一部を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QflqMkZIpPBL"
      },
      "source": [
        "　コードの詳しい解説はおこないませんが、次の2つのコードセルを実行すると、確率的勾配降下法による学習過程の一部を動画にすることができます。\n",
        "\n",
        "<small>*※ 学習過程のデータを動画用データに変換するコードは、こちらに記述しています: [gradient_method.py](https://github.com/CropEvol/lecture/blob/master/textbook_2019/modules/gradient_method.py)の`plot_reg`関数*</small>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RilY-10dlSTb"
      },
      "source": [
        "# 説明変数xとして使うデータ\n",
        "# gene_1からgene_50であれば、どれでもOK（初期値: gene_11）\n",
        "use_col = \"gene_11\"\n",
        "\n",
        "# 確率的勾配降下法の設定値\n",
        "n_iter = 100  # 学習回数\n",
        "eta   = 0.1   # 学習率 \n",
        "\n",
        "# ==========　以下は変更しないでください ==========\n",
        "# ========== 前処理 ==========\n",
        "# 説明変数x、目的変数y\n",
        "import numpy as np\n",
        "x = np.array(df[use_col]).reshape(-1,1)\n",
        "y = np.array(df[\"phenotype\"])\n",
        "\n",
        "# 説明変数xの正規化\n",
        "from sklearn.preprocessing import MinMaxScaler \n",
        "mms = MinMaxScaler()\n",
        "x = mms.fit_transform(x)\n",
        "\n",
        "# ========== モデル作成・学習 ==========\n",
        "from copy import deepcopy\n",
        "# 記録用リスト\n",
        "log_coef = []      # 係数（傾き）\n",
        "log_intercept = [] # 誤差(切片)\n",
        "log_cost = []      # 残差二乗和\n",
        "\n",
        "# モデル作成\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "model_gd = SGDRegressor(eta0=eta, random_state=1, learning_rate=\"constant\")\n",
        "\n",
        "# 設定数の学習を繰り返す\n",
        "for iteration in range(n_iter):\n",
        "  # トレーニングデータをランダムに並べ替える\n",
        "  r = np.random.permutation(len(y))\n",
        "\n",
        "  # 1サンプルずつ学習する\n",
        "  for i in range(0, len(y), 1):\n",
        "    idx = r[i:i+1]  # サンプルのインデックスを取得\n",
        "    x_i = x[idx]       # サンプルのデータを取得\n",
        "    y_i = y[idx]\n",
        "    # 学習\n",
        "    model_gd.partial_fit(x_i, y_i)\n",
        "  \n",
        "  # 記録\n",
        "  log_coef.append(deepcopy(model_gd.coef_))\n",
        "  log_intercept.append(model_gd.intercept_)\n",
        "  cost = ((model_gd.predict(x) - y)**2).sum() / 2.0 / len(y)  # コスト（残差二乗和）の計算\n",
        "  log_cost.append(cost)\n",
        "\n",
        "# ========== 学習後のパラメータと決定係数 ==========\n",
        "# 学習後の係数と誤差を確認\n",
        "print(\"b=\", model_gd.coef_)\n",
        "print(\"e=\", model_gd.intercept_)\n",
        "# 決定係数\n",
        "print(\"R2=\", model_gd.score(x, y))\n",
        "\n",
        "# ========== 動画 ==========\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc, rcParams\n",
        "from IPython.display import HTML\n",
        "from gradient_method import plot_reg\n",
        "rcParams['animation.embed_limit'] = 2**128\n",
        "\n",
        "# パラメータのログを取得\n",
        "b_ = log_coef\n",
        "e_ = log_intercept\n",
        "c_ = log_cost\n",
        "\n",
        "# 動画実行\n",
        "fig = plt.figure(figsize=[16, 4])\n",
        "plt.close()\n",
        "frames = plot_reg(fig, x, y, b_, e_, c_, n_frames=100)\n",
        "ani = animation.ArtistAnimation(fig, frames, interval=100, blit=True)\n",
        "# ani.save('gradient_descent.mp4', writer=\"ffmpeg\")\n",
        "rc('animation', html='jshtml')\n",
        "ani"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXCiUm9meFfV"
      },
      "source": [
        "### おもな勾配法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiJyfvDZJkRk"
      },
      "source": [
        "　「確率的勾配降下法」の他にも勾配法はいくつかあります。代表的なものとして以下の3つが挙げられます。これらの違いは、勾配の計算をおこなうときに使うサンプルセットが異なっている点です。そのほかのアルゴリズムはほとんど同じです。\n",
        "\n",
        "- **確率的勾配降下法 Stochastic Gradient Descent**\n",
        "  - ランダムに選んだ1サンプルのデータを使って勾配計算をおこない、その値を使ってパラメータを更新する\n",
        "  $$ 更新後パラメータ := 更新前パラメータ - 学習率 \\times 勾配$$\n",
        "- **最急降下法（バッチ勾配降下法） Gradient Descent**\n",
        "  - データセットの全サンプルに対して勾配計算をおこない、その平均値でパラメータを更新する。  \n",
        "  $$ 更新後パラメータ := 更新前パラメータ - 学習率 \\times \\frac{1}{N}\\sum 勾配$$\n",
        "\n",
        "- **ミニバッチ勾配降下法 Mini batch Gradient Descent**\n",
        "  - 確率的勾配降下法と最急降下法の中間的な手法。\n",
        "  - ランダムに選んだ複数サンプルのデータセットを使って勾配の計算をおこない、パラメータを更新する\n",
        "\n",
        "\n",
        "　また、以下に、「確率的勾配降下法」と「最急降下法」のおもな違いを表にまとめています。\n",
        "\n",
        "|| 確率的勾配降下法 | 最急降下法 |\n",
        "|---:|---:|---:|\n",
        "| パラメータ更新 | 1サンプル毎 | データセット毎 |\n",
        "| 計算量 | 一定 | サンプル数に依存して多くなる |\n",
        "| 解周辺に辿り着く早さ | 早い | サンプル数に依存して遅くなる |\n",
        "| 1サンプル（外れ値）の影響 | 影響を受けやすい | 平均値を使うので影響が緩和される |\n",
        "| パラメータ更新の方向<br>（学習率が低い場合） | 総じて目標の解の方向に更新されるが、<br>ときどき反対方向にも更新される | 一方向に更新される |\n",
        "| 局所解 | 抜け出せる可能性がある | 抜け出せない |\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/local_optimum.png?raw=true\" alt=\"local_optimum\" height=\"200px\">\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzVyjWKuFg0I"
      },
      "source": [
        "---\n",
        "## まとめ\n",
        "\n",
        "　パラメータの数が膨大な場合やコスト関数の形状が複雑な場合、最小二乗法のような\"解析的\"な手法ではパラメータの最適値（厳密解）を見つけられなくなります。そのようなときに、**勾配法**のような\"数値的\"にパラメータ推定をおこなう方法が役立ちます。\n",
        "\n",
        "　今回、勾配法のうち、**確率的勾配降下法**の概要と実装方法を学びました。 確率的勾配降下法は、ランダムに選んだ1サンプルのデータを使ってパラメータを更新していく方法です。\n",
        "\n",
        "　確率的勾配降下法にはいくつか利点がありますが、とくに重要なのは、**局所解にたどり着いた場合でもそこから抜け出せる可能性がある**点です。これは、パラメータ更新の方向が固定されていない（正の方向に移動することもあれば、負の方向に移動することもある）ためです。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/lr_diff.png?raw=true\" alt=\"gradient_method\" height=\"300px\">\n",
        "\n",
        "[参考:Exploring Stochastic Gradient Descent with Restarts](https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)\n",
        "\n",
        "　人生も、局所解に陥らないために、時々ふだんとは別の方向に歩んでみるのも良いかもしれません（どこに大域解があるかわかりませんが・・・）。\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}