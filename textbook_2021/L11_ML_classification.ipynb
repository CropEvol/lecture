{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L11_ML_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tr3VQJtuPumA",
        "_hsBGAhUQGK7",
        "LqpX0D6FQb2c",
        "DAuQXi-4QPal",
        "TPUF6jU3dfW1",
        "lr67xF2KxVOA",
        "mbFzM-F8-XL0",
        "F8Z-rCOUlhfT",
        "0O9KBzLqhbR8",
        "aYMhW0RGJ1G9",
        "MWU88Zyyjpxt",
        "aGJvLTNhdT7A",
        "R-tImsCQcclG",
        "jqzYBkDSdxFZ",
        "jpCFpMjlk9_U",
        "SMKI4YILgR6q",
        "V4t7glyTeRPy",
        "1MEfY5fbgJbx",
        "92eNbSuDyvVN",
        "K0bRHgtx6g-K",
        "q90BpMEsCfm7",
        "lbRvxEtJRxhb"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DrdvBwenQaTK"
      },
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/pw/AM-JKLVhTn_UySwMdfMwXvoq8l3VN7IkrY9cwtH2YJVMxAlMznUBWC9IpFtgPRIyfAXru4oykkYD-1WjWi0Ao5XgkB9JICvzDBcfn0L_5X2_KOOppsURK5DfSifCC-s7Vx5oQrBUn_BNWn_hfAPdhlVbKQGE=w1097-h235-no?authuser=0\" alt=\"2021年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">\n",
        "\n",
        "<div align=\"right\"><a href=\"https://github.com/CropEvol/lecture#section2\">実習表ページに戻る</a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwcC4dXNwBbj"
      },
      "source": [
        "# 機械学習 - 分類 -\n",
        "\n",
        "　前回までは、目的変数が連続値データのときの機械学習（**回帰; Regression**）をおこないました。\n",
        "\n",
        "　今回、目的変数が離散値データのときの機械学習（**分類; Classification**）をおこないます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/bunrui.png?raw=true\" alt=\"classification\" height=\"300px\">\n",
        "\n",
        "　ここではおもに分類のアルゴリズムをいくつか勉強していきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abOkroywxBzb"
      },
      "source": [
        "## この実習で使用するデータセット\n",
        "\n",
        "　各種の分類アルゴリズムを試すために、次のような[データセット](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/AB_classification.csv)を使います。\n",
        "\n",
        "- 説明変数（これまで説明変数と呼んできましたが、機械学習では、**特徴量（Feature）**と呼ぶことが多いです）\n",
        "  - x1, x2 （2つの遺伝子の発現量）\n",
        "\n",
        "- 目的変数（分類では、**クラス（Class）**や**ラベル（Label）**と呼ぶことが多いです）\n",
        "  - class: C, D\n",
        "\n",
        "- サンプル数（計446サンプル）\n",
        "  - class C: 300サンプル\n",
        "  - class D: 146サンプル\n",
        "\n",
        "<small>※ このデータセットは、UCI Machine Learning Repositorの [gene expression cancer RNA-Seq Data Set](https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq)を改変したデータセットです。</small>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjoFnGBDyAhc"
      },
      "source": [
        "# /// 実習前に、このセルを実行してください。 ///\n",
        "# サンプルデータをダウンロード\n",
        "!wget -q -O CD_classification.csv https://raw.githubusercontent.com/CropEvol/lecture/master/data/CD_classification.csv\n",
        "# 「分類」アルゴリズムの描画関数などをダウンロード\n",
        "!wget -q -O classification.py https://raw.githubusercontent.com/CropEvol/lecture/master/modules/classification.py\n",
        "# 決定木描画用のライブラリをインストール\n",
        "!pip install -q dtreeviz\n",
        "\n",
        "# データの読み込み\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"CD_classification.csv\", sep=\",\", header=0)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhuGS4vCKAC6"
      },
      "source": [
        "# グラフ\n",
        "import matplotlib.pyplot as plt\n",
        "a = df[df[\"class\"] == \"C\"]\n",
        "b = df[df[\"class\"] == \"D\"]\n",
        "plt.scatter(a[\"x1\"], a[\"x2\"], color=\"red\", label=\"C\")\n",
        "plt.scatter(b[\"x1\"], b[\"x2\"], color=\"blue\", label=\"D\")\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left') # 凡例をグラフの外側に設置\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cとラベル付けされたデータ、Dとラベル付けされたデータ、それぞれに特有の特徴がありそうです。"
      ],
      "metadata": {
        "id": "cg0K3PPTSAX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9ztyFWexIJ1"
      },
      "source": [
        "## 今回の実習内容\n",
        "1. 前処理\n",
        "1. ロジスティック回帰 Logistic regression\n",
        "1. サポートベクトルマシン Support vector machine (SVM)\n",
        "1. 決定木 Decision tree\n",
        "1. ランダムフォレスト Random Forest\n",
        "1. ニューラルネットワーク Neural network\n",
        "1. 評価\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuQtkQE5Fh7D"
      },
      "source": [
        "## 1. 前処理\n",
        "\n",
        "　「回帰」のときと同じく、「分類」でも前処理は必要です。ここでは、次の4つの前処理をおこないます。\n",
        "1. データの準備\n",
        "2. データの分割\n",
        "3. 説明変数のスケーリング\n",
        "4. 目的変数の数値変換\n",
        "\n",
        "　まず、最初の二つをおこないましょう。\n",
        "\n",
        "1. データの準備: データセットから使用するデータを取り出す\n",
        "  - 説明変数（特徴量）: `x1`, `x2`\n",
        "  - 目的変数（ラベル）: `class`\n",
        "1. データの分割: トレーニングデータとテストデータを作る\n",
        "  - トレーニングデータ 75%\n",
        "  - テストデータ 25%\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(説明変数, 目的変数, test_size=分割比率, stratify=目的変数)\n",
        "# stratify: 分割後のラベルの割合を元データとほぼ同じにする\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzfEibXIAkv7"
      },
      "source": [
        "# (1) データの準備\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,[\"x1\",\"x2\"]]) # 説明変数（特徴量）\n",
        "y = np.array(df.loc[:,\"class\"])     # 目的変数（ラベル）\n",
        "\n",
        "# (2) データの分割\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, stratify=y)\n",
        "\n",
        "# サンプル数の確認\n",
        "print(\"original: \", np.unique(y, return_counts=True))       # 元データセット\n",
        "print(\"training: \", np.unique(y_train, return_counts=True)) # トレーニングデータ\n",
        "print(\"test: \", np.unique(y_test, return_counts=True))     # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83DKmRJCGzu3"
      },
      "source": [
        "　つぎに、説明変数（特徴量）のスケーリングをおこないましょう。\n",
        "\n",
        "3. 説明変数のスケーリング: 説明変数の尺度を揃える\n",
        "  - トレーニングデータを「標準化」（平均0、標準偏差1となるように変換）\n",
        "  - テストデータも、トレーニングデータの変換水準で「標準化」\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "スケーリング変数 = StandardScaler()\n",
        "変換後のトレーニングデータ = スケーリング変数.fit_transform(トレーニングデータ)\n",
        "変換後のテストデータ = スケーリング変数.transform(テストデータ)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9eRHCAGXPE"
      },
      "source": [
        "# (3) 説明変数のスケーリング\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train) # トレーニングデータ\n",
        "x_test_ss = ss.transform(x_test)     # テストデータ\n",
        "\n",
        "# 確認: トレーニングデータ10個\n",
        "print(x_train[:10])      # 変換前\n",
        "print(x_train_ss[:10]) # 変換後"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU78YM28JUlV"
      },
      "source": [
        "4. 目的変数の数値変換: 目的変数（ラベル）を整数値に変換する\n",
        "  - `C` → `0`\n",
        "  - `D` → `1`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "ラベルエンコーダ変数 = LabelEncoder()\n",
        "変換後のトレーニングデータ = ラベルエンコーダ変数.fit_transform(トレーニングデータ)\n",
        "変換後のテストデータ = ラベルエンコーダ変数.transform(テストデータ)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9hqt7u_GZlm"
      },
      "source": [
        "# (4) 目的変数の数値変換\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train) # トレーニングデータ\n",
        "y_test_le = le.transform(y_test)     # テストデータ\n",
        "\n",
        "# 確認\n",
        "print(y_train)    # 変換前\n",
        "print(y_train_le) # 変換後"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc2I-x793LlT"
      },
      "source": [
        "### 実習1\n",
        "\n",
        "　次のコードセルに追記をおこない、トレーニングデータとテストデータのラベルの数値変換をおこなってください。\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "ラベルエンコーダ変数 = LabelEncoder()\n",
        "変換後のトレーニングデータ = ラベルエンコーダ変数.fit_transform(トレーニングデータ)\n",
        "変換後のテストデータ = ラベルエンコーダ変数.transform(テストデータ)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYBo0gpuPwxz"
      },
      "source": [
        "# データ\n",
        "import numpy as np\n",
        "crop1 = np.array([\"rice\", \"rice\", \"wheat\", \"wheat\"]) # トレーニングデータ\n",
        "crop2 = np.array([\"wheat\", \"rice\", \"rice\", \"wheat\"]) # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le2 = LabelEncoder()\n",
        "crop1_le = le2.fit_transform()  # トレーニングデータ\n",
        "crop2_le = le2.transform()     # テストデータ\n",
        "# ============== 編集エリア(end) =============\n",
        "\n",
        "# 確認\n",
        "print(crop1)    # トレーニングデータ（変換前）\n",
        "print(crop1_le) # トレーニングデータ（変換後）\n",
        "print(crop2)    # テストデータ（変換前）\n",
        "print(crop2_le) # テストデータ（変換後）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr3VQJtuPumA"
      },
      "source": [
        "#### 解答例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16i1cc1F5Glj"
      },
      "source": [
        "# データ\n",
        "import numpy as np\n",
        "crop1 = np.array([\"rice\", \"rice\", \"wheat\", \"wheat\"]) # トレーニングデータ\n",
        "crop2 = np.array([\"wheat\", \"rice\", \"rice\", \"wheat\"]) # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le2 = LabelEncoder()\n",
        "crop1_le = le2.fit_transform(crop1)  # トレーニングデータ\n",
        "crop2_le = le2.transform(crop2)     # テストデータ\n",
        "# ============== 編集エリア(end) =============\n",
        "\n",
        "# 確認\n",
        "print(crop1)    # トレーニングデータ（変換前）\n",
        "print(crop1_le) # トレーニングデータ（変換後）\n",
        "print(crop2)    # テストデータ（変換前）\n",
        "print(crop2_le) # テストデータ（変換後）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tqJO6PHP3M4"
      },
      "source": [
        "## 2. ロジスティック回帰 Logistic regression\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/logistic_want.png?raw=true\" alt=\"logistic\" height=\"300px\">\n",
        "\n",
        "　**ロジスティック回帰（Logisitic regression）**は、線形回帰に少し手を加えて、**あるクラスに分類される確率を求める手法**です。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/logistic_regression.png?raw=true\" alt=\"logistic_regression\" height=\"250px\">\n",
        "\n",
        "　ロジスティック回帰は次の式で表されるモデルです（詳しくは「ロジスティック回帰の式を詳しく　」を参照）。\n",
        "\n",
        "$$ z = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon $$\n",
        "$$ p = \\frac{1}{1+e^{-z}} $$\n",
        "\n",
        "- $k$番目($k=1,2,3,...,m$)の係数$\\beta$と説明変数$x$をそれぞれ$\\beta_k$、$x_k$で表しています。$\\epsilon$（epsilon）は誤差（切片）です。\n",
        "- $p$はあるクラスに属している確率を表しています。$e$はネイピア数（=2.7182...）です。  \n",
        "\n",
        "　このロジスティック回帰モデルをトレーニングすると、係数$\\beta$や誤差$\\epsilon$が求まります。その予測モデルに、新しいデータ$x=(x_1, x_2, ..., x_k, ..., x_m)$を与えると、あるクラスに属している確率$p$がわかります。\n",
        "\n",
        "　どうやってモデルの学習するかについては、後述の「ロジスティック回帰のトレーニング方法」を参照してください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hsBGAhUQGK7"
      },
      "source": [
        "### ロジスティック回帰の式を詳しく...\n",
        "<small>*※ 読み飛ばしてOKです。ロジスティック回帰の式の背景を数式を使って説明しています。*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QY0Jd4DQLrg"
      },
      "source": [
        "　ロジスティック回帰の式を理解するためには、**オッズ(odds)**から始める必要があります。\n",
        "\n",
        "　ある事象Aが起こる確率を$p$としたとき、Aが起こらない確率は$1-p$となります。このとき、Aの起こる確率とAが起こらない確率の比は、$$\\frac{p}{1-p}$$となります。これがオッズと呼ばれるもので、Aが起こる「見込み」を表しています。\n",
        "\n",
        "確率である$p$の値は0から1の範囲しかとれませんが、**オッズに変換し対数をとる**ことで$-\\infty$から$\\infty$まで値を取ることが出来ます。先程確率は0から1までしかとれないので回帰式が作れないという話をしましたが、オッズを対象にすると線形回帰で扱えそうな値になりますね。\n",
        "\n",
        "さて、ここで最終的に知りたいことは、**データから確率$p$を推測する式**です。\n",
        "\n",
        "　オッズの対数（**対数オッズ**）をとり、関数とみなすと、$$f(p) = \\log{\\frac{p}{1-p}}$$となります。これは、**ロジット関数（logit）**として知られる関数です。\n",
        "\n",
        "　ロジット関数を逆関数にすると、$f(p)$から$p$を求める関数が得られます。\n",
        "\\begin{align*}\n",
        "f(p) &= \\log{\\frac{p}{1-p}} \\\\\n",
        "e^{f(p)} &= \\frac{p}{1-p} \\\\\n",
        "e^{f(p)} - e^{f(p)} p &= p \\\\\n",
        "p &= {\\frac{e^{f(p)}}{e^{f(p)}+1}} \\\\ &= \\frac{1}{1+e^{-f(p)}}\n",
        "\\end{align*}\n",
        "\n",
        "　ここで、$f(p) = z$ とすると、ロジスティック回帰のふたつ目の式が得られます。これは、**ロジスティック関数（logistic）**として知られる関数です。$$ p = \\frac{1}{1+e^{-z}} $$この関数は、入力値$z$からAが起こる確率$p$を求める関数とみなせます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/logistic_curve.png?raw=true\" alt=\"logistic_curve\" height=\"200px\">\n",
        "\n",
        "　ロジスティック回帰は、線形回帰の式をロジスティック関数の入力$z$につなげたものです。これは、「データ$x$」を「Aがおこる確率$p$」に変換する式です。\n",
        "\n",
        "$$ z = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon $$\n",
        "\n",
        "$$ p = \\frac{1}{1+e^{-(\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + \\epsilon)}} $$\n",
        "\n",
        "　"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqpX0D6FQb2c"
      },
      "source": [
        "### ロジスティック回帰モデルのコスト関数\n",
        "<small>*※ 読み飛ばしてOKです。*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBaUKxOcQiQ0"
      },
      "source": [
        "　ロジスティック回帰モデルも、線形回帰と同じく、正解ラベルと予測ラベルが最も一致するように学習をおこないます。異なるのは、ロジスティック回帰の出力が「確率」である点です。そこで、ロジスティック回帰は、正解ラベルと一致している確率が最も高くなるように学習します。それを表したのが次の式です。\n",
        "\n",
        "$$L = \\prod_{i=1}^{n}(p_{i})^{y_{i}}(1-p_{i})^{1-y_{i}}$$\n",
        "\n",
        "ここで、$i$はサンプル番号、$p_i$はラベル「1」である予測確率、$y_i$は正解ラベルを表しています。この式は、**尤度 Likelihood**と呼ばれる式です。\n",
        "\n",
        "　仮に、1番目のサンプルの正解ラベルが$y_1=1$であった場合、\n",
        "$$L_1 = (p_1)^{1}（1-p_1）^{1-1}=p_1$$\n",
        "になります。これは、ラベル「1」である予測確率です。\n",
        "\n",
        "一方で、2番目のサンプルの正解ラベルが$y_2=0$であった場合、\n",
        "$$L_2 = (p_2)^{0}（1-p_2）^{1-0}=1-p_2$$\n",
        "になり、ラベル「0」である予測確率（ラベル「1」ではない予測確率）となります。\n",
        "\n",
        "このような計算をすべてのサンプルにわたっておこない、尤度を求めます。\n",
        "$$例: L = p_1 (1-p_2) p_3 p_4 (1-p_5) ... (1-p_{n-1}) p_n$$\n",
        "簡単に言うと、全てのサンプルの正解のラベルの予測確率を掛け合わせたものです。\n",
        "\n",
        "　ロジスティック回帰モデルでは、目的関数「尤度（予測確率の積）」が最大となる$\\beta_{optimum}$や$\\epsilon_{optimum}$を探索します。実際には、微分を容易にするために（また、小数点数の総積によるアンダフローを防ぐ目的で）、尤度の式を対数（**対数尤度 Log likelihood**）にします。\n",
        "$$\\log{L} = \\sum_{i=1}^{n}{ [ y_{i} \\log{p_{i}} + (1-y_{i}) \\log{(1-p_{i})} ] }  $$\n",
        "\n",
        "　さらに、勾配法などで最小値を調べるために、上の式に-1をかけて「コスト関数」にしています。これは、**交差エントロピー誤差関数（Cross-entropy error function）**と呼ばれる関数です。\n",
        "\n",
        "$$ Cost = -\\log{L} = \\sum_{i=1}^{n}{ [ -y_{i} \\log{p_{i}} - (1-y_{i}) \\log{(1-p_{i})} ] }$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAuQXi-4QPal"
      },
      "source": [
        "### ロジスティック回帰モデルのトレーニング方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIfI0dKDQVH4"
      },
      "source": [
        "　ロジスティック回帰の $\\beta$ や $\\epsilon$ の最適解をどうやって求めるか? ロジスティック回帰では、**交差エントロピー誤差関数（Cross-entropy error function）**と呼ばれる関数を最小化する$\\beta$ や $\\epsilon$ を探索します（Wikipedia「[Cross-entropy error function and logistic regression](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression)」）。この関数は、非常に複雑で、最小二乗法のような解析的な手法で $\\beta$ や $\\epsilon$ を求められません。そこで、勾配法などのパラメータを繰り返し更新する「反復法 Iterative method」で最適解に近づきます。どのような手法が使われているか知りたい場合は、[scikit-learn ユーザーガイド](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression)などを参照してください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc4aMjCWR7YX"
      },
      "source": [
        "### 実装\n",
        "\n",
        "　ロジスティック回帰を実装してみましょう。\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "モデル変数 = LogisticRegression(solver=\"sag\", max_iter=100)\n",
        "モデル変数.fit(説明変数, 目的変数) # トレーニングデータ\n",
        "```\n",
        "\n",
        "ここで使っている設定値 (詳しくはscikit-learn  [LogisiticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)参照): \n",
        "- `solver=\"sag\"`: 最適化アルゴリズムに「Stochastic Average Gradient (SAG)」と呼ばれる確率的勾配法を使用する\n",
        "- `max_iter=100`: トレーニング回数（反復回数）を100回にする\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<small>※ 次のコードセルのグラフ描画の関数[`draw_decision_boundary`](https://github.com/CropEvol/lecture/blob/master/textbook_2019/modules/classification.py)は、こちらで用意した自作関数です。</small>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap3wyAa0SB8Y"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# モデルを作成\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model_lr = LogisticRegression(solver=\"sag\", max_iter=100)\n",
        "# モデルをトレーニング\n",
        "model_lr.fit(X_train, Y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "print(\"Coefficient: b =\", model_lr.coef_)\n",
        "print(\"Intercept: e =\", model_lr.intercept_)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model_lr, xlabel=\"x1\", ylabel=\"x2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLGhyWluWXSN"
      },
      "source": [
        "　予測モデルの評価もおこないましょう。「分類」問題では、`score`で正解率（Accuracy）を調べることができます。\n",
        "\n",
        "<small>※ 詳しくは、後述の「モデルの評価」で学びます。</small>\n",
        "\n",
        "```python\n",
        "# 正解率\n",
        "モデル変数.score(トレーニングデータ)\n",
        "モデル変数.score(テストデータ)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5yuEKvMWIQD"
      },
      "source": [
        "# 評価（正解率）\n",
        "print(\"training data: \", model_lr.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model_lr.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z11IyyG4bwGw"
      },
      "source": [
        "### 実習2\n",
        "\n",
        "　`LogisticRegression()`内の`max_iter`オプションに与える数値を以下に設定し、実行結果（分類の境界線の位置）が変わることを確認してください。\n",
        "\n",
        "- `max_iter=0`\n",
        "- `max_iter=1`\n",
        "- `max_iter=10`\n",
        "- `max_iter=100`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbX9FjrtdC6O"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルを作成\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model2_lr = LogisticRegression(solver=\"sag\", max_iter=0)\n",
        "# ============== 編集エリア(end) =============\n",
        "# モデルをトレーニング\n",
        "model2_lr.fit(X_train, Y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "print(\"Coefficient: b =\", model2_lr.coef_)\n",
        "print(\"Intercept: e =\", model2_lr.intercept_)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_lr, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_lr.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_lr.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPUF6jU3dfW1"
      },
      "source": [
        "#### 解答例\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OGC6Eopd4Hq"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルを作成\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model2_lr = LogisticRegression(solver=\"sag\", max_iter=0)\n",
        "# model2_lr = LogisticRegression(solver=\"sag\", max_iter=1)\n",
        "# model2_lr = LogisticRegression(solver=\"sag\", max_iter=10)\n",
        "# model2_lr = LogisticRegression(solver=\"sag\", max_iter=100)\n",
        "# ============== 編集エリア(end) =============\n",
        "# モデルをトレーニング\n",
        "model2_lr.fit(X_train, Y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "print(\"Coefficient: b =\", model2_lr.coef_)\n",
        "print(\"Intercept: e =\", model2_lr.intercept_)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_lr, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_lr.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_lr.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fmZcYm5ehi_"
      },
      "source": [
        "## 3. サポートベクトルマシン Suport vector machine (SVM)\n",
        "\n",
        "　**サポートベクトルマシン（Suport vector machine: SVM）**は、たいへん人気のある分類手法です。この手法は、**最も近いデータまでの距離（マージン margin）が最大となる境界を作り、データを二分する方法**です。この予測モデルは、線形関数+[符号関数（sign関数）](https://ja.wikipedia.org/wiki/%E7%AC%A6%E5%8F%B7%E9%96%A2%E6%95%B0)です。\n",
        "\n",
        "　サポートベクトルマシンは、下図のようなイメージです。トレーニングデータを学習して、マージンを最大化できる境界線（または境界面）を探索します。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm.png?raw=true\" alt=\"svm\" height=\"250px\">\n",
        "\n",
        "$$ 予測モデル: f(x) = \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + e $$\n",
        "\n",
        "$$\n",
        "y = \\begin{cases} \n",
        "1 & (f(x) \\geq 0) \\\\\n",
        "-1 & (f(x) < 0)\n",
        "\\end{cases} $$\n",
        "\n",
        "$$ 境界: \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + ... + \\beta_mx_m + e = 0 $$\n",
        "\n",
        "- $k$番目($k=1,2,3,...,m$)の係数$\\beta$と説明変数$x$をそれぞれ$\\beta_k$、$x_k$で表しています。$e$は誤差（切片）です。\n",
        "- $y=1$であれば「正クラス」、$y=-1$であれば「正クラスではない（負クラス）」と予測します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr67xF2KxVOA"
      },
      "source": [
        "### サポートベクトルマシンを詳しく...\n",
        "<small>*※ 読み飛ばしてOKです。数式を使って、サポートベクトルマシンを詳しく説明しています。*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDhE6aARJTwO"
      },
      "source": [
        "　まず、2つのクラスを完璧に分割できる境界$\\beta_1x_1 + \\beta_2x_2 + ... + \\beta_mx_m + e = 0$を考えます。ベクトルを使って記述をすると、次のように書けます。\n",
        "\n",
        "$$\n",
        "境界: \\beta^{T} X + e = 0 \\\\\n",
        "\\beta=\\begin{pmatrix}\n",
        "\\beta_1 \\\\\n",
        "\\beta_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\beta_m\n",
        "\\end{pmatrix}, \n",
        "X=\\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_m\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "　次に、この境界に最も近い正クラスデータ $X_+$ と負クラスデータ $X_-$ について考えます。このような境界に最も近いデータのことを**サポートベクトル**と言います。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail1.png?raw=true\" alt=\"svm_detail1\" height=\"200px\">\n",
        "\n",
        "　さらに、正クラス側のサポートベクトルを通り、境界に平行な直線（または面）を考えます。同様に、負クラス側の直線も考えると、次の2つの式を用意できます。これらは、マージンの両端を表す式です。\n",
        "\n",
        "(細かい話になるので省略していますが、$\\beta$の値を標準化し、$e$の値を調整することで、右辺を1や-1にすることが出来ます。)\n",
        "\n",
        "\\begin{align*}\n",
        "\\beta^{T} X_+ + e &= 1 \\\\\n",
        "\\beta^{T} X_- + e &= -1\n",
        "\\end{align*}\n",
        "すなわち、\n",
        "$$|\\beta^{T} X_- + e| = 1$$\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail2.png?raw=true\" alt=\"svm_detail2\" height=\"200px\">\n",
        "\n",
        "　正ラベル側のすべての点について考えると、それらの点は、マージンの外側、すなわち、次の不等式で表されるエリアにあるはずです。\n",
        "$$ \\beta^{T} X_i + e \\geq 1 $$\n",
        "正ラベルの値は「1」で表されます。ラベルの値 $t_i=1$ を上の式にかけた不等式は、\n",
        "$$ t_i (\\beta^{T} X_i + e) \\geq 1 $$\n",
        "\n",
        "負ラベル（$t_i=-1$）側についても同様に、\n",
        "$$ \\beta^{T} X_i + e \\leq -1 $$\n",
        "$$ t_i (\\beta^{T} X_i + e) \\geq 1 $$\n",
        "\n",
        "したがって、すべての点は次の式で表せます。この式は、最適な$\\beta$や$e$を探索する際の条件のひとつです。\n",
        "$$ t_i (\\beta^{T} X_i + e) \\geq 1 $$\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/svm_detail3.png?raw=true\" alt=\"svm_detail3\" height=\"200px\">\n",
        "\n",
        "　今度は、最小化関数（コスト関数）について考えていきます。サポートベクトルと境界の距離$d$は、次の式で表されます（これは高校で習った「垂線の長さ（点と直線の距離）」を求める式と同じです）。\n",
        "$$ d = \\frac{| \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_m x_m + e |}{\\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2 }} = \\frac{ |\\beta^{T} X + e | }{ \\|\\beta\\| } $$\n",
        "\n",
        "この式の分子は、$ |\\beta^{T} X + e | = 1$より、\n",
        "$$ d =  \\frac{ 1 }{ \\|\\beta\\| } $$\n",
        "\n",
        "　したがって、境界間の距離（マージン）は、次のとおりです。\n",
        "$$ \\frac{ 2 }{ \\|\\beta\\| },  \\|\\beta\\| = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2} $$\n",
        "\n",
        "　サポートベクトルマシンは、このマージンを最大化するように学習していきます。「マージンの最大化」は、「$\\|\\beta\\|$の最小化」と同じです。また、$\\|\\beta\\| = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_m^2} \\geq 0 $ のため、「$\\|\\beta\\|$の最小化」は「$\\|\\beta\\|^2$の最小化」と同じです。そこで、サポートベクトルマシンは、この$\\|\\beta\\|^2$を2で割った関数を「コスト関数」として、問題を解いています（2で割っているのは、この関数を微分したときの計算が簡単になるからです）。\n",
        "$$Cost = \\frac{\\|\\beta\\|^2}{2}$$\n",
        "\n",
        "\n",
        "　以上をまとめると、サポートベクトルマシンでは、\n",
        "$$ 制約条件: t_i (\\beta^{T} X_i + e) \\geq 1 $$を満たしつつ、\n",
        "$$ コスト関数: Cost = \\frac{\\|\\beta\\|^2}{2}$$\n",
        "を最小化させる$\\beta$と$e$を求めます。\n",
        "\n",
        "　この$\\beta$と$e$の最適値を求める方法については、さらに複雑です。通常、「逐次最小問題最適化法 Sequential Minimal Optimization (SMO)」と呼ばれる方法で、解を探索します。\n",
        "\n",
        "　なお、上述したサポートベクトルマシンの説明は、2つのクラスを完全に線形分離できるという仮定で話をしていました。そのマージンのことを **ハードマージン（Hard margin）** と言います。しかし、実際の問題では、多くの場合、線形方程式では完全に分離できません。そこで、ある程度の誤分類を許容した方法も考案されており、よく利用されています。なお、この誤分類を許容したマージンのことを**ソフトマージン（Soft margin）**と言います。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/hard_soft.png?raw=true\" alt=\"hard_soft\" height=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbFzM-F8-XL0"
      },
      "source": [
        "### 非線形のサポートベクトルマシン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TII5jowD_K-G"
      },
      "source": [
        "　サポートベクトルマシンは、線形方程式では決して分離できない問題にも対応可能です。\n",
        "\n",
        "　どのように対応するか? **カーネルトリック Kernel trick**と言われる方法を使います。この方法は、オリジナルな低次元のデータから、高次元のデータを作り出し、その高次元データで問題を解決しようという方法です。\n",
        "\n",
        "実空間では綺麗に分離できないデータを、超平面で分離できる空間に写像することで、SVMでデータを分離する形になります。\n",
        "\n",
        "　例えば、2個の説明変数 $(x_1, x_2)$ で線形分離できない問題に対して、次のようなカーネル化をおこないます（なお、カーネル化方法は他にもあります）。\n",
        "$$(x_1, x_2) \\rightarrow (x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$$\n",
        "そして、この新しくできた$(x_1^2, \\sqrt{2} x_1 x_2, x_2^2)$を使って、SVMモデルを構築します。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/kernelSVM.png?raw=true\" alt=\"kernelSVM\" height=\"200px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhw6M-jlsmDg"
      },
      "source": [
        "### 実装\n",
        "\n",
        "　サポートベクトルマシンを実装してみましょう。scikit-learnを使うと簡単にコーディングできます。\n",
        "\n",
        "```python\n",
        "# 書き方\n",
        "from sklearn.svm import SVC\n",
        "モデル変数 = SVC(kernel=\"カーネル化手法\")\n",
        "モデル変数.fit(説明変数, 目的変数) # トレーニングデータ\n",
        "```\n",
        "\n",
        "\n",
        "ここで使っている設定値（詳しくはscikit-learn [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)参照）:\n",
        "- `kernel=\"linear\"`: 線形SVM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4chSOv9spE9"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# モデルを作成\n",
        "from sklearn.svm import SVC\n",
        "model_svm = SVC(kernel=\"linear\")\n",
        "# モデルをトレーニング\n",
        "model_svm.fit(X_train, Y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "if model_svm.kernel == \"linear\": \n",
        "  print(\"Coefficient=\", model_svm.coef_)\n",
        "  print(\"Intercept: e =\", model_svm.intercept_)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model_svm, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model_svm.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model_svm.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw8hXCU0leQL"
      },
      "source": [
        "### 実習3\n",
        "\n",
        "　`SVC()`内の`kernel`オプションを以下に変更してください。\n",
        "\n",
        "変更後:\n",
        "- `kernel=\"rbf\"`（カーネル化関数: RBF (Radial Basis Function) kernel）   \n",
        "または \n",
        "- `kernel=\"poly\"`（カーネル化関数: Polynomial kernel）\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf6mf3SDlhLy"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルの作成\n",
        "from sklearn.svm import SVC\n",
        "model2_svm = SVC(kernel=\"linear\")\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルの学習\n",
        "model2_svm.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_svm, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_svm.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_svm.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8Z-rCOUlhfT"
      },
      "source": [
        "#### 解答例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OD3dmXllli2i"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルの作成\n",
        "from sklearn.svm import SVC\n",
        "model2_svm = SVC(kernel=\"rbf\")\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルの学習\n",
        "model2_svm.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_svm, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_svm.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_svm.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUtOrUcAewf2"
      },
      "source": [
        "## 4. 決定木 Decision tree\n",
        "\n",
        "　**決定木（Decision tree）**は、**説明変数に対する問い（True or False）を繰り返して、木構造の分類モデルを作る方法**です。\n",
        "\n",
        "決定木を用いて作成したモデルは、下図のように解釈が非常にしやすいモデルになります。どの様な特徴量が重要なのかを理解しやすいモデルです。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree.png?raw=true\" alt=\"decision_tree\" height=\"350px\">\n",
        "\n",
        "しかし、簡単に過学習してしまう手法でもあります。\n",
        "\n",
        "どれだけ決定木を分岐させるかを決定木の「深さ」と呼んだりしますが、学習データに合わせて決定木をどんどん深くしてしまうと、学習データにのみ都合の良い汎用性の低いモデルとなってしまいます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/fukasa.png?raw=true\" alt=\"fukasa\" height=\"300px\">\n",
        "\n",
        "決定木の深さを調整することで過学習を防ぐことも可能ですが、近年では後ほど紹介するRandom Forestをはじめとした、決定木を応用した様々な手法が開発されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O9KBzLqhbR8"
      },
      "source": [
        "### 各分岐をどのように設定するか？\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaxJ_dxJjovz"
      },
      "source": [
        "　決定木の各分岐は、適当に設定されているわけではありません。\"なるべく少ない質問でクラス分けする\"ように木構造を作っていきます（木に例えると、枝を伸ばしていきます）。\n",
        "\n",
        "　どの説明変数を分岐の質問に使うか？　分割前と分割後の「**情報量**または**不純度**」の差（**情報利得 Information gain**）が大きくなる質問を、あらゆる質問の中から選択します。「情報量」や「不純度」は、多様なラベル（クラス）がそのデータセットにどのぐらい含まれているかを表す指標です。「情報利得」は、分岐によりどれだけクラス分けできるかを表す指標です。\n",
        "\n",
        "$$情報利得 = I_{pre} - (I_{true} + I_{false})$$\n",
        "\n",
        "- $I_{pre}$は分割前データの情報量（不純度）\n",
        "- $I_{true}$は分割後Trueデータの情報量（不純度）\n",
        "- $I_{false}$は分割後Falseデータの情報量（不純度）\n",
        "\n",
        "　決定木では、情報量を表す指標として、**エントロピー（Entropy）**が使われます。また、不純度を表す指標には、**ジニ不純度（Gini impurity）**と呼ばれる指標が使われます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/entropy.png?raw=true\" alt=\"entropy\" height=\"300px\">\n",
        "\n",
        "\n",
        "　\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYMhW0RGJ1G9"
      },
      "source": [
        "### 分岐の設定方法を直感的に理解する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLsIUwZWJ28a"
      },
      "source": [
        "　次のようなデータセットから、鳥類・哺乳類の分類モデル（決定木）を作りたいとします。最初の質問にはどの説明変数を使うと良いでしょうか？\n",
        "\n",
        "||A. 食性|B. 発生形態|C. 体温|分類ラベル（マーク）|\n",
        "|:---|---:|---:|---:|---:|\n",
        "|ニワトリ|草食|卵生|恒温|鳥類（●）|\n",
        "|ペンギン|肉食|卵生|恒温|鳥類（●）|\n",
        "|カモノハシ|肉食|卵生|恒温|哺乳類（○）|\n",
        "|ウシ|草食|胎生|恒温|哺乳類（○）|\n",
        "|ヒツジ|草食|胎生|恒温|哺乳類（○）|\n",
        "|ライオン|肉食|胎生|恒温|哺乳類（○）|\n",
        "\n",
        "　それぞれの説明変数で分けてみると、次のようになります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree_which_features.png?raw=true\" alt=\"decision_tree_which_features\" height=\"230px\">\n",
        "\n",
        "- 「A. 食性」で分けた場合、分割後のそれぞれのサブセットの中には、まだ両方のラベルが混ざっています。ラベルの比率は、どちらのサブセットも元のデータセットの比率が維持されています。この質問は、ラベル分割に関して言えば、ほとんど情報を得ることができない質問です。\n",
        "- 「B. 発生形態」で分けた場合、「卵生」サブセットは、両方のラベルが含まれています。ラベル比率は、元のデータセットからすると、鳥類（●）の割合が高くなっています。一方、「胎生」サブセット側は、1種類のラベルのみとなっています。この質問は、\"元のデータセットから哺乳類（○）の多くを分ける\"質問であることがわかります。すなわち、ラベル分割に関して、情報を得ることが可能な質問です。\n",
        "- 「C. 体温」を使った場合、全く分割できません。この説明変数を使ったとしても、元のデータセットと同じ状態が維持されており、何も情報を得ることができていません。\n",
        "\n",
        "\n",
        "\n",
        "　\"なるべく少ない質問でクラス分け\" するためには、最も情報が得られそうな「B. 発生形態」を最初の質問に採用するのが良さそうです。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWU88Zyyjpxt"
      },
      "source": [
        "### エントロピー 、ジニ不純度、情報利得\n",
        "<small>*※ 読み飛ばしてOKです。「各分岐をどのように設定するか？」や「分岐の設定方法を直感的に理解する」を数式を使って、詳しく説明しています。*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IijqnJmpjsU0"
      },
      "source": [
        "#### 情報利得\n",
        "\n",
        "　決定木の学習アルゴリズムにおける目的関数は、「情報利得（Information gain）」です。情報利得を最大化するように、分岐を作成し、木構造を発達させていきます。情報利得を定式化すると次のようになります。\n",
        "\n",
        "$$ 情報利得: IG = I(D_{pre}) - (\\frac{N_{true}}{N_{pre}}I(D_{true}) + \\frac{N_{false}}{N_{pre}}I(D_{false})) $$ \n",
        "\n",
        "- $I(D_{pre})$は、データセットにおけるエントロピーまたはジニ不純度\n",
        "- $I(D_{true})$は、分割後のTrueデータセットにおけるエントロピーまたはジニ不純度\n",
        "- $I(D_{false})$は、分割後のFalseデータセットにおけるエントロピーまたはジニ不純度\n",
        "- $N_{pre}$は、データセットのサンプル数\n",
        "- $N_{false}$は、分割後のTrueデータセットのサンプル数\n",
        "- $N_{false}$は、分割後のFalseデータセットのサンプル数\n",
        "\n",
        "　次に、$I(D)$についてです。これにはいくつか種類あり、「エントロピー（Entropy）」または「ジニ不純度（Gini impurity）」がよく使われます。どちらの指標も、データセット中に含まれる各クラスの割合をその計算に使います。\n",
        "\n",
        "以下では、ラベルAとラベルBの分類を例にしており、データセット中のそれぞれのラベルの割合を$p_{A}$、$p_{B}$と表記しています。\n",
        "\n",
        "#### エントロピー\n",
        "　データセットの「エントロピー」は、次の式で計算されます。なお、詳細は省きますが、この式は、データセットの平均情報量（情報量の期待値）を求める式でもあります。\n",
        "\n",
        "$$エントロピー: I(D) = p_{A}log_2 \\frac{1}{p_{A}} + p_{B}log_2 \\frac{1}{p_{B}} = -p_{A}log_2 p_{A} - p_{B}log_2 p_{B}$$\n",
        "\n",
        "#### ジニ不純度\n",
        "　　データセットの「ジニ不純度」は、次の式で求めます。\n",
        "\n",
        "$$ジニ不純度: I(D) = p_{A}(1 - p_{A}) + p_{B}(1 - p_{B})$$\n",
        "\n",
        "例えば、分割した後のデータセットにAしか含まれていなかった場合($p_{A}=1$)、エントロピーもジニ不純度も0になりますが、分割してみたもののAもBも両方が同じだけ混ざっている場合($p_{A}=0.5, p_{B}=0.5$)、エントロピーは1、ジニ不純度は0.5ととても高くなってしまいます。\n",
        "\n",
        "分割することでエントロピーやジニ不純度の低いデータに分けることが出来れば、情報利得は大きくなるという訳です。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/entropy.png?raw=true\" alt=\"entropy\" height=\"300px\">\n",
        "\n",
        "#### 計算例\n",
        "　上述「分岐の設定方法を直感的に理解する」の例において、それぞれの説明変数の「エントロピーを使った情報利得」や「ジニ不純度を使った情報利得」がいくつになるか、以下で求めてみます。\n",
        "\n",
        "||A. 食性|B. 発生形態|C. 体温|分類ラベル（マーク）|\n",
        "|:---|---:|---:|---:|---:|\n",
        "|ニワトリ|草食|卵生|恒温|鳥類（●）|\n",
        "|ペンギン|肉食|卵生|恒温|鳥類（●）|\n",
        "|カモノハシ|肉食|卵生|恒温|哺乳類（○）|\n",
        "|ウシ|草食|胎生|恒温|哺乳類（○）|\n",
        "|ヒツジ|草食|胎生|恒温|哺乳類（○）|\n",
        "|ライオン|肉食|胎生|恒温|哺乳類（○）|\n",
        "\n",
        "- エントロピーを使った情報利得\n",
        "  - A. 食性\n",
        "$$\n",
        "I(D_{pre}) = - \\frac{2}{6} log_{2} \\frac{2}{6} - \\frac{4}{6} log_{2} \\frac{4}{6} = 0.9182... \\\\\n",
        "I(D_{true}) = -\\frac{1}{3} log_{2} \\frac{1}{3} - \\frac{2}{3} log_{2} \\frac{2}{3} = 0.9182... \\\\\n",
        "I(D_{false}) = - \\frac{1}{3} log_{2} \\frac{1}{3} - \\frac{2}{3} log_{2} \\frac{2}{3} = 0.9182... \\\\\n",
        "IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0\n",
        "$$\n",
        "  - B. 発生形態\n",
        "$$\n",
        "I(D_{pre}) = 0.9182... \\\\\n",
        "I(D_{true}) = 0.9182... \\\\\n",
        "I(D_{false}) = 0 \\\\\n",
        "IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0.4591...\n",
        "$$\n",
        "  - C. 体温\n",
        "$$\n",
        "I(D_{pre}) = 0.9182... \\\\\n",
        "I(D_{true}) = 0.9182... \\\\\n",
        "IG = I(D_{pre}) - \\frac{6}{6} I(D_{true}) = 0\n",
        "$$\n",
        "  - 分岐の質問には『「B. 発生形態」は卵生/胎生？』が選ばれます。\n",
        "\n",
        "- ジニ不純度を使った情報利得\n",
        "  - A. 食性\n",
        "$$\n",
        "I(D_{pre}) = \\frac{2}{6} (1-\\frac{4}{6}) + \\frac{4}{6} (1-\\frac{2}{6}) = 0.4444... \\\\\n",
        "I(D_{true}) = \\frac{1}{3} (1-\\frac{1}{3}) + \\frac{2}{3} (1-\\frac{2}{3}) = 0.4444... \\\\\n",
        "I(D_{false}) = \\frac{1}{3} (1-\\frac{1}{3}) + \\frac{2}{3} (1-\\frac{2}{3}) = 0.4444... \\\\\n",
        "IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0\n",
        "$$\n",
        "  - B. 発生形態\n",
        "$$\n",
        "I(D_{pre}) = 0.4444... \\\\\n",
        "I(D_{true}) = 0.4444... \\\\\n",
        "I(D_{false}) = 0 \\\\\n",
        "IG = I(D_{pre}) - ( \\frac{3}{6} I(D_{true}) + \\frac{3}{6} I(D_{false}) ) = 0.2222...\n",
        "$$\n",
        "  - C. 体温\n",
        "$$\n",
        "I(D_{pre}) = 0.4444... \\\\\n",
        "I(D_{true}) = 0.4444... \\\\\n",
        "IG = I(D_{pre}) - \\frac{6}{6} I(D_{true}) = 0\n",
        "$$\n",
        "  - 分岐の質問には『「B. 発生形態」は卵生/胎生？』が選ばれます。\n",
        "　\n",
        "- どちらの評価指標を使っても、通常はほとんど同じ結果になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGJvLTNhdT7A"
      },
      "source": [
        "### 決定木では、説明変数をスケーリングしなくて良い"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt_OiS06dizx"
      },
      "source": [
        "　各データの順序は、スケーリングの前後で変わりません。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/scalling.png?raw=true\" alt=\"scalling\" height=\"300px\">\n",
        "\n",
        "　決定木による解析において、スケーリングする前とスケーリングした後では、閾値の値が変更されるのみで、スケーリングの有無で分岐の結果が変わることはありません。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/decision_tree_with_scalling.png?raw=true\" alt=\"decision_tree_with_scalling\" height=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuxG11Qij6AH"
      },
      "source": [
        "### 実装\n",
        "\n",
        "　scikit-learnを使って、決定木を実装してみましょう。\n",
        "\n",
        "```python\n",
        "# 書き方\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "モデル変数 = DecisionTreeClassifier(criterion=\"不純度指標\", max_depth=決定技の深さ)\n",
        "モデル変数.fit(説明変数, 目的変数) # トレーニングデータ\n",
        "```\n",
        "\n",
        "\n",
        "ここで使っている設定値（詳しくはscikit-learn [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)参照）:\n",
        "- `criterion=\"gini\"`: 不純度の指標として「ジニ不純度」を使う\n",
        "- `max_depth=3`: 決定木の深さを「3」に設定する\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChwiFfVxj7pU"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# モデルを作成\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model_dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
        "\n",
        "# モデルをトレーニング\n",
        "model_dt.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model_dt, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model_dt.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model_dt.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8l2hioqtVJV"
      },
      "source": [
        "　決定木を可視化すると、"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jT95vTetbWE"
      },
      "source": [
        "# 予測モデルをグラフ用データに変換\n",
        "from dtreeviz.trees import dtreeviz\n",
        "viz = dtreeviz(model_dt, X_train, Y_train, \n",
        "    feature_names=[\"x1\", \"x2\"],  \n",
        "    class_names=[\"C\", \"D\"], \n",
        "    target_name='class',\n",
        "    orientation ='LR')\n",
        "# 表示\n",
        "viz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26IeTgPBZ-hu"
      },
      "source": [
        "### 実習4\n",
        "\n",
        "　`DecisionTreeClassifier()`内のオプション`max_depth`の値を変更して、実行結果が変わることを観察してください。\n",
        "\n",
        "変更するオプション:  \n",
        "- 決定木の深さ `max_depth=None`（上限なし）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSN5OKm1aAtv"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルの作成\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model2_dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=3)\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルの学習\n",
        "model2_dt.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_dt, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_dt.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_dt.score(X_test, Y_test))  # テストデータ\n",
        "\n",
        "# 予測モデルをグラフ用データに変換\n",
        "from dtreeviz.trees import dtreeviz\n",
        "viz = dtreeviz(model2_dt, X_train, Y_train, \n",
        "    feature_names=[\"x1\", \"x2\"],  \n",
        "    class_names=[\"C\", \"D\"], \n",
        "    target_name='class',\n",
        "    orientation ='LR')\n",
        "# 表示\n",
        "viz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-tImsCQcclG"
      },
      "source": [
        "#### 解答例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6bRtm_2ceoE"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) =============\n",
        "# モデルの作成\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model2_dt = DecisionTreeClassifier(criterion=\"gini\", max_depth=None)\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルの学習\n",
        "model2_dt.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_dt, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_dt.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_dt.score(X_test, Y_test))  # テストデータ\n",
        "\n",
        "# 予測モデルをグラフ用データに変換\n",
        "from dtreeviz.trees import dtreeviz\n",
        "viz = dtreeviz(model2_dt, X_train, Y_train, \n",
        "    feature_names=[\"x1\", \"x2\"],  \n",
        "    class_names=[\"C\", \"D\"], \n",
        "    target_name='class',\n",
        "    orientation ='LR')\n",
        "# 表示\n",
        "viz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqzYBkDSdxFZ"
      },
      "source": [
        "### 過学習（オーバーフィッティング Overfitting）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nta8azGdxzF"
      },
      "source": [
        "　トレーニングデータの正解率が高い一方で、テストデータの正解率が低い状態を、**過学習（オーバーフィッティング overfitting）**と言いました。\n",
        "\n",
        "トレーニングデータに\"合わせ過ぎた\"予測モデルが作られており、新しいデータに対する予測精度を損なっている状態です。\n",
        "\n",
        "先程も紹介しましたが、決定木の場合は木の深さを深くするほど、容易に過学習してしまいます。\n",
        "\n",
        "　過学習に陥るおもな要因は、\n",
        "- トレーニングデータの量がモデルの複雑さに対して少なすぎる。もしくは、偏ったデータのみをトレーニングデータとして使っている。\n",
        "- 言い換えると、手に入るデータ量に対してモデルが複雑すぎる。\n",
        "\n",
        "　トレーニングデータ量や質の問題は、データの量を増やすし、質を改善するという直接的な方法になりますが、データによっては簡単に量が増やせないことも多いです。\n",
        "\n",
        "　モデルの複雑さについては、より単純なモデルを選択することで解決できる場合があります。\n",
        "\n",
        "決定木の場合、木の深さを制限して、単純なモデルを構築するか、別の手法（ランダムフォレストなど）を用いることで解決する形になります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01FrjDefeLJ9"
      },
      "source": [
        "## 5. ランダムフォレスト Random forest\n",
        "\n",
        "　**ランダムフォレスト（Random forest）**は、複数の決定木を作り、**各決定木の予測結果を多数決して、最終的な予測値を決める手法**です。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/random_forest.png?raw=true\" alt=\"random_forest\" height=\"250px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpCFpMjlk9_U"
      },
      "source": [
        "### アルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bam4bHfo6l"
      },
      "source": [
        "ランダムフォレストは、複数の決定木を使ったモデルです。このモデルのアルゴリズムは次のとおりです。\n",
        "1. オリジナルのデータセットから、$n$個のサンプルをランダムに抽出します。この時、復元抽出（重複を許して抽出）をおこないます。このようなサンプリング方法を、**ブートストラップサンプリング（Bootstrap sampling）**といいます。\n",
        "\n",
        "1. このブートストラップ標本を使って、決定木を構築します。\n",
        "\n",
        "1. 1,2のステップを$k$回繰り返します。得られたk個の決定木がランダムフォレストです。\n",
        "\n",
        "1. 新しいデータのラベルを予測する際には、各決定木から予測値を集め、多数決により最終的な予測値を決定します。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/random_forest_algorithm.png?raw=true\" alt=\"random_forest_algorithm\" height=\"180px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWHd65v3la71"
      },
      "source": [
        "### 実装\n",
        "\n",
        "　ランダムフォレストを実装してみましょう。\n",
        "\n",
        "```python\n",
        "# 書き方\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "モデル変数 = RandomForestClassifier(n_estimators=決定木の個数, criterion=\"不純度指標\", max_depth=決定木の深さ)\n",
        "モデル変数.fit(説明変数, 目的変数) # トレーニングデータ\n",
        "```\n",
        "\n",
        "ここで使っている設定値（詳しくはscikit-learn [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)参照）:\n",
        "- `n_estimators=100`: 「100」個の決定木を作る\n",
        "- `criterion=\"gini\"`: 不純度の指標として「ジニ不純度」を使う \n",
        "- `max_depth=3`: 木の深さの最大値を「3」に設定する\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUb7WplklcVR"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# モデルを作成\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_rf = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=3)\n",
        "\n",
        "# モデルをトレーニング\n",
        "model_rf.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model_rf, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model_rf.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model_rf.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMKI4YILgR6q"
      },
      "source": [
        "### 各説明変数の重要度"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YO9q15-gXNR"
      },
      "source": [
        "　線形回帰では$\\beta$の値が推定されるのでどの$x$(説明変数)が重要なのかの指標になっていました。しかしランダムフォレストでは、$\\beta$は計算されません。\n",
        "　\n",
        "\n",
        "　この時、ランダムフォレストでは別の指標が用いられることがあります。\n",
        "\n",
        "　まず、ランダムフォレストでは、**各説明変数の重要度（Feature importances）**というものが計算されます。これは、クラス分けに重要な説明変数を表したもので、学習の時に決定木における分割が説明変数ごとにどのくらいうまくいっているかを定量化して表しています。\n",
        "\n",
        "　また、学習したモデルを元に、**Permutation Importance**と呼ばれる手法を使って、ある説明変数がどれだけモデルの予測精度向上に寄与しているのかを調べることも出来ます。\n",
        "\n",
        "　「Permutation Importance」の概要は次のとおりです。\n",
        "\n",
        "1. 説明変数をひとつ選びます。\n",
        "\n",
        "1. 選んだ説明変数のデータをシャッフルします。\n",
        "1. その結果、精度がどの程度下がるかを調べます。\n",
        "  - ほとんど精度が変わらないようであれば、その説明変数の重要度は低い\n",
        "  - 精度が劇的に下がるようであれば、その重要度は高い\n",
        "1. 1-3をすべての説明変数についておこないます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/permutation_importance.png?raw=true\" alt=\"permutation_importance\" height=\"300px\">\n",
        "\n",
        "\n",
        "```python\n",
        "# 各説明変数の重要度\n",
        "モデル変数.feature_importances_\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29o_xM1jlH87"
      },
      "source": [
        "# 各説明変数の重要度\n",
        "print('Importances:', model_rf.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KGTyA8OeOrm"
      },
      "source": [
        "### 実習5\n",
        "\n",
        "　`RandomForestClassifier()`内のオプション`max_depth`の値を変更して、その実行結果がどのようになるか観察してください。\n",
        "\n",
        "変更するオプション:\n",
        "- 決定木の深さ `max_depth=None`（上限なし）に変更\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYSuo79GeQ0y"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) ==============\n",
        "# モデルを作成\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model2_rf = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=3)\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルをトレーニング\n",
        "model2_rf.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_rf, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_rf.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_rf.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4t7glyTeRPy"
      },
      "source": [
        "#### 解答例"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wO3ohtheSmy"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# ============== 編集エリア(start) ==============\n",
        "# モデルを作成\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model2_rf = RandomForestClassifier(n_estimators=100, criterion=\"gini\", max_depth=None)\n",
        "# ============== 編集エリア(end) ==============\n",
        "# モデルをトレーニング\n",
        "model2_rf.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model2_rf, xlabel=\"x1\", ylabel=\"x2\")\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model2_rf.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model2_rf.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRyJ8gGThGHV"
      },
      "source": [
        "## ニューラルネットワーク Neural network\n",
        "\n",
        "　**ニューラルネットワーク（Neural network）**は、画像認識や自然言語処理などの分野、いわゆる人工知能と呼ばれる分野でよく使われている機械学習手法です（例をあげると、Google翻訳やデジタルカメラやスマホカメラの顔認識機能、など）。\n",
        "\n",
        "　**神経細胞が信号を伝達する仕組みを模した機械学習アルゴリズム**が使われています。この予測モデルからは、各分類クラスに含まれる確率が出力されます。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/neural_network.png?raw=true\" alt=\"neural_network\" height=\"250px\">\n",
        "\n",
        "　なお、ニューラルネットワークには多くの種類があります（参考: [The Neural Network Zoo](https://www.asimovinstitute.org/neural-network-zoo/) / THE ASIMOV INSTITUTE）。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MEfY5fbgJbx"
      },
      "source": [
        "### ニューラルネットワークのユニット"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VAmGOexiUz7"
      },
      "source": [
        "　ニューラルネットワークは、入力値（Input）である数値データが、入力層、中間層、出力層を経て、出力値（Output）に変換される機械学習手法です。各層は、1個以上の**ユニット**（上述の図では、$x_1$、$h_{11}$、$y$など、円で描かれているもの）で構成されています。\n",
        "\n",
        "　以下では、各層のユニットがどのような値を受け取るかを、どのような値を出力するかを説明しています。\n",
        "\n",
        "<small>*※ 各層には、バイアス（誤差）を表すユニットもありますが、ここではバイアスユニットを省略しています。*</small>\n",
        "\n",
        "\n",
        "\n",
        "#### ◆ 入力層（Input layer）\n",
        "　この層のユニットの数は、説明変数の数と同じです。各ユニットは、説明変数の数値をそのまま受け取ります。入力層の各ユニットが受け取った数値は、次の層（隠れ層 第1層）の入力データになります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_input_layer.png?raw=true\" alt=\"NN_input_layer\" height=\"300px\">\n",
        "\n",
        "\n",
        "\n",
        "#### ◆ 隠れ層（Hidden layers）\n",
        "　隠れ層 第1層の各ユニットは、入力層のすべてのユニットからデータを受け取ります。その際、入力層の各ユニットから出力される数値にいくらかの**重み（weight）**をかけた値を受け取ります。さらに、受け取った数値の合計値を **活性化関数（Activation function）** と言われる関数で変換して、次の層への出力値を発生させます。\n",
        "\n",
        "\\begin{align}\n",
        "例)　t_{11} &= w_{x_{1}\\to h_{11}} x_{1} + w_{x_{2}\\to h_{11}} x_{2} \\\\\n",
        "h_{11} &= activation(t_{11})\n",
        "\\end{align}\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_hidden_layer.png?raw=true\" alt=\"NN_hidden_layer\" height=\"300px\">\n",
        "\n",
        "よく使われる活性化関数:\n",
        "- ReLU関数\n",
        "- シグモイド関数（ロジスティック関数）\n",
        "- tanh関数\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_activation_function.png?raw=true\" alt=\"NN_activation_function\" height=\"200px\">\n",
        "\n",
        "　「活性化関数」は、非線形（直線ではない）関数を基本的には使います。この理由については、下記の「活性化関数に非線形関数を使うのはなぜか？」を参照してください。\n",
        "\n",
        "　隠れ層　第2層以降についても基本的には同じです。前層のすべてのユニットから出力される数値に重みをかけた値を受け取ります。そして、活性化関数によって、数値の変換をおこなった後、次の層に向けて出力します。\n",
        "\n",
        "\\begin{align}\n",
        "例)　t_{21} &= w_{h_{11}\\to h_{21}} h_{11} + w_{h_{12}\\to h_{21}} h_{12} + w_{h_{13}\\to h_{21}} h_{13} \\\\\n",
        "h_{21} &= activation(t_{21})\n",
        "\\end{align}\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_hidden_layer2.png?raw=true\" alt=\"NN_hidden_layer2\" height=\"300px\">\n",
        "\n",
        "#### ◆ 出力層（output layer）\n",
        "　\n",
        "　出力層も、数値の受け取りに関しては、隠れ層と同じです: 隠れ層の最後の層にある各ユニットから出力される数値に重みをかけた値を受け取ります。受け取った数値の合計値を**ソフトマックス関数（Softmax関数）**で変換します。\n",
        "\n",
        "ソフトマックス関数についての詳細は省きますが、変換後の値を各クラスに所属する「確率(0~1の値)」とみなして予測値を得ます。\n",
        "\n",
        "\\begin{align}\n",
        "例)　t_{y} &= w_{h_{31}\\to y} h_{31} + w_{h_{32}\\to y} h_{32} + w_{h_{33}\\to y} h_{33} \\\\\n",
        "y &= softmax(t_{y})\n",
        "\\end{align}\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_output_layer.png?raw=true\" alt=\"NN_output_layer\" height=\"300px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92eNbSuDyvVN"
      },
      "source": [
        "### ニューラルネットワークの重みの更新"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVJ1TjqFUQ1L"
      },
      "source": [
        "　各ユニット間の重みは、モデルの学習で随時更新されます。非線形な活性化関数が間に入っているため、解析的に（計算でピンポイントに）それぞれの重み$w$の最適値を求めることはできません。そこで、確率的勾配降下法などの反復法を使って、重みを徐々に更新し、最適値を探します。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/NN_update_w.png?raw=true\" alt=\"NN_update_w\" height=\"450px\">\n",
        "\n",
        "ニューラルネットワークでは**誤差逆伝播法**と呼ばれる手法を用いる事で重みを更新していく計算の効率が大幅に上昇しており、実用的なものとなっています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0bRHgtx6g-K"
      },
      "source": [
        "### 活性化関数に非線形関数を使うのはなぜか？\n",
        "<small>*※ 読み飛ばしてOKです。*</small>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KAkPb2m7E_R"
      },
      "source": [
        "　この答えは、活性化関数に線形関数（直線になる関数）を使うと、わざわざ隠れ層を作る意味がなくなるからです。\n",
        "\n",
        "　簡単な例をあげると、次のような3層のネットワークがあったとします（各層のユニット数は1個）。\n",
        "- 第1層: $t_{1} = a_{1}x$\n",
        "- 第1層の活性化関数: $y_{1} = b_{1}t_{1} $\n",
        "- 第2層: $t_{2} = a_{2}y_{1}$\n",
        "- 第2層の活性化関数: $y_{2} = b_{2}t_{2} $\n",
        "- 第3層: $t_{3} = a_{3}y_{2}$\n",
        "- 第3層の活性化関数: $y_{3} = b_{3}t_{3} $\n",
        "\n",
        "　この6つの式を1行で書くと、\n",
        "$$y_3 = b_{3}(a_{3}(b_{2}(a_{2}(b_{1}(a_{1}x))))) = a_{1}a_{2}a_{3}b_{1}b_{2}b_{3}x$$\n",
        "\n",
        "$c = a_{1}a_{2}a_{3}b_{1}b_{2}b_{3}$とすると、\n",
        "\n",
        "$$ y_3 = c x $$\n",
        "\n",
        "となり、単層の式で表すことが可能になります。各層のユニット数が増えても同様に単層の形に収束してしまいます。\n",
        "\n",
        "　活性化関数に線形関数を使った場合、多層のニューラルネットワークを構築しても計算量が増えるだけで、まったくメリットはありません。そのため、活性化関数には非線形関数を使います。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfOF1-dRrF0g"
      },
      "source": [
        "### 実装\n",
        "\n",
        "　scikit-learnを使ったニューラルネットワークの実装方法は次のとおりです。\n",
        "\n",
        "```python\n",
        "# 書き方\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "モデル変数 = MLPClassifier(hidden_layer_sizes=(第1層ユニット数,第2層ユニット数,第3層ユニット数,),\n",
        "                  , solver=\"最適化手法\", learning_rate_init=学習率, max_iter=トレーニング回数)\n",
        "モデル変数.fit(説明変数, 目的変数) # トレーニングデータ\n",
        "```\n",
        "\n",
        "以下で使っているハイパーパラメータ（詳しくはscikit-learn [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)参照）:\n",
        "\n",
        "- `hidden_layer_sizes=(3,3,3,)`: 「3層各3ユニット」のニューラルネットワークを作る\n",
        "- `solver=\"sgd\"`: 最適化手法として「確率的勾配降下法」を使用する\n",
        "- `learning_rate_init=0.1`: 学習率を「0.1」に設定する\n",
        "- `max_iter=100`: 学習回数を「100」回に設定する\n",
        "\n",
        "<small>*※ ニューラルネットワーク用のライブラリは、他にも多数あります（TensorFlow、Keras、Pytorch、Caffe、Jaxなど）。より高度なニューラルネットワークを構築するには、それらのライブラリを使う方が良いでしょう。*</small>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP9aoNr2W_NI"
      },
      "source": [
        "# 使用するデータ\n",
        "X_train, Y_train = x_train_ss, y_train_le  # トレーニングデータ\n",
        "X_test, Y_test  = x_test_ss, y_test_le   # テストデータ\n",
        "\n",
        "# モデルを作成\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "model_nn = MLPClassifier(hidden_layer_sizes=(3,3,3,), solver=\"sgd\", \n",
        "                        learning_rate_init=0.1, max_iter=1000)\n",
        "# モデルをトレーニング\n",
        "model_nn.fit(X_train, Y_train)\n",
        "\n",
        "# グラフ\n",
        "from classification import draw_decision_boundary\n",
        "draw_decision_boundary(x1_train=X_train[:,0], x2_train=X_train[:,1], \n",
        "                       x1_test=X_test[:,0], x2_test=X_test[:,1], \n",
        "                       y_train=Y_train, y_test=Y_test, \n",
        "                       labels=le.inverse_transform(Y_train), \n",
        "                       model=model_nn, xlabel=\"x1\", ylabel=\"x2\")\n",
        "\n",
        "# モジュール間の各コネクションの重み\n",
        "# print(\"Coefficients=\", model_nn.coefs_)\n",
        "\n",
        "# 評価（正解率）\n",
        "print(\"training data: \", model_nn.score(X_train, Y_train)) # トレーニングデータ\n",
        "print(\"test data: \",    model_nn.score(X_test, Y_test))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wg2BXIcmSUC"
      },
      "source": [
        "## モデルの評価\n",
        "\n",
        "　二値分類（A / not A）の場合、実測値（実ラベル）とモデルから得られた予測値（予測ラベル）の一致/不一致を調べると、次の4つのタイプの結果が得られます。\n",
        "\n",
        "||予測ラベル=A|予測ラベル=not A|\n",
        "|:---|:---|:---|\n",
        "|**実ラベル=A**|真陽性<br>True positive: TP|偽陰性<br>False negative: FN|\n",
        "|**実ラベル=not A**|偽陽性<br>False positive: FP|真陰性<br>True negative: TN|\n",
        "\n",
        "　よく使われる評価指標は、実ラベルと予測ラベルがどれだけ一致していたかを表す指標、**正解率（Accuracy）**です。\n",
        "\n",
        "$$Accuracy = \\frac{TP+TN}{TP + FN + FP + TN}$$\n",
        "\n",
        "　今回の実習では、各モデルの精度を正解率で評価していきました。再度、それぞれの正解率を確認しておきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnys3JLpmX-B"
      },
      "source": [
        "# ロジスティック回帰モデル\n",
        "print(\"=== Logistica regression ===\")\n",
        "print(\"training data: \", model_lr.score(x_train_ss, y_train_le)) # トレーニングデータ\n",
        "print(\"test data: \",    model_lr.score(x_test_ss, y_test_le))  # テストデータ\n",
        "\n",
        "# サポートベクトルマシンモデル\n",
        "print(\"=== Support vector machine ===\")\n",
        "print(\"training data: \", model_svm.score(x_train_ss, y_train_le)) # トレーニングデータ\n",
        "print(\"test data: \",    model_svm.score(x_test_ss, y_test_le))  # テストデータ\n",
        "\n",
        "# 決定木モデル\n",
        "print(\"=== Decision tree ===\")\n",
        "print(\"training data: \", model_dt.score(x_train_ss, y_train_le)) # トレーニングデータ\n",
        "print(\"test data: \",    model_dt.score(x_test_ss, y_test_le))  # テストデータ\n",
        "\n",
        "# ランダムフォレストモデル\n",
        "print(\"=== Random forest ===\")\n",
        "print(\"training data: \", model_rf.score(x_train_ss, y_train_le)) # トレーニングデータ\n",
        "print(\"test data: \",    model_rf.score(x_test_ss, y_test_le))  # テストデータ\n",
        "\n",
        "# ニューラルネットワークモデル\n",
        "print(\"=== Neural network ===\")\n",
        "print(\"training data: \", model_nn.score(x_train_ss, y_train_le)) # トレーニングデータ\n",
        "print(\"test data: \",    model_nn.score(x_test_ss, y_test_le))  # テストデータ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q90BpMEsCfm7"
      },
      "source": [
        "### その他の評価指標"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRO7klaxCnAs"
      },
      "source": [
        "　先ほどの表を下記に再度載せています。この行列は、**混同行列（Confusion matrix）**と呼ばれています。\n",
        "\n",
        "||予測ラベル=A<br>（ポジティブラベル）|予測ラベル=not A<br>（ネガティブラベル）|\n",
        "|:---|:---|:---|\n",
        "|**実ラベル=A<br>（ポジティブラベル）**|真陽性<br>True positive: TP|偽陰性<br>False negative: FN|\n",
        "|**実ラベル=not A<br>（ネガティブラベル）**|偽陽性<br>False positive: FP|真陰性<br>True negative: TN|\n",
        "\n",
        "　正解率のほかにも、評価指標はあります。ここでは、おもなものリストアップします。\n",
        "\n",
        "- 精度 Precision  \n",
        "ポジティブラベルと予測されたもののうち、実際にポジティブラベルであるサンプルの割合\n",
        "$$Precision = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "- False discovery rate (FDR)\n",
        "ポジティブラベルと予測されたもののうち、誤って予測されたサンプルの割合。なお、ゲノム解析のひとつ、遺伝子発現解析でよく使われる指標です。\n",
        "$$FDR = \\frac{FP}{TP + FP} = 1 - Precision$$ \n",
        "\n",
        "- 検出率 Recall / 真陽性率 True positive rate / 感度 Sensitivity  \n",
        "ポジティブラベルのサンプルのうち、ポジティブラベルと予測された割合\n",
        "$$Recall = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "- 偽陰性率 False negative rate (FNR)  \n",
        "ポジティブラベルのサンプルのうち、ネガティブラベルと予測された割合\n",
        "$$FNR = \\frac{FP}{FP + TN} = 1 - Recall$$\n",
        "\n",
        "- 偽陽性率 False positive rate (FPR)  \n",
        "ネガティブラベルのサンプルのうち、ポジティブラベルと予測された割合\n",
        "$$FPR = \\frac{FP}{FP + TN}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWkirafnOqK-"
      },
      "source": [
        "　以下では、混同行列（Confusion matrix）を作るコードを紹介しています。また、精度（Precision）をscikit-learnを使って調べる方法も紹介しています。\n",
        "\n",
        "```python\n",
        "# 混同行列\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(実ラベル、予測ラベル)\n",
        "\n",
        "# 精度\n",
        "from sklearn.metrics import precision_score\n",
        "precision_score(実ラベル、予測ラベル)\n",
        "```\n",
        "\n",
        "　例として、ロジスティック回帰モデルからの予測ラベルを使っています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJYl4kHBCmV2"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, precision_score\n",
        "\n",
        "# 実測値\n",
        "Y_train = y_train_le\n",
        "Y_test = y_test_le\n",
        "\n",
        "# ロジスティック回帰モデルからの予測値\n",
        "y_train_pred = model_lr.predict(x_train_ss)\n",
        "y_test_pred = model_lr.predict(x_test_ss)\n",
        "\n",
        "# トレーニングデータの混同行列\n",
        "cm = confusion_matrix(Y_train, y_train_pred)\n",
        "pre = precision_score(Y_train, y_train_pred, average=None)\n",
        "print(\"=== training data ===\")\n",
        "print(cm)\n",
        "print(\"precision=\", pre)\n",
        "\n",
        "# テストデータの混同行列\n",
        "cm = confusion_matrix(Y_test, y_test_pred)\n",
        "pre = precision_score(Y_test, y_test_pred, average=None)\n",
        "print(\"=== test data ===\")\n",
        "print(cm)\n",
        "print(\"precision=\", pre)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbRvxEtJRxhb"
      },
      "source": [
        "## 多クラス(Multi class)の分類"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0pK6DcER0y6"
      },
      "source": [
        "　ここでは2種類の方法を紹介します。\n",
        "\n",
        "- 一対他 One-vs-Rest (OvR)  \n",
        "　各クラスに対して、2クラスの分類をおこないます（例えば、「種Aかそれ以外か」「種Bかそれ以外か」「種Cかそれ以外か」）。そうすると、各クラスである予測確率が得られます。そのうち、最も予測確率が高いクラスを「予測値」として採用します。\n",
        "  - 各確率の合計が1にならないこともあります\n",
        "  - Aの確率 $P_{A} = 0.70$、A以外の確率 $P_{A} = 0.30$\n",
        "  - Bの確率 $P_{B} = 0.20$、B以外の確率 $P_{B} = 0.80$\n",
        "  - Cの確率 $P_{C} = 0.01$、C以外の確率 $P_{C} = 0.99$\n",
        "  - 予測ラベル: A\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/OvR.png?raw=true\" alt=\"OvR\" height=\"200px\">\n",
        "\n",
        "- Multinomial  \n",
        "　各クラスの予測確率を同時に得る方法です。最も確率が高いものを「予測値」として採用します。\n",
        "  - 各確率の合計は1になります\n",
        "  - Aの確率 $P_{A} = 0.70$\n",
        "  - Bの確率 $P_{B} = 0.25$\n",
        "  - Cの確率 $P_{C} = 0.05$\n",
        "  - 予測ラベル: A\n",
        "\n",
        "　どの方法が使えるかについては、分類アルゴリズムや最適化手法で決まります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3aTxJmKTtjq"
      },
      "source": [
        "---\n",
        "\n",
        "## まとめ\n",
        "\n",
        "　今回、前処理と5つのアルゴリズムの概要、その実装方法、モデルの評価方法を勉強しました。\n",
        "\n",
        "　まず、前処理では、**データの分割**や**スケーリング**、**ラベル（目的変数）の数値変換**をおこないました。\n",
        "\n",
        "　その次に、5つのアルゴリズムを勉強しました。それぞれのアルゴリズムは、3行のコードで実装できます。\n",
        "- **ロジスティック回帰**: あるクラスに分類される確率を求める手法\n",
        "\n",
        "- **サポートベクトルマシン**: 最も近いデータまでの距離（マージン margin）が最大となる境界を作り、データを二分する方法\n",
        "\n",
        "- **決定木**: 説明変数に対する問い（True or False）を繰り返して、木構造の分類モデルを作る方法\n",
        "\n",
        "- **ランダムフォレスト**: 各決定木の予測結果を多数決して、最終的な予測値を決める手法\n",
        "\n",
        "- **ニューラルネットワーク**: 神経細胞が信号を伝達する仕組みを模した機械学習アルゴリズム\n",
        "\n",
        "　最後に、ラベルの**正解率**でそれぞれのモデルの評価をおこないました。\n",
        "\n",
        "　このテキストでは、取り上げませんでしたが、分類アルゴリズムは他にもたくさんあります（ナイーブベイズ、k最近傍法など）。興味があれば、勉強してみてください。"
      ]
    }
  ]
}