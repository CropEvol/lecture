{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L09_ML_regression_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HhiREQJGpwnr",
        "-bAnZL1-Cb09",
        "x-jpAVs1M7jP"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9fGSNj8QRzB"
      },
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/pw/AM-JKLVhTn_UySwMdfMwXvoq8l3VN7IkrY9cwtH2YJVMxAlMznUBWC9IpFtgPRIyfAXru4oykkYD-1WjWi0Ao5XgkB9JICvzDBcfn0L_5X2_KOOppsURK5DfSifCC-s7Vx5oQrBUn_BNWn_hfAPdhlVbKQGE=w1097-h235-no?authuser=0\" alt=\"2021年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">\n",
        "\n",
        "<div align=\"right\"><a href=\"https://github.com/CropEvol/lecture#section2\">実習表ページに戻る</a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtPzXrSAtBm8"
      },
      "source": [
        "# 機械学習 - 線形回帰（2）-\n",
        "\n",
        "　[機械学習 - 線形回帰（1）](https://colab.research.google.com/github/CropEvol/lecture/blob/master/textbook_2021/L08_ML_regression_1.ipynb)では、説明変数（1遺伝子の発現量）をひとつ使い、その遺伝子発現量から表現型値を予測する線形回帰モデルを構築しました。しかし、そのモデルの評価値（決定係数 $R^2$ ）は低く、良いモデルではありませんでした。\n",
        "\n",
        "　今回、使用する説明変数を増やして、より良い線形回帰モデルを作ってみましょう。\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/regression_models.png?raw=true\" alt=\"regression_models\" height=\"400px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUijXJo0q5in"
      },
      "source": [
        "---\n",
        "\n",
        "## 今回の実習内容\n",
        "\n",
        "1. 線形回帰 Linear regression（2変数以上）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhiREQJGpwnr"
      },
      "source": [
        "### 線形回帰モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czigWGYAp1uO"
      },
      "source": [
        "　ここでおこなう解析の目標は次のとおりです。\n",
        "> 遺伝子発現量から表現型値を予測する線形回帰モデルを作る\n",
        "\n",
        "　「線形回帰モデル」とは何か？　遺伝子発現量（$x$）と表現型値（$y$）の関係を次の方程式で表したモデルのことです。\n",
        "\n",
        "線形回帰モデルの方程式: \n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/09_statistics/data/regression_base.png?raw=true\" alt=\"regression\" height=\"130px\">\n",
        "\n",
        "- **目的変数（objective variable）**: 予測される変数$y$。今回の場合、表現型値。\n",
        "- **説明変数（explanatory variable）**: 予測に使う変数$x$。今回の場合、各遺伝子発現量。\n",
        "- **偏回帰係数（coefficient）**: 各説明変数の重み。目的変数の予測にその変数がどれぐらい影響するかを示す指標。\n",
        "- **誤差（intercept; 切片）**: 説明変数以外の影響を示す項。\n",
        "\n",
        "　偏回帰係数や誤差の値は、データから推定します。「線形回帰モデルを作る」とは、それらの推定値を得ることに相当します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOaYdFcfbFMX"
      },
      "source": [
        "### 実習で使用するデータセット\n",
        "\n",
        "　次のコードセルを実行して、データファイル（[gene_expression.csv](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/gene_expression.csv)）をダウンロードしてください。\n",
        "\n",
        "ファイルの詳細:\n",
        "- ファイル名: gene_expression.csv\n",
        "- カンマ区切りテキストファイル\n",
        "- 100行（100サンプル） x 51列（表現型値 + 50個の遺伝子発現量）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKU2jUPHbXDV"
      },
      "source": [
        "### このコードセルは実行のみ ###\n",
        "# サンプルデータのダウンロード\n",
        "!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/gene_expression.csv\n",
        "\n",
        "# pandasで読み込み\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (補足)乱数について\n",
        "乱数とは０から９までの数字が不規則かつ等確率に現れるように配列されたものです。\n",
        "学習の過程では様々な点で乱数が使用されています。\n",
        "例えば、トレーニングデータとテストデータにデータを7:3に分割する際に、乱数に基づいてデータをランダムに選択する時に利用されています。\n",
        "また、モデルの学習においてもパラメータの調整の変遷や初期値などに乱数が使用されていることも多いです。\n",
        "\n",
        "下記のコードを動かしてみると、毎回トレーニングデータとテストデータの分けられ方が異なることが分かります。乱数は文字通りランダムに生成されるので、コードを動かすたびに異なる値が生成されています。\n",
        "\n"
      ],
      "metadata": {
        "id": "hNJxjbgbQ6sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数の確認\n",
        "from sklearn.model_selection import train_test_split\n",
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "print(x_train, x_test)"
      ],
      "metadata": {
        "id": "5nvNLBeeSvkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この乱数は固定することが出来ます。\n",
        "乱数を利用している関数にはseedやrandom_seed, random_stateという引数が用意されており、この値を決めておくことで同じ乱数のもと、関数を実行できます。\n",
        "つまり、同じテストデータとトレーニングデータの分け方を再現可能になります。"
      ],
      "metadata": {
        "id": "5ARZF6UjS6SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数を固定\n",
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "print(x_train, x_test)\n",
        "print(y_train, y_test)"
      ],
      "metadata": {
        "id": "-n2eM-07VRee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "サイエンスにおいて結果の「再現性」というものは非常に重要です。再現性というのは、\n",
        "\n",
        "「XXという実験をYYという条件の下で行うと、ZZという結果が得られた」という実験結果が、何度繰り返しても、異なる実験者が行っても再現できるという事を示します。\n",
        "\n",
        "一昔前にSTAP細胞に関する事件が起きましたが、あの件は他の研究者によってSTAP細胞の再現ができなかったため、虚偽ではないかとなったわけですね。\n",
        "\n",
        "これは実験での話だけでなく、今皆さんがやっている計算やモデルの学習においても同じです。科学でこれらの知識を利用するなら同じように「再現性」を取る必要があります。\n",
        "そのため、通常は乱数を固定してデータの分け方や学習を行い、使用したライブラリのバージョンや乱数を決めるrandom_seedなどの値はしっかり記録しておいて、どの様な乱数のもとデータを生成し学習していったのかを確実に再現できる様にしておく必要があります。"
      ],
      "metadata": {
        "id": "_wMFI4DyVTs4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEg9g2e0UYch"
      },
      "source": [
        "## 2. 線形回帰 Linear regression（2変数以上）\n",
        "\n",
        "　使用する説明変数を3つに増やして、良いモデルを作ることが出来るか試してみましょう。\n",
        "\n",
        "使用する説明変数\n",
        "- gene_7の遺伝子発現量 `gene_7`\n",
        "- gene_11の遺伝子発現量 `gene_11`\n",
        "- gene_28の遺伝子発現量 `gene_28`\n",
        "\n",
        "> 3個の遺伝子発現量から表現型値（phenotype）を予測する線形回帰モデルを作る\n",
        "  $$ y = \\beta_{gene\\_7} x_{gene\\_7} + \\beta_{gene\\_11} x_{gene\\_11} + \\beta_{gene\\_28} x_{gene\\_28} + e $$\n",
        "\n",
        "　説明変数の個数が2つ以上になっても、基本的な手順は1変数のときと同じです。異なる点は、前処理に**スケーリング（scaling）**と呼ばれる操作が入る点です。\n",
        "\n",
        "- 2-1. 前処理\n",
        "  - **スケーリング（scaling）**\n",
        "- 2-2. モデルの選択\n",
        "- 2-3. モデルの学習\n",
        "- 2-4. モデルの評価\n",
        "- 2-5. 予測\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/supervised_learning_process.png?raw=true\" alt=\"supervised_learning_process\" height=\"60px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwH2Nub0sN5O"
      },
      "source": [
        "### 2-1. 前処理"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### スケーリングとは\n",
        "説明変数の値のスケールを変化させ共通のスケールに変換することで、各変数間の関係などを分かりやすくすることです。\n",
        "\n",
        "試験でよく計算される偏差値もスケーリングの一例です。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/scaling_exam.png?raw=true\" alt=\"scaling\" height=\"200px\">"
      ],
      "metadata": {
        "id": "cb_U0Bw7JbUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### スケーリングの目的\n",
        "\n",
        "今後講義で紹介していく手法である、決定木などの手法では影響を受けませんが、ロジスティック回帰や主成分分析など、様々な手法で入力データのスケールは悪影響を与える場合があります。特に、このノートで後ほど紹介するLassoやRidge回帰と呼ばれる手法では悪影響が大きいです。\n",
        "また、複数の説明変数（身長や体重など）のスケールを揃えずにモデルの学習をおこなうと、得られた学習結果を解釈する（影響力の大きい説明変数がどれかを判断する）ことも難しくなります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/scaling.png?raw=true\" alt=\"scaling\" height=\"400px\">\n"
      ],
      "metadata": {
        "id": "e35WWmXKX6uy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-BQtibXe6NZ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 使用する変数\n",
        "x = np.array(df.loc[:,[\"gene_7\", \"gene_11\", \"gene_28\"]]) # 説明変数3つ\n",
        "y = np.array(df[\"phenotype\"])                             # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"training: \", x_train.shape, y_train.shape)\n",
        "print(\"test: \", x_test.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuhKmuuSiKvv"
      },
      "source": [
        "\n",
        "スケーリングにはおもに2つの方法があります。 \n",
        "- **正規化 normalization**: 各変数の値を0 ~ 1の範囲に変換する  \n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()  # 変数名「mms」の部分は任意\n",
        "x_train_mms = mms.fit_transform(x_train)\n",
        "x_test_mms = mms.transform(x_test)\n",
        "```\n",
        "\n",
        "- **標準化 standardization**: 各変数の値を平均値0、標準偏差1になるように変換する\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler() # 変数名「ss」の部分は任意\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "```\n",
        "\n",
        "　ここでは、標準化をおこないます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLPw_MejxlKJ"
      },
      "source": [
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# 確認\n",
        "#x_train_ss\n",
        "x_test_ss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewlyUXtviu-"
      },
      "source": [
        "### 2-2 ~ 2-3. モデルの選択・学習\n",
        "\n",
        "　線形回帰モデルを用意し、スケーリング後のトレーニングデータでモデルの学習をおこないます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ICaAnSBi6N9"
      },
      "source": [
        "# モデル選択＆学習\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr.coef_\n",
        "e = lr.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "print(\"Intercept=\"  , e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAnZL1-Cb09"
      },
      "source": [
        "### 各項の係数 Coefficient について"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rR-WxRnCrQ8"
      },
      "source": [
        "　$$ 線形回帰モデル: f(x) = \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + ... + \\beta_mx_m+ e $$\n",
        "\n",
        "　各項の係数$\\beta$（Coefficient）は、それぞれの説明変数$x_1, x_2...x_m$が目的変数$y$にどの程度影響を持っているかを表しています。影響の大きさを知りたいのであれば、係数の絶対値をみます。影響の方向性（プラスかマイナスか）を知りたいのであれば、係数の符号をみます。\n",
        "\n",
        "\n",
        "　先程説明した通り、前処理でおこなったスケーリングはこの係数に影響してきます。モデルの各係数の値を調べたい場合（例えば、各遺伝子の発現が表現型値にどの程度関わっているか調べたい場合）、スケーリングをせずに解析すると、誤った結果を得ることになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns7-YTWL8A7Z"
      },
      "source": [
        "### 2-4. モデルの評価\n",
        "\n",
        "　モデルの評価でも、スケーリング後のトレーニングデータ・テストデータを使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKoQUHTq4QZc"
      },
      "source": [
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FGjVcn8D4f"
      },
      "source": [
        "### 2-5. 予測\n",
        "\n",
        "　まだ改善の余地はありそうですが、1変数のときよりも良いモデルが得られました。新しい入力データをこの予測モデルに入れて、その予測値を調べてみましょう。\n",
        "\n",
        "　ここでは、新しく3サンプルの遺伝子発現データが得られたとして、その表現型値を予測します。\n",
        "\n",
        "$$\n",
        "(gene_{7}, gene_{11}, gene_{28}) = (x_1, x_2, x_3) = (10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)\n",
        "$$\n",
        "\n",
        "予測の手順:  \n",
        "1. 新しいデータのスケーリングをします。トレーニングデータと同じ基準で、スケーリングする必要があります（テストデータのスケーリングと同じ操作です）。\n",
        "```python\n",
        "スケーリング名.transform(新しいデータ)\n",
        "```\n",
        "\n",
        "1. `predict`で新しいデータの予測値を得ます。\n",
        "```python\n",
        "モデル名.predict(スケーリング後のデータ)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4fPHC7p8Gqn"
      },
      "source": [
        "# 新しいデータ\n",
        "x_new = np.array([(10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)])\n",
        "\n",
        "# スケーリング（標準化 standardization）\n",
        "x_new_ss = ss.transform(x_new)\n",
        "\n",
        "# 予測\n",
        "lr.predict(x_new_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実習1\n",
        "\n",
        "今回の講義では遺伝子7,11, 28の発現量のデータを使って線形モデルを作成してみました。\n",
        "\n",
        "皆さんは自分の好きな遺伝子のセット(3つ以上でもOK)を使って線形モデルを作成し、\n",
        "1. 前処理(スケーリング)\n",
        "2. モデルの選択\n",
        "3. モデルの学習\n",
        "4. モデルの評価\n",
        "までを行ってみて下さい。\n",
        "\n",
        "そして、4.のモデルの評価の結果をもとに色んな遺伝子セットを試してみて、どの遺伝子の組み合わせから形質データを説明するのが一番良さそうか試してみて下さい。\n",
        "\n"
      ],
      "metadata": {
        "id": "2uKhwnYomPqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 前処理\n",
        "# 使用する変数\n",
        "x = \n",
        "y = \n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "print(\"training: \", x_train.shape, y_train.shape)\n",
        "print(\"test: \", x_test.shape, y_test.shape)\n",
        "\n",
        "# スケーリング\n",
        "ss = \n",
        "x_train_ss = \n",
        "x_test_ss =\n",
        "\n",
        "# 線形回帰モデルの準備\n",
        "lr2 =\n",
        "\n",
        "# モデルの学習\n",
        "lr2\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = \n",
        "r2_test_ss =\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "metadata": {
        "id": "Rim6DU0xPGcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MiVC5B6IE0L"
      },
      "source": [
        "## 遺伝子50個を使った予測モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_UEcMeKITSm"
      },
      "source": [
        "　さて、それでは次に50個の遺伝子すべてのデータを使って予測モデルを作ってみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o15K_qRBHEdv"
      },
      "source": [
        "# 遺伝子50個を使った線形回帰モデル\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "# random_state ... 乱数を固定することが出来る\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr50 = LinearRegression()\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "# print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トレーニングデータの決定係数は高くなりましたが、テストデータの決定係数は非常に低い値になります（**過学習(overfitting)**と言われる状態です）。良いモデルを作るためには、使用する遺伝子を選ぶ必要がありそうですね。"
      ],
      "metadata": {
        "id": "ZzUWxPSHoS1V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jpAVs1M7jP"
      },
      "source": [
        "### モデル比較"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-opZ32NM8aw"
      },
      "source": [
        "　ここでは、モデル間の比較で重要な考え方を紹介します。\n",
        "\n",
        "```\n",
        "精度が高いモデルが良い\n",
        "かつ、汎用性の高いモデルが良い。(未知のデータに対しても予測精度が保たれている)\n",
        "```\n",
        "\n",
        "　機械学習では、新しいデータに対して上手く予測できるモデル、すなわち「精度が高い」モデルが好まれます（実習で使った決定係数$R^2$はモデルの精度を測る指標の一つです）。\n",
        "\n",
        "　一般的に、モデルの精度を高くしようとすると、複雑になる傾向にあります。特に、先程の50個の遺伝子を使った例の様に、回帰モデルでは変数を増やせば増やすほど残差平方和をいくらでも小さくすることが出来ます。しかし、無暗にパラメータ数(今回だと説明変数の数)を増やし、誤差なども無理やり説明しようとすると、他のデータに対して合わなくなる(Overfitting)問題が生じます。\n",
        "そこで，単に残差平方和の大小($R^2$)を比較するだけでなくパラメータ数も考慮した，評価基準が必要となります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/overfit.png?raw=true\" alt=\"scaling\" height=\"400px\">\n",
        "\n",
        "　精度とパラーメータ数の両方を考慮してモデルを評価する指標として、次のような指標があります。これらは、モデル間の比較に使われます。詳しく知りたい方は、統計モデリングの本を参照してください。\n",
        "- AIC(赤池情報量基準): 予測能力が最良のモデルを良いとする指標\n",
        "- BIC(ベイズ情報量規準): 真のモデルである確率が最も大きいモデルを良いとする指標\n",
        "\n",
        "また、モデルの複雑さで言うと、変数が少ない「単純なモデル」は、どの変数が強い影響を持っているのか等を理解しやすいという利点がある場合があり、精度だけを求めた複雑なモデルよりも好まれる場合が多々あります。\n",
        "\n",
        "「統計モデリング」の勉強におすすめの本: \n",
        "- [データ解析のための統計モデリング入門](https://www.iwanami.co.jp/book/b257893.html)\n",
        "- [Bayesian Analysis with Python](https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition)（原書）/ [Pythonによるベイズ統計モデリング](https://www.kyoritsu-pub.co.jp/bookdetail/9784320113374)（日本語訳）"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfittingを回避する方法の一例\n",
        "\n",
        "遺伝子50個を使ったモデルでは、学習データに対してはよくあてはまっているモデルを作成することが出来ましたが、過学習(Overfitting)してしまい、学習データ以外のデータに対して当てはまりが良くない結果になりました。\n",
        "\n",
        "今回の様に説明変数が50個ではなくより大きなデータセットだと更に大変になります。特に、手に入るサンプル数より多くの説明変数を考慮した線形回帰を行うと、ほぼ確実に過学習が生じます。\n"
      ],
      "metadata": {
        "id": "1EWvSh9Wojtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 説明変数の数に応じた予測式の変化例\n",
        "\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "def poly(intercept, coef, x):\n",
        "    return intercept + sum([w * x**(n + 1) for n, w in enumerate(coef)])\n",
        " \n",
        "xmin, xmax = -3, 1\n",
        "xlim_min, xlim_max = -4, 2\n",
        "ylim_min, ylim_max = -2, 4\n",
        " \n",
        " # データ数・説明変数の数・グラフとして表示する説明変数の設定\n",
        "n_data = 10\n",
        "n_features = 20\n",
        "n_terms_list = [2, 4, 6, 8, 10, 12]\n",
        " \n",
        "x = np.linspace(xmin, xmax, n_data)\n",
        "y = np.exp(x) + [rnd.uniform(-0.5, 0.5) for n in range(n_data)]\n",
        " \n",
        "df = pd.DataFrame(y, columns=['y'])\n",
        "for n in range(n_features):\n",
        "    df[\"x^{}\".format(n+1)] = x**(n+1)\n",
        "print(df.shape)\n",
        " \n",
        "fig, axs = plt.subplots(2, 3, figsize=(12, 6.4))\n",
        "axs_1d = axs.reshape(1, -1)[0]\n",
        " \n",
        "linreg = LinearRegression()\n",
        " \n",
        "x_graph = np.linspace(xlim_min, xlim_max)\n",
        " \n",
        "for ax, n_terms in zip(axs_1d, n_terms_list):\n",
        "    linreg.fit(df.iloc[:, 1:n_terms+1], df['y'])\n",
        "    y_linreg = poly(linreg.intercept_, linreg.coef_, x_graph)\n",
        "    ax.scatter(df['x^1'], df['y'], c='r', zorder=10)\n",
        "    ax.plot(x_graph, y_linreg, c='gray', linewidth=2,\n",
        "        label=\"n_terms={}\".format(n_terms))\n",
        " \n",
        "    ax.set_xlim(xlim_min, xlim_max)\n",
        "    ax.set_ylim(ylim_min, ylim_max)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.legend(loc='upper left')\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2wC10dyVRArT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "そのため、このような多くの変数から回帰モデルを作成する場合にはOverfittingを避けるような工夫をする必要があります。"
      ],
      "metadata": {
        "id": "fj3PKYPcSwLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 情報量基準等を使用して変数選択をする。\n",
        "\n",
        "全ての説明変数を使うのではなく、適切な変数の組み合わせを探してモデルを作る形です。さきほど皆さんに実習でやってもらった方法ですね。\n",
        "\n",
        "AICが良くなるモデルだったり、$R^2$を使う場合は学習データだけでなく、テストデータの評価も良くなるようなモデルの作成を目指します。\n",
        "\n",
        "モデルを作る際に、本当に目的変数(y)に影響を与えている重要な説明変数のみを上手く選択できれば良いモデルを作ることが出来そうです。"
      ],
      "metadata": {
        "id": "cqyOVGU5Jb0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 例えば、2つの遺伝子の組み合わせ全て試して、テストデータを用いたR^2が良いものを探す…とか。\n",
        "\n",
        "import itertools # 配列や組み合わせを計算してくれるライブラリ\n",
        "combinations = itertools.combinations(df.columns[1:], 2)\n",
        "\n",
        "# 数が多いので100個だけ\n",
        "for combination in list(combinations)[:100]:\n",
        "\n",
        "    # 使用する変数\n",
        "    import numpy as np\n",
        "    x = np.array(df.loc[:, combination]) # 説明変数50個\n",
        "    y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "    # データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "    # random_state ... 乱数を固定することが出来る\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "    # 標準化\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    ss = StandardScaler()\n",
        "    x_train_ss = ss.fit_transform(x_train)\n",
        "    x_test_ss = ss.transform(x_test)\n",
        "\n",
        "    # モデル選択＆学習\n",
        "    from sklearn.linear_model import LinearRegression\n",
        "    lr_tmp = LinearRegression()\n",
        "    lr_tmp.fit(x_train_ss, y_train)\n",
        "\n",
        "    # モデルの評価: 決定係数R2\n",
        "    r2_train_ss = lr_tmp.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "    r2_test_ss = lr_tmp.score(x_test_ss, y_test)    # テストデータ\n",
        "    print(combination, \"training: \", r2_train_ss, \"test: \"    , r2_test_ss)\n"
      ],
      "metadata": {
        "id": "eX-cY4_CJkV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このアプローチは説明変数の選択を別の基準でも行える場合に重要です。(遺伝子11は生物学的にyの値に影響を与えていることが知られている、など。)\n",
        "\n",
        "しかし、今回の様に役割なども良く分かっていない場合、良いモデルを作るためには50個の遺伝子の中から、膨大な遺伝子の組み合わせをちまちま探す必要が出てきてしまいます。そこで出て来るのが正則化と呼ばれる手法になります。"
      ],
      "metadata": {
        "id": "8e4mL0R5898I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 損失関数に正則化項を入れて過学習を抑制する。\n",
        "そこで出て来るのが正則化と呼ばれる手法になります。\n",
        "\n",
        "$$ y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + ...  + \\beta_{k} x_{k} + e $$\n",
        "\n",
        "線形回帰モデルの学習では、残差の二乗の合計値、**目的関数**である**残差平方和 (residual sum of squares)** が最も小さくなる係数 $\\beta$ と誤差 $e$ の直線を求めていました。\n",
        "\n",
        "$$ 残差平方和: \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 $$\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3eIFmh8PDRx64eFArwdgxO2CGt3PEi272ny1dyqAMue0un_yL_GMgZ0CsyvBnX4lEC9BfOEdfTNGsiEG-R4xZDPM9zMHwHcINcnQFxcdTmSgsF7LotLsBpwzs0S49fZtN1fQrbHY7JrB9m2kwuDGb9r=w815-h560-no?authuser=0\" alt=\"least squares\" height=\"180px\">\n",
        "\n",
        "正則化では、この残差平方和に正則化項(罰則項)というものを加え、この値を最小化するような係数 $\\beta$ と誤差 $e$ を求めます。\n",
        "\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|^p$$\n",
        "\n",
        "正則化項を加えることで、全体的に偏回帰係数 $\\beta$ の重みの影響を制限しようとする (係数の絶対値を小さくする) 形になります。\n",
        "\n",
        "言葉で説明すると...、学習データの誤差や外れ値まで説明できるように、実際はあまり重要ではない変数の偏回帰係数の値を大きくしたら、罰則化項によって目的関数が大きくなってしまうので、多くの学習データの説明に重要な影響を与えている様な説明変数の偏回帰係数にだけ重み付けを行えるようにしている。という形でしょうか。\n",
        "\n",
        "このノルムにおいて、p=1(L1ノルム)の場合をLasso回帰、p=2(L2ノルム)の場合をRidge回帰と呼び、どちらも回帰係数の重みに対して制限をしようとしますが、以下の様な特徴が異なります。\n",
        "\n",
        "**Ridge回帰**\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|^2$$\n",
        "\n",
        "係数の重みの強さを制限する効果を持ち、説明変数間の相関性が強い場合などにも予測式が不安定にならない傾向がある。\n",
        "\n",
        "**Lasso回帰**\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|$$\n",
        "\n",
        "Ridge回帰と同じく、係数の重みを制限し、正則化を強めるとともに係数がゼロとなり、選択された説明変数のみでモデルを構築できる。\n",
        "\n",
        "また、両者の正則化項を組み合わせた**ElasticNet**と呼ばれるものもあります。"
      ],
      "metadata": {
        "id": "QcfHOM7eJjoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 遺伝子50個を使用し、Ridge回帰を使ってみる\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習、ここでRidgeを指定。alphaは正則化の強さ\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "lr50 = Ridge(alpha=3)\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "metadata": {
        "id": "sbaxtRdggCG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 遺伝子50個を使用し、Lasso回帰を使ってみる\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習、ここでLassoを指定。alphaは正則化の強さ\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "lr50 = Lasso(alpha=0.3)\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "metadata": {
        "id": "V1fgfK0BoizF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso回帰だと何をやってるのかイメージしやすいと思います。\n",
        "\n",
        "正則化を強めていくことで偏回帰係数が0となる説明変数が増えていくので、少ない説明変数で目的変数を説明しようとするモデルになっていきます。\n",
        "\n",
        "正則化なし$$ \\space y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + \\beta_{5} x_{5} + \\beta_{6} x_{6} + \\beta_{7} x_{7} + \\beta_{8} x_{8} + \\beta_{9} x_{9} + \\beta_{10} x_{10} + e $$\n",
        "\n",
        "正則化あり($\\alpha$=0.1)$$ \\space y = 0× x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + \\beta_{5} x_{5} + 0× x_{6} + \\beta_{7} x_{7} + \\beta_{8} x_{8} + 0× x_{9} + \\beta_{10} x_{10} + e $$\n",
        "$$ → \\space y = \\beta_{2} x_{2} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + \\beta_{5} x_{5} + \\beta_{7} x_{7} + \\beta_{8} x_{8} + \\beta_{10} x_{10} + e $$\n",
        "\n",
        "正則化あり($\\alpha$=1)$$ \\space y = 0× x_{1} + 0× x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + 0× x_{5} + 0× x_{6} + \\beta_{7} x_{7} + \\beta_{8} x_{8} + 0× x_{9} + 0× x_{10} + e $$\n",
        "$$ → \\space y =  \\beta_{3} x_{3} + \\beta_{4} x_{4} + \\beta_{7} x_{7} + \\beta_{8} x_{8} + e $$\n"
      ],
      "metadata": {
        "id": "uiYna5WchycO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実習2\n",
        "\n",
        "使う変数を自由に選択したり(50遺伝子全て使っても良いし、30遺伝子でスタートしても良い)、\n",
        "Lasso回帰やRidge回帰(ElasticNetも使い方を調べて使ってみても良いです)を使用したり正則化の強さを調整したりして、時間内で作ることが出来る最高のモデル(テストデータの$R^2$が最高のもの)を作ってみてください。"
      ],
      "metadata": {
        "id": "sV4ajNUngQo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = \n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# モデル選択＆学習"
      ],
      "metadata": {
        "id": "ytJr2RI-gK0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-YywDR48RdG"
      },
      "source": [
        "---\n",
        "\n",
        "## まとめ\n",
        "\n",
        "　使用する説明変数の数を増やして、予測モデルの改善をおこないました。「機械学習 - 線形回帰（1）-」のテキストよりは良いスコア（決定係数）が得られたと思います。興味があれば、さらに良い予測モデルの構築をおこなってみてください。\n",
        "\n",
        "　次回は、目的関数（コスト関数）の最小値を探すアルゴリズム **勾配法 Gradient method** を学びます。勾配法は、機械学習では非常に重要なアルゴリズムです。高度な機械学習モデル、ニューラルネットワーク（Neural network）などにも使われています。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AMFfdtV_AeDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}