{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L09_ML_regression_2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "HhiREQJGpwnr",
        "-bAnZL1-Cb09",
        "x-jpAVs1M7jP",
        "-MiVC5B6IE0L"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9fGSNj8QRzB"
      },
      "source": [
        "<img src=\"https://lh3.googleusercontent.com/pw/AM-JKLVhTn_UySwMdfMwXvoq8l3VN7IkrY9cwtH2YJVMxAlMznUBWC9IpFtgPRIyfAXru4oykkYD-1WjWi0Ao5XgkB9JICvzDBcfn0L_5X2_KOOppsURK5DfSifCC-s7Vx5oQrBUn_BNWn_hfAPdhlVbKQGE=w1097-h235-no?authuser=0\" alt=\"2021年度ゲノム情報解析入門\" height=\"100px\" align=\"middle\">\n",
        "\n",
        "<div align=\"right\"><a href=\"https://github.com/CropEvol/lecture#section2\">実習表ページに戻る</a></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtPzXrSAtBm8"
      },
      "source": [
        "# 機械学習 - 線形回帰（2）-\n",
        "\n",
        "　[機械学習 - 線形回帰（1）](https://colab.research.google.com/github/CropEvol/lecture/blob/master/textbook_2021/L12_ML_regression_1.ipynb)では、説明変数（1遺伝子の発現量）をひとつ使い、その遺伝子発現量から表現型値を予測する線形回帰モデルを構築しました。しかし、そのモデルの評価値（決定係数 $R^2$ ）は低く、良いモデルではありませんでした。\n",
        "\n",
        "　今回、使用する説明変数を増やして、より良い線形回帰モデルを作ってみましょう。\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/regression_models.png?raw=true\" alt=\"regression_models\" height=\"400px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUijXJo0q5in"
      },
      "source": [
        "---\n",
        "\n",
        "## 今回の実習内容\n",
        "\n",
        "1. 線形回帰 Linear regression（2変数以上）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhiREQJGpwnr"
      },
      "source": [
        "### 線形回帰モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czigWGYAp1uO"
      },
      "source": [
        "　ここでおこなう解析の目標は次のとおりです。\n",
        "> 遺伝子発現量から表現型値を予測する線形回帰モデルを作る\n",
        "\n",
        "　「線形回帰モデル」とは何か？　遺伝子発現量（$x$）と表現型値（$y$）の関係を次の方程式で表したモデルのことです。\n",
        "\n",
        "線形回帰モデルの方程式: \n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/2018/textbook_2018/09_statistics/data/regression_base.png?raw=true\" alt=\"regression\" height=\"130px\">\n",
        "\n",
        "- **目的変数（objective variable）**: 予測される変数$y$。今回の場合、表現型値。\n",
        "- **説明変数（explanatory variable）**: 予測に使う変数$x$。今回の場合、各遺伝子発現量。\n",
        "- **偏回帰係数（coefficient）**: 各説明変数の重み。目的変数の予測にその変数がどれぐらい影響するかを示す指標。\n",
        "- **誤差（intercept; 切片）**: 説明変数以外の影響を示す項。\n",
        "\n",
        "　偏回帰係数や誤差の値は、データから推定します。「線形回帰モデルを作る」とは、それらの推定値を得ることに相当します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOaYdFcfbFMX"
      },
      "source": [
        "### 実習で使用するデータセット\n",
        "\n",
        "　次のコードセルを実行して、データファイル（[gene_expression.csv](https://github.com/CropEvol/lecture/blob/master/textbook_2019/dataset/gene_expression.csv)）をダウンロードしてください。\n",
        "\n",
        "ファイルの詳細:\n",
        "- ファイル名: gene_expression.csv\n",
        "- カンマ区切りテキストファイル\n",
        "- 100行（100サンプル） x 51列（表現型値 + 50個の遺伝子発現量）\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKU2jUPHbXDV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "cd08c344-72c5-4de3-ca8f-574327706a09"
      },
      "source": [
        "### このコードセルは実行のみ ###\n",
        "# サンプルデータのダウンロード\n",
        "!wget -q -O gene_expression.csv https://raw.githubusercontent.com/CropEvol/lecture/master/textbook_2019/dataset/gene_expression.csv\n",
        "\n",
        "# pandasで読み込み\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"gene_expression.csv\", sep=\",\", header=0)\n",
        "df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dcf02817-f492-4db9-8e6b-51de28ab9b03\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>phenotype</th>\n",
              "      <th>gene_1</th>\n",
              "      <th>gene_2</th>\n",
              "      <th>gene_3</th>\n",
              "      <th>gene_4</th>\n",
              "      <th>gene_5</th>\n",
              "      <th>gene_6</th>\n",
              "      <th>gene_7</th>\n",
              "      <th>gene_8</th>\n",
              "      <th>gene_9</th>\n",
              "      <th>gene_10</th>\n",
              "      <th>gene_11</th>\n",
              "      <th>gene_12</th>\n",
              "      <th>gene_13</th>\n",
              "      <th>gene_14</th>\n",
              "      <th>gene_15</th>\n",
              "      <th>gene_16</th>\n",
              "      <th>gene_17</th>\n",
              "      <th>gene_18</th>\n",
              "      <th>gene_19</th>\n",
              "      <th>gene_20</th>\n",
              "      <th>gene_21</th>\n",
              "      <th>gene_22</th>\n",
              "      <th>gene_23</th>\n",
              "      <th>gene_24</th>\n",
              "      <th>gene_25</th>\n",
              "      <th>gene_26</th>\n",
              "      <th>gene_27</th>\n",
              "      <th>gene_28</th>\n",
              "      <th>gene_29</th>\n",
              "      <th>gene_30</th>\n",
              "      <th>gene_31</th>\n",
              "      <th>gene_32</th>\n",
              "      <th>gene_33</th>\n",
              "      <th>gene_34</th>\n",
              "      <th>gene_35</th>\n",
              "      <th>gene_36</th>\n",
              "      <th>gene_37</th>\n",
              "      <th>gene_38</th>\n",
              "      <th>gene_39</th>\n",
              "      <th>gene_40</th>\n",
              "      <th>gene_41</th>\n",
              "      <th>gene_42</th>\n",
              "      <th>gene_43</th>\n",
              "      <th>gene_44</th>\n",
              "      <th>gene_45</th>\n",
              "      <th>gene_46</th>\n",
              "      <th>gene_47</th>\n",
              "      <th>gene_48</th>\n",
              "      <th>gene_49</th>\n",
              "      <th>gene_50</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>106</td>\n",
              "      <td>13.363546</td>\n",
              "      <td>9.278575</td>\n",
              "      <td>5.358185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.765873</td>\n",
              "      <td>8.267274</td>\n",
              "      <td>8.848567</td>\n",
              "      <td>8.651335</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.002230</td>\n",
              "      <td>8.366921</td>\n",
              "      <td>8.514351</td>\n",
              "      <td>10.887784</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.774787</td>\n",
              "      <td>5.190678</td>\n",
              "      <td>3.764463</td>\n",
              "      <td>12.259178</td>\n",
              "      <td>11.218678</td>\n",
              "      <td>10.832407</td>\n",
              "      <td>2.571919</td>\n",
              "      <td>8.963477</td>\n",
              "      <td>10.996029</td>\n",
              "      <td>10.287828</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.988457</td>\n",
              "      <td>3.559663</td>\n",
              "      <td>10.178441</td>\n",
              "      <td>11.031060</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.856906</td>\n",
              "      <td>9.876285</td>\n",
              "      <td>12.381172</td>\n",
              "      <td>1.024462</td>\n",
              "      <td>1.484705</td>\n",
              "      <td>3.811430</td>\n",
              "      <td>7.298979</td>\n",
              "      <td>6.028980</td>\n",
              "      <td>2.052242</td>\n",
              "      <td>7.156204</td>\n",
              "      <td>9.181366</td>\n",
              "      <td>6.790668</td>\n",
              "      <td>8.519664</td>\n",
              "      <td>8.670568</td>\n",
              "      <td>5.904554</td>\n",
              "      <td>9.069909</td>\n",
              "      <td>0.925468</td>\n",
              "      <td>13.735503</td>\n",
              "      <td>8.783810</td>\n",
              "      <td>9.430137</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>109</td>\n",
              "      <td>13.046248</td>\n",
              "      <td>11.941187</td>\n",
              "      <td>6.384982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.924542</td>\n",
              "      <td>8.799204</td>\n",
              "      <td>9.670269</td>\n",
              "      <td>8.171432</td>\n",
              "      <td>1.096936</td>\n",
              "      <td>6.360194</td>\n",
              "      <td>9.221483</td>\n",
              "      <td>8.613462</td>\n",
              "      <td>9.515351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.157463</td>\n",
              "      <td>6.023834</td>\n",
              "      <td>2.546092</td>\n",
              "      <td>12.522574</td>\n",
              "      <td>10.916798</td>\n",
              "      <td>11.271603</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.275510</td>\n",
              "      <td>10.878472</td>\n",
              "      <td>10.277322</td>\n",
              "      <td>0.361432</td>\n",
              "      <td>10.539614</td>\n",
              "      <td>4.106859</td>\n",
              "      <td>9.382164</td>\n",
              "      <td>13.289126</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.387618</td>\n",
              "      <td>9.704644</td>\n",
              "      <td>11.847981</td>\n",
              "      <td>1.147828</td>\n",
              "      <td>1.712772</td>\n",
              "      <td>3.165542</td>\n",
              "      <td>7.152082</td>\n",
              "      <td>3.956205</td>\n",
              "      <td>1.712772</td>\n",
              "      <td>8.527016</td>\n",
              "      <td>9.119561</td>\n",
              "      <td>7.666501</td>\n",
              "      <td>9.198592</td>\n",
              "      <td>8.630143</td>\n",
              "      <td>4.655506</td>\n",
              "      <td>7.797830</td>\n",
              "      <td>0.650305</td>\n",
              "      <td>13.732358</td>\n",
              "      <td>9.361080</td>\n",
              "      <td>8.794585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "      <td>13.097211</td>\n",
              "      <td>9.040640</td>\n",
              "      <td>4.770866</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.704101</td>\n",
              "      <td>7.710614</td>\n",
              "      <td>8.763461</td>\n",
              "      <td>8.279875</td>\n",
              "      <td>0.471760</td>\n",
              "      <td>8.292386</td>\n",
              "      <td>8.464962</td>\n",
              "      <td>8.098364</td>\n",
              "      <td>9.326176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.089808</td>\n",
              "      <td>2.393553</td>\n",
              "      <td>3.362133</td>\n",
              "      <td>12.664074</td>\n",
              "      <td>11.155229</td>\n",
              "      <td>11.293477</td>\n",
              "      <td>2.591703</td>\n",
              "      <td>9.319205</td>\n",
              "      <td>11.125568</td>\n",
              "      <td>10.224629</td>\n",
              "      <td>0.826681</td>\n",
              "      <td>9.593610</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.201876</td>\n",
              "      <td>12.856612</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.822771</td>\n",
              "      <td>9.918771</td>\n",
              "      <td>11.291251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.610759</td>\n",
              "      <td>9.226304</td>\n",
              "      <td>4.888568</td>\n",
              "      <td>1.731487</td>\n",
              "      <td>8.571476</td>\n",
              "      <td>8.162089</td>\n",
              "      <td>7.402986</td>\n",
              "      <td>9.271246</td>\n",
              "      <td>9.998943</td>\n",
              "      <td>6.334062</td>\n",
              "      <td>6.989695</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.804555</td>\n",
              "      <td>9.067929</td>\n",
              "      <td>7.143628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>93</td>\n",
              "      <td>15.187503</td>\n",
              "      <td>8.813056</td>\n",
              "      <td>5.511784</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.434128</td>\n",
              "      <td>9.329489</td>\n",
              "      <td>10.592504</td>\n",
              "      <td>7.566518</td>\n",
              "      <td>4.776025</td>\n",
              "      <td>7.920996</td>\n",
              "      <td>8.632912</td>\n",
              "      <td>8.309035</td>\n",
              "      <td>10.498610</td>\n",
              "      <td>0.703544</td>\n",
              "      <td>10.775907</td>\n",
              "      <td>8.046338</td>\n",
              "      <td>2.984389</td>\n",
              "      <td>11.851566</td>\n",
              "      <td>9.509466</td>\n",
              "      <td>10.458437</td>\n",
              "      <td>0.703544</td>\n",
              "      <td>8.616836</td>\n",
              "      <td>10.440931</td>\n",
              "      <td>9.815484</td>\n",
              "      <td>1.174470</td>\n",
              "      <td>9.902380</td>\n",
              "      <td>5.749454</td>\n",
              "      <td>8.294671</td>\n",
              "      <td>9.954150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.984389</td>\n",
              "      <td>9.618461</td>\n",
              "      <td>11.097670</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.217153</td>\n",
              "      <td>9.311049</td>\n",
              "      <td>6.574106</td>\n",
              "      <td>4.523480</td>\n",
              "      <td>6.834610</td>\n",
              "      <td>10.464229</td>\n",
              "      <td>8.637473</td>\n",
              "      <td>8.434903</td>\n",
              "      <td>8.889617</td>\n",
              "      <td>2.984389</td>\n",
              "      <td>7.276041</td>\n",
              "      <td>3.950123</td>\n",
              "      <td>14.342589</td>\n",
              "      <td>9.908996</td>\n",
              "      <td>12.049794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84</td>\n",
              "      <td>14.076757</td>\n",
              "      <td>7.670217</td>\n",
              "      <td>4.338645</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.950235</td>\n",
              "      <td>9.461948</td>\n",
              "      <td>9.954554</td>\n",
              "      <td>9.512503</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.813361</td>\n",
              "      <td>7.406503</td>\n",
              "      <td>6.645653</td>\n",
              "      <td>10.005274</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.659796</td>\n",
              "      <td>5.721960</td>\n",
              "      <td>2.683629</td>\n",
              "      <td>13.191783</td>\n",
              "      <td>9.930696</td>\n",
              "      <td>10.792310</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.557602</td>\n",
              "      <td>10.465964</td>\n",
              "      <td>10.450283</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.246337</td>\n",
              "      <td>4.504550</td>\n",
              "      <td>6.774721</td>\n",
              "      <td>9.236043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.069943</td>\n",
              "      <td>11.052935</td>\n",
              "      <td>11.918971</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.990084</td>\n",
              "      <td>3.374344</td>\n",
              "      <td>9.280504</td>\n",
              "      <td>3.625574</td>\n",
              "      <td>2.683629</td>\n",
              "      <td>10.251021</td>\n",
              "      <td>9.960994</td>\n",
              "      <td>6.819502</td>\n",
              "      <td>9.416861</td>\n",
              "      <td>9.173680</td>\n",
              "      <td>3.152265</td>\n",
              "      <td>4.762205</td>\n",
              "      <td>2.154486</td>\n",
              "      <td>13.137153</td>\n",
              "      <td>9.657865</td>\n",
              "      <td>7.594929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>97</td>\n",
              "      <td>13.346763</td>\n",
              "      <td>8.601243</td>\n",
              "      <td>5.679286</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.508676</td>\n",
              "      <td>8.637904</td>\n",
              "      <td>8.885413</td>\n",
              "      <td>6.878897</td>\n",
              "      <td>1.056930</td>\n",
              "      <td>7.110854</td>\n",
              "      <td>8.058165</td>\n",
              "      <td>8.405975</td>\n",
              "      <td>9.197526</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.545727</td>\n",
              "      <td>6.821481</td>\n",
              "      <td>2.173639</td>\n",
              "      <td>11.483569</td>\n",
              "      <td>11.405126</td>\n",
              "      <td>10.793969</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.072205</td>\n",
              "      <td>10.140536</td>\n",
              "      <td>9.828387</td>\n",
              "      <td>1.389952</td>\n",
              "      <td>9.103007</td>\n",
              "      <td>6.540901</td>\n",
              "      <td>2.678635</td>\n",
              "      <td>10.399257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.532953</td>\n",
              "      <td>9.385317</td>\n",
              "      <td>11.888507</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.344942</td>\n",
              "      <td>5.527033</td>\n",
              "      <td>6.824921</td>\n",
              "      <td>6.381766</td>\n",
              "      <td>3.098201</td>\n",
              "      <td>9.582710</td>\n",
              "      <td>7.940948</td>\n",
              "      <td>7.895842</td>\n",
              "      <td>8.682219</td>\n",
              "      <td>9.885536</td>\n",
              "      <td>6.010762</td>\n",
              "      <td>10.732532</td>\n",
              "      <td>2.336826</td>\n",
              "      <td>14.542288</td>\n",
              "      <td>9.426110</td>\n",
              "      <td>11.623287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>103</td>\n",
              "      <td>12.535288</td>\n",
              "      <td>7.530406</td>\n",
              "      <td>6.515419</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.315795</td>\n",
              "      <td>9.186164</td>\n",
              "      <td>8.653254</td>\n",
              "      <td>8.906494</td>\n",
              "      <td>0.842939</td>\n",
              "      <td>7.766072</td>\n",
              "      <td>8.279884</td>\n",
              "      <td>7.785230</td>\n",
              "      <td>9.381895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.287368</td>\n",
              "      <td>5.649891</td>\n",
              "      <td>4.537526</td>\n",
              "      <td>13.552969</td>\n",
              "      <td>10.561689</td>\n",
              "      <td>11.174757</td>\n",
              "      <td>0.612683</td>\n",
              "      <td>9.534352</td>\n",
              "      <td>9.998999</td>\n",
              "      <td>9.682914</td>\n",
              "      <td>0.338681</td>\n",
              "      <td>8.530633</td>\n",
              "      <td>3.830702</td>\n",
              "      <td>9.530744</td>\n",
              "      <td>10.843317</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.466444</td>\n",
              "      <td>9.188782</td>\n",
              "      <td>12.262999</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.967242</td>\n",
              "      <td>2.824788</td>\n",
              "      <td>5.337019</td>\n",
              "      <td>2.653381</td>\n",
              "      <td>2.877666</td>\n",
              "      <td>10.363861</td>\n",
              "      <td>7.677487</td>\n",
              "      <td>5.579799</td>\n",
              "      <td>9.144281</td>\n",
              "      <td>9.286756</td>\n",
              "      <td>5.021640</td>\n",
              "      <td>9.429762</td>\n",
              "      <td>1.371503</td>\n",
              "      <td>14.691651</td>\n",
              "      <td>9.820975</td>\n",
              "      <td>5.619158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>102</td>\n",
              "      <td>14.522330</td>\n",
              "      <td>3.591536</td>\n",
              "      <td>4.703632</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.932421</td>\n",
              "      <td>8.076816</td>\n",
              "      <td>8.589984</td>\n",
              "      <td>10.027173</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.058912</td>\n",
              "      <td>8.648120</td>\n",
              "      <td>7.101640</td>\n",
              "      <td>10.820841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.518055</td>\n",
              "      <td>6.853023</td>\n",
              "      <td>0.452595</td>\n",
              "      <td>14.012721</td>\n",
              "      <td>7.825830</td>\n",
              "      <td>10.824362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.969487</td>\n",
              "      <td>11.826894</td>\n",
              "      <td>10.004586</td>\n",
              "      <td>0.796598</td>\n",
              "      <td>9.931333</td>\n",
              "      <td>2.932231</td>\n",
              "      <td>9.214093</td>\n",
              "      <td>9.849364</td>\n",
              "      <td>0.796598</td>\n",
              "      <td>2.785739</td>\n",
              "      <td>9.260771</td>\n",
              "      <td>11.216073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.981122</td>\n",
              "      <td>3.871282</td>\n",
              "      <td>8.092482</td>\n",
              "      <td>0.796598</td>\n",
              "      <td>3.006208</td>\n",
              "      <td>6.352266</td>\n",
              "      <td>8.712902</td>\n",
              "      <td>6.862191</td>\n",
              "      <td>8.837830</td>\n",
              "      <td>8.374635</td>\n",
              "      <td>3.976345</td>\n",
              "      <td>9.487277</td>\n",
              "      <td>0.796598</td>\n",
              "      <td>12.828307</td>\n",
              "      <td>9.107296</td>\n",
              "      <td>7.272975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>106</td>\n",
              "      <td>12.892788</td>\n",
              "      <td>8.228314</td>\n",
              "      <td>6.621552</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.877891</td>\n",
              "      <td>8.382339</td>\n",
              "      <td>8.783231</td>\n",
              "      <td>7.195239</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.841690</td>\n",
              "      <td>8.587556</td>\n",
              "      <td>8.629582</td>\n",
              "      <td>9.400442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.127943</td>\n",
              "      <td>5.120526</td>\n",
              "      <td>2.775683</td>\n",
              "      <td>12.300876</td>\n",
              "      <td>11.573851</td>\n",
              "      <td>10.990891</td>\n",
              "      <td>2.705558</td>\n",
              "      <td>8.921115</td>\n",
              "      <td>10.619120</td>\n",
              "      <td>9.614067</td>\n",
              "      <td>1.711142</td>\n",
              "      <td>9.132124</td>\n",
              "      <td>3.872356</td>\n",
              "      <td>7.823183</td>\n",
              "      <td>12.333832</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.381519</td>\n",
              "      <td>9.623011</td>\n",
              "      <td>11.904514</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.405883</td>\n",
              "      <td>4.286770</td>\n",
              "      <td>5.902180</td>\n",
              "      <td>4.855003</td>\n",
              "      <td>2.705558</td>\n",
              "      <td>9.103440</td>\n",
              "      <td>8.020202</td>\n",
              "      <td>7.182394</td>\n",
              "      <td>8.481396</td>\n",
              "      <td>8.909464</td>\n",
              "      <td>5.497756</td>\n",
              "      <td>9.621228</td>\n",
              "      <td>0.405883</td>\n",
              "      <td>14.026420</td>\n",
              "      <td>8.785358</td>\n",
              "      <td>9.154504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>80</td>\n",
              "      <td>16.037942</td>\n",
              "      <td>7.115106</td>\n",
              "      <td>8.542842</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.526372</td>\n",
              "      <td>9.280956</td>\n",
              "      <td>10.920561</td>\n",
              "      <td>7.069047</td>\n",
              "      <td>2.164883</td>\n",
              "      <td>7.115106</td>\n",
              "      <td>7.219982</td>\n",
              "      <td>7.690522</td>\n",
              "      <td>8.693898</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.254296</td>\n",
              "      <td>10.265580</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13.673177</td>\n",
              "      <td>9.445000</td>\n",
              "      <td>8.519062</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.894026</td>\n",
              "      <td>9.798266</td>\n",
              "      <td>7.596257</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.653547</td>\n",
              "      <td>4.851729</td>\n",
              "      <td>5.390282</td>\n",
              "      <td>9.781542</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.455334</td>\n",
              "      <td>10.698080</td>\n",
              "      <td>9.837254</td>\n",
              "      <td>1.422879</td>\n",
              "      <td>1.853277</td>\n",
              "      <td>3.982619</td>\n",
              "      <td>6.295100</td>\n",
              "      <td>5.477483</td>\n",
              "      <td>3.143997</td>\n",
              "      <td>10.704872</td>\n",
              "      <td>10.279796</td>\n",
              "      <td>8.352194</td>\n",
              "      <td>10.008078</td>\n",
              "      <td>8.767089</td>\n",
              "      <td>3.623399</td>\n",
              "      <td>8.640549</td>\n",
              "      <td>3.279590</td>\n",
              "      <td>14.463773</td>\n",
              "      <td>9.095837</td>\n",
              "      <td>8.720183</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 51 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcf02817-f492-4db9-8e6b-51de28ab9b03')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcf02817-f492-4db9-8e6b-51de28ab9b03 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcf02817-f492-4db9-8e6b-51de28ab9b03');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    phenotype     gene_1     gene_2  ...    gene_48   gene_49    gene_50\n",
              "0         106  13.363546   9.278575  ...  13.735503  8.783810   9.430137\n",
              "1         109  13.046248  11.941187  ...  13.732358  9.361080   8.794585\n",
              "2         119  13.097211   9.040640  ...  13.804555  9.067929   7.143628\n",
              "3          93  15.187503   8.813056  ...  14.342589  9.908996  12.049794\n",
              "4          84  14.076757   7.670217  ...  13.137153  9.657865   7.594929\n",
              "..        ...        ...        ...  ...        ...       ...        ...\n",
              "95         97  13.346763   8.601243  ...  14.542288  9.426110  11.623287\n",
              "96        103  12.535288   7.530406  ...  14.691651  9.820975   5.619158\n",
              "97        102  14.522330   3.591536  ...  12.828307  9.107296   7.272975\n",
              "98        106  12.892788   8.228314  ...  14.026420  8.785358   9.154504\n",
              "99         80  16.037942   7.115106  ...  14.463773  9.095837   8.720183\n",
              "\n",
              "[100 rows x 51 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (補足)乱数について\n",
        "乱数とは０から９までの数字が不規則かつ等確率に現れるように配列されたものです。\n",
        "学習の過程では様々な点で乱数が使用されています。\n",
        "例えば、トレーニングデータとテストデータにデータを7:3に分割する際に、乱数に基づいてデータをランダムに選択する時に利用されています。\n",
        "また、モデルの学習においてもパラメータの調整の変遷や初期値などに乱数が使用されていることも多いです。\n",
        "\n",
        "下記のコードを動かしてみると、毎回トレーニングデータとテストデータの分けられ方が異なることが分かります。乱数は文字通りランダムに生成されるので、コードを動かすたびに異なる値が生成されています。\n",
        "\n"
      ],
      "metadata": {
        "id": "hNJxjbgbQ6sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数の確認\n",
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "print(x_train, x_test)"
      ],
      "metadata": {
        "id": "5nvNLBeeSvkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この乱数は固定することが出来ます。\n",
        "乱数を利用している関数にはseedやrandom_seedという引数が用意されており、この値を決めておくことで同じ乱数のもと、関数を実行できます。\n",
        "つまり、同じテストデータとトレーニングデータの分け方を再現可能になります。"
      ],
      "metadata": {
        "id": "5ARZF6UjS6SU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 乱数を固定\n",
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "y = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "print(x_train, x_test)"
      ],
      "metadata": {
        "id": "-n2eM-07VRee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "特に、科学において結果の「再現性」というものは非常に重要です。再現性というのは、\n",
        "\n",
        "「XXという実験をYYという条件の下で行うと、ZZという結果が得られた」という実験結果が、何度繰り返しても、異なる実験者が行っても再現できるという事を示します。\n",
        "\n",
        "一昔前にSTAP細胞に関する事件が起きましたが、あの件は他の研究者によってSTAP細胞の再現ができなかったため、虚偽ではないかとなったわけですね。\n",
        "\n",
        "これは実験での話だけでなく、今皆さんがやっている計算やモデルの学習においても同じです。科学でこれらの知識を利用するなら同じように「再現性」を取る必要があります。\n",
        "そのため、通常は乱数を固定してデータの分け方や学習を行い、使用したライブラリのバージョンや乱数を決めるrandom_seedなどの値はしっかり記録しておいて、どの様な乱数のもとデータを生成し学習していったのかを確実に再現できる様にしておく必要があります。"
      ],
      "metadata": {
        "id": "_wMFI4DyVTs4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEg9g2e0UYch"
      },
      "source": [
        "## 2. 線形回帰 Linear regression（2変数以上）\n",
        "\n",
        "　使用する説明変数を3つに増やして、少し良いモデルを作ってみましょう。\n",
        "\n",
        "使用する説明変数\n",
        "- gene_7の遺伝子発現量 `gene_7`\n",
        "- gene_11の遺伝子発現量 `gene_11`\n",
        "- gene_28の遺伝子発現量 `gene_28`\n",
        "\n",
        "> 3個の遺伝子発現量から表現型値（phenotype）を予測する線形回帰モデルを作る\n",
        "  $$ y = \\beta_{gene\\_7} x_{gene\\_7} + \\beta_{gene\\_11} x_{gene\\_11} + \\beta_{gene\\_28} x_{gene\\_28} + e $$\n",
        "\n",
        "　説明変数の個数が2つ以上になっても、基本的な手順は1変数のときと同じです。異なる点は、前処理に**スケーリング（scaling）**と呼ばれる操作が入る点です。\n",
        "\n",
        "- 2-1. 前処理\n",
        "  - **スケーリング（scaling）**\n",
        "- 2-2. モデルの選択\n",
        "- 2-3. モデルの学習\n",
        "- 2-4. モデルの評価\n",
        "- 2-5. 予測\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2019/images/supervised_learning_process.png?raw=true\" alt=\"supervised_learning_process\" height=\"60px\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwH2Nub0sN5O"
      },
      "source": [
        "### 2-1. 前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-BQtibXe6NZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85bf628b-e8f5-4880-8520-79c47fb5e0d0"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 使用する変数\n",
        "x = np.array(df.loc[:,[\"gene_7\", \"gene_11\", \"gene_28\"]]) # 説明変数3つ\n",
        "y = np.array(df[\"phenotype\"])                             # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "print(\"training: \", x_train.shape, y_train.shape)\n",
        "print(\"test: \", x_test.shape, y_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training:  (70, 3) (70,)\n",
            "test:  (30, 3) (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuhKmuuSiKvv"
      },
      "source": [
        "\n",
        "スケーリングにはおもに2つの方法があります。 \n",
        "- **正規化 normalization**: 各変数の値を0 ~ 1の範囲に変換する  \n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "mms = MinMaxScaler()  # 変数名「mms」の部分は任意\n",
        "x_train_mms = mms.fit_transform(x_train)\n",
        "x_test_mms = mms.transform(x_test)\n",
        "```\n",
        "\n",
        "- **標準化 standardization**: 各変数の値を平均値0、標準偏差1になるように変換する\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler() # 変数名「ss」の部分は任意\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "```\n",
        "\n",
        "　ここでは、標準化をおこないます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLPw_MejxlKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44d063dc-f891-4e2a-a8d2-688f061c884f"
      },
      "source": [
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# 確認\n",
        "#x_train_ss\n",
        "x_test_ss"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.29887699e+00, -8.86730001e-02, -8.70119718e-01],\n",
              "       [ 4.39679649e-01, -1.40405516e+00, -1.09033397e+00],\n",
              "       [ 5.12328557e-01,  5.62607558e-01,  4.89795869e-01],\n",
              "       [-1.76884435e+00,  1.32114978e+00,  6.19008722e-01],\n",
              "       [ 2.13760463e+00,  3.39451742e-01,  5.75238772e-01],\n",
              "       [ 2.33729997e-01,  1.19488755e+00, -3.57043157e-01],\n",
              "       [ 2.76558985e-01,  5.46097291e-01,  7.96540892e-01],\n",
              "       [-1.07500769e+00, -2.45118456e+00,  7.20651255e-01],\n",
              "       [ 8.76032679e-02, -1.20115700e+00, -2.11830549e-01],\n",
              "       [-9.99287847e-01,  3.25769388e-01, -1.46341967e+00],\n",
              "       [ 1.77936287e-01, -2.49921586e+00, -6.32221461e-01],\n",
              "       [-2.01107812e+00,  5.92300118e-01, -2.65507026e-01],\n",
              "       [-9.69780429e-01,  1.57888451e+00,  4.03509370e-01],\n",
              "       [ 9.02889157e-01, -7.89238830e-01, -1.74193276e+00],\n",
              "       [ 2.68579749e+00, -1.48562995e+00, -3.48331677e-01],\n",
              "       [ 1.12540143e+00,  7.97264212e-01,  9.55279272e-01],\n",
              "       [ 1.07157363e+00, -1.24470073e+00,  9.19078227e-02],\n",
              "       [ 1.24030129e+00,  6.47559090e-01,  9.31388114e-01],\n",
              "       [ 2.05221386e-02, -1.33758374e+00,  3.67040276e-01],\n",
              "       [ 5.96524978e-01,  1.09970968e+00,  9.21051890e-01],\n",
              "       [-7.76560831e-01, -4.12830299e-03,  1.17426113e+00],\n",
              "       [-4.71510153e-01, -5.46857737e-01,  9.07434383e-01],\n",
              "       [ 3.18915923e-01,  1.47688053e+00,  1.22470614e-01],\n",
              "       [ 6.22864865e-01,  1.94736643e+00,  5.14368808e-01],\n",
              "       [ 2.24536368e-01,  8.16620159e-01,  4.83169088e-01],\n",
              "       [-3.49950797e-01, -4.82224511e+00, -1.64905984e+00],\n",
              "       [-2.87690222e-01,  4.25167547e-01,  6.96125255e-01],\n",
              "       [ 1.88255496e-01,  2.10922417e-01, -1.23262055e-01],\n",
              "       [-1.78872914e+00, -5.24193335e-01,  3.90777628e-01],\n",
              "       [-1.18370062e+00,  5.57235926e-01,  4.70466508e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### スケーリングの目的\n",
        "\n",
        "今後講義で紹介していく手法である、決定木などの手法では影響を受けませんが、ロジスティック回帰や主成分分析など、様々な手法で入力データのスケールは悪影響を与える場合があります。特に、このノートで後ほど紹介するLassoやRidge回帰と呼ばれる手法では悪影響が大きいです。\n",
        "また、複数の説明変数（身長や体重など）のスケールを揃えずにモデルの学習をおこなうと、、得られた学習結果を解釈する（影響力の大きい説明変数がどれかを判断する）ことも難しくなります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/scaling.png?raw=true\" alt=\"scaling\" height=\"400px\">\n"
      ],
      "metadata": {
        "id": "e35WWmXKX6uy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BewlyUXtviu-"
      },
      "source": [
        "### 2-2 ~ 2-3. モデルの選択・学習\n",
        "\n",
        "　線形回帰モデルを用意し、スケーリング後のトレーニングデータでモデルの学習をおこないます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ICaAnSBi6N9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da54dfc0-d4e4-4795-c5b9-5abcb1a74789"
      },
      "source": [
        "# モデル選択＆学習\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr = LinearRegression()\n",
        "lr.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr.coef_\n",
        "e = lr.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "print(\"Intercept=\"  , e)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient= [-2.56637755  3.53324926  1.03224762]\n",
            "Intercept= 97.15714285714283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAnZL1-Cb09"
      },
      "source": [
        "### 各項の係数 Coefficient について"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rR-WxRnCrQ8"
      },
      "source": [
        "　$$ 線形回帰モデル: f(x) = \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_3 + ... + \\beta_mx_m+ e $$\n",
        "\n",
        "　各項の係数$\\beta$（Coefficient）は、それぞれの説明変数$x_1, x_2...x_m$が目的変数$y$にどの程度影響を持っているかを表しています。影響の大きさを知りたいのであれば、係数の絶対値をみます。影響の方向性（プラスかマイナスか）を知りたいのであれば、係数の符号をみます。\n",
        "\n",
        "\n",
        "　先程説明した通り、前処理でおこなったスケーリングはこの係数に影響してきます。モデルの各係数の値を調べたい場合（例えば、各遺伝子の発現が表現型値にどの程度関わっているか調べたい場合）、スケーリングをせずに解析すると、誤った結果を得ることになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns7-YTWL8A7Z"
      },
      "source": [
        "### 2-4. モデルの評価\n",
        "\n",
        "　モデルの評価でも、スケーリング後のトレーニングデータ・テストデータを使います。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKoQUHTq4QZc"
      },
      "source": [
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2FGjVcn8D4f"
      },
      "source": [
        "### 2-5. 予測\n",
        "\n",
        "　まだ改善の余地はありそうですが、1変数のときよりも良いモデルが得られました。新しい入力データをこの予測モデルに入れて、その予測値を調べてみましょう。\n",
        "\n",
        "　ここでは、新しく3サンプルの遺伝子発現データが得られたとして、その表現型値を予測します。\n",
        "\n",
        "$$\n",
        "(gene_{7}, gene_{11}, gene_{28}) = (x_1, x_2, x_3) = (10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)\n",
        "$$\n",
        "\n",
        "予測の手順:  \n",
        "1. 新しいデータのスケーリングをします。トレーニングデータと同じ基準で、スケーリングする必要があります（テストデータのスケーリングと同じ操作です）。\n",
        "```python\n",
        "スケーリング名.transform(新しいデータ)\n",
        "```\n",
        "\n",
        "1. `predict`で新しいデータの予測値を得ます。\n",
        "```python\n",
        "モデル名.predict(スケーリング後のデータ)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4fPHC7p8Gqn"
      },
      "source": [
        "# 新しいデータ\n",
        "x_new = np.array([(10.0, 9.0, 12.0), (8.0, 10.5, 13.0), (9.1, 12.3, 8.9)])\n",
        "\n",
        "# スケーリング（標準化 standardization）\n",
        "x_new_ss = ss.transform(x_new)\n",
        "\n",
        "# 予測\n",
        "lr.predict(x_new_ss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実習1\n",
        "\n",
        "今回の講義では遺伝子7,11, 28の発現量のデータを使って線形モデルを作成してみました。\n",
        "\n",
        "皆さんは自分の好きな遺伝子のセット(3つ以上でもOK)を使って線形モデルを作成し、\n",
        "1. 前処理(スケーリング)\n",
        "2. モデルの選択\n",
        "3. モデルの学習\n",
        "4. モデルの評価\n",
        "までを行ってみて下さい。\n",
        "\n",
        "そして、4.のモデルの評価の結果をもとに色んな遺伝子セットを試してみて、どの遺伝子の組み合わせから形質データを説明するのが一番良さそうか試してみて下さい。\n",
        "\n",
        "```python\n",
        "# 前処理\n",
        "# 使用する変数\n",
        "x = \n",
        "y = \n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "x_train, x_test, y_train, y_test = \n",
        "print(\"training: \", x_train.shape, y_train.shape)\n",
        "print(\"test: \", x_test.shape, y_test.shape)\n",
        "\n",
        "# スケーリング\n",
        "ss = \n",
        "x_train_ss = \n",
        "x_test_ss =\n",
        "\n",
        "# 線形回帰モデルの準備\n",
        "モデル変数 =\n",
        "\n",
        "# モデルの学習\n",
        "モデル変数\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = \n",
        "r2_test_ss =\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)\n",
        "```\n"
      ],
      "metadata": {
        "id": "2uKhwnYomPqW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MiVC5B6IE0L"
      },
      "source": [
        "## 遺伝子50個を使った予測モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_UEcMeKITSm"
      },
      "source": [
        "　さて、それでは次に50個の遺伝子すべてのデータを使って予測モデルを作ってみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o15K_qRBHEdv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea37721-fe70-4bd7-db37-8900425fe9e2"
      },
      "source": [
        "# 遺伝子50個を使った線形回帰モデル\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "# random_state ... 乱数を固定することが出来る\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lr50 = LinearRegression()\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "# print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training:  0.9679683095510192\n",
            "test:  -0.0912175240915416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "トレーニングデータの決定係数は高くなりましたが、テストデータの決定係数は非常に低い値になります（**過学習(overfitting)**と言われる状態です）。良いモデルを作るためには、使用する遺伝子を選ぶ必要があります。"
      ],
      "metadata": {
        "id": "ZzUWxPSHoS1V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-jpAVs1M7jP"
      },
      "source": [
        "### モデル比較"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-opZ32NM8aw"
      },
      "source": [
        "　ここでは、モデル間の比較で重要な考え方を紹介します。\n",
        "\n",
        "```\n",
        "精度が高いモデルが良い\n",
        "複雑なモデルより、単純なモデルが良い\n",
        "```\n",
        "\n",
        "　機械学習では、新しいデータに対して上手く予測できるモデル、すなわち「精度が高い」モデルが好まれます（実習で使った決定係数$R^2$はモデルの精度を測る指標の一つです）。\n",
        "\n",
        "　一般的に、モデルの精度を高くしようとすると、複雑になる傾向にあります。特に、先程の50個の遺伝子を使った例の様に、回帰モデルでは変数を増やせば増やすほど残差平方和をいくらでも小さくすることが出来ます。しかし、無暗にパラメータ数を増やし、誤差なども無理やり説明しようとすると、他のデータに対して合わなくなる(Overfitting)問題が生じます。\n",
        "そこで，単に残差平方和の大小($R^2$)を比較するだけでなくパラメータ数(今回だと説明変数の数)も考慮した，評価基準が必要となります。\n",
        "\n",
        "<img src=\"https://github.com/CropEvol/lecture/blob/master/textbook_2021/images/overfit.png?raw=true\" alt=\"scaling\" height=\"400px\">\n",
        "\n",
        "　精度とパラーメータ数の両方を考慮してモデルを評価する指標として、次のような指標があります。これらは、モデル間の比較に使われます。詳しく知りたい方は、統計モデリングの本を参照してください。\n",
        "- AIC(赤池情報量基準): 予測能力が最良のモデルを良いとする指標\n",
        "- BIC(ベイズ情報量規準): 真のモデルである確率が最も大きいモデルを良いとする指標\n",
        "\n",
        "また、モデルの複雑さで言うと、変数が少ない「単純なモデル」は、どの変数が強い影響を持っているのか等を理解しやすいという利点がある場合があり、精度だけを求めた複雑なモデルよりも好まれる場合が多々あります。\n",
        "\n",
        "「統計モデリング」の勉強におすすめの本: \n",
        "- [データ解析のための統計モデリング入門](https://www.iwanami.co.jp/book/b257893.html)\n",
        "- [Bayesian Analysis with Python](https://www.packtpub.com/big-data-and-business-intelligence/bayesian-analysis-python-second-edition)（原書）/ [Pythonによるベイズ統計モデリング](https://www.kyoritsu-pub.co.jp/bookdetail/9784320113374)（日本語訳）"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfittingを回避する方法の一例\n",
        "\n",
        "遺伝子50個を使ったモデルでは、学習データに対してはよくあてはまっているモデルを作成することが出来ましたが、過学習(Overfitting)してしまい、学習データ以外のデータに対して当てはまりが良くない結果になりました。\n",
        "\n",
        "今回の様に説明変数が50個ではなくより大きなデータセットだと更に大変になります。特に、手に入るサンプル数より多くの説明変数を考慮した線形回帰を行うと、ほぼ確実に過学習が生じます。\n"
      ],
      "metadata": {
        "id": "1EWvSh9Wojtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 説明変数の数に応じた予測式の変化例\n",
        "\n",
        "import numpy as np\n",
        "import random as rnd\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "def poly(intercept, coef, x):\n",
        "    return intercept + sum([w * x**(n + 1) for n, w in enumerate(coef)])\n",
        " \n",
        "xmin, xmax = -3, 1\n",
        "xlim_min, xlim_max = -4, 2\n",
        "ylim_min, ylim_max = -2, 4\n",
        " \n",
        " # データ数・説明変数の数・グラフとして表示する説明変数の設定\n",
        "n_data = 10\n",
        "n_features = 20\n",
        "n_terms_list = [2, 4, 6, 8, 10, 12]\n",
        " \n",
        "x = np.linspace(xmin, xmax, n_data)\n",
        "y = np.exp(x) + [rnd.uniform(-0.5, 0.5) for n in range(n_data)]\n",
        " \n",
        "df = pd.DataFrame(y, columns=['y'])\n",
        "for n in range(n_features):\n",
        "    df[\"x^{}\".format(n+1)] = x**(n+1)\n",
        "print(df.shape)\n",
        " \n",
        "fig, axs = plt.subplots(2, 3, figsize=(12, 6.4))\n",
        "axs_1d = axs.reshape(1, -1)[0]\n",
        " \n",
        "linreg = LinearRegression()\n",
        " \n",
        "x_graph = np.linspace(xlim_min, xlim_max)\n",
        " \n",
        "for ax, n_terms in zip(axs_1d, n_terms_list):\n",
        "    linreg.fit(df.iloc[:, 1:n_terms+1], df['y'])\n",
        "    y_linreg = poly(linreg.intercept_, linreg.coef_, x_graph)\n",
        "    ax.scatter(df['x^1'], df['y'], c='r', zorder=10)\n",
        "    ax.plot(x_graph, y_linreg, c='gray', linewidth=2,\n",
        "        label=\"n_terms={}\".format(n_terms))\n",
        " \n",
        "    ax.set_xlim(xlim_min, xlim_max)\n",
        "    ax.set_ylim(ylim_min, ylim_max)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.legend(loc='upper left')\n",
        " \n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "2wC10dyVRArT",
        "outputId": "b96c49bf-11db-43c0-dd2a-6cb11ff012e3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 21)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x460.8 with 6 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "そのため、このような多くの変数から回帰モデルを作成する場合にはOverfittingを避けるような工夫をする必要があります。"
      ],
      "metadata": {
        "id": "fj3PKYPcSwLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 情報量基準等を使用して変数選択をする。\n",
        "\n",
        "全ての説明変数を使うのではなく、適切な変数の組み合わせを探してモデルを作る形です。さきほど皆さんに実習でやってもらった方法ですね。\n",
        "\n",
        "AICが良くなるモデルだったり、$R^2$を使う場合は学習データだけでなく、テストデータの評価も良くなるようなモデルの作成を目指します。\n",
        "\n",
        "モデルを作る際に、本当に目的変数(y)に影響を与えている重要な説明変数のみを上手く選択できれば良いモデルを作ることが出来そうです。"
      ],
      "metadata": {
        "id": "cqyOVGU5Jb0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 例えば、3つの遺伝子の組み合わせ全て試して、テストデータを用いたR^2が良いものを探す…とか。\n",
        "combinations\n"
      ],
      "metadata": {
        "id": "eX-cY4_CJkV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "とはいえ、これでは良いモデルを作るためには50個の遺伝子の中から、膨大な遺伝子の組み合わせをちまちま探す必要が出てきてしまいます。そこで出て来るのが正則化と呼ばれる手法になります。\n",
        "\n",
        "このアプローチは説明変数の選択を別の基準で行える場合に重要です。(遺伝子11は生物学的にyの値に影響を与えていることが知られている、など。)"
      ],
      "metadata": {
        "id": "8e4mL0R5898I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 損失関数に正則化項を入れて過学習を抑制する。\n",
        "そこで出て来るのが正則化と呼ばれる手法になります。\n",
        "\n",
        "$$ y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + ...  + \\beta_{k} x_{k} + e $$\n",
        "\n",
        "線形回帰モデルの学習では、残差の二乗の合計値、**目的関数**である**残差平方和 (residual sum of squares)** が最も小さくなる係数 $\\beta$ と誤差 $e$ の直線を求めていました。\n",
        "\n",
        "$$ 残差平方和: \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 $$\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/pw/ACtC-3eIFmh8PDRx64eFArwdgxO2CGt3PEi272ny1dyqAMue0un_yL_GMgZ0CsyvBnX4lEC9BfOEdfTNGsiEG-R4xZDPM9zMHwHcINcnQFxcdTmSgsF7LotLsBpwzs0S49fZtN1fQrbHY7JrB9m2kwuDGb9r=w815-h560-no?authuser=0\" alt=\"least squares\" height=\"180px\">\n",
        "\n",
        "正則化では、この残差平方和に正則化項(罰則項)というものを加え、この値を最小化するような係数 $\\beta$ と誤差 $e$ を求めます。\n",
        "\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|^p$$\n",
        "\n",
        "正則化項を加えることで、全体的に偏回帰係数 $\\beta$ の重みの影響を制限しようとする (係数の絶対値を小さくする) 形になります。\n",
        "\n",
        "言葉で説明すると...、学習データの誤差や外れ値まで説明できるように、実際はあまり重要ではない変数の偏回帰係数の値を大きくしたら、罰則化項によって目的関数が大きくなってしまうので、多くの学習データの説明に重要な影響を与えている様な説明変数の偏回帰係数にだけ重み付けを行えるようにしている。という形でしょうか。\n",
        "\n",
        "このノルムにおいて、p=1(L1ノルム)の場合をLasso回帰、p=2(L2ノルム)の場合をRidge回帰と呼び、どちらも回帰係数の重みに対して制限をしようとしますが、以下の様な特徴が異なります。\n",
        "\n",
        "**Ridge回帰**\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|^2$$\n",
        "\n",
        "係数の重みの強さを制限する効果を持ち、説明変数間の相関性が強い場合などにも予測式が不安定にならない傾向がある。\n",
        "\n",
        "**Lasso回帰**\n",
        "$$ \\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^2 + \\alpha\\sum_{j=1}^{k} |\\beta_j|$$\n",
        "\n",
        "Ridge回帰と同じく、係数の重みを制限し、正則化を強めるとともに係数がゼロとなり、選択された説明変数のみでモデルを構築できる。\n",
        "\n",
        "また、両者の正則化項を組み合わせた**ElasticNet**と呼ばれるものもあります。"
      ],
      "metadata": {
        "id": "QcfHOM7eJjoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 遺伝子50個を使用し、Ridge回帰を使ってみる\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習、ここでRidgeを指定。alphaは正則化の強さ\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "lr50 = Ridge(alpha=3)\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbaxtRdggCG9",
        "outputId": "4f122dc1-0d1d-4994-971e-d2d5c86d2a40"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient= [-0.09837905 -1.20946717 -0.30252653 -0.45922002 -0.54602016 -1.40379843\n",
            " -0.53169951 -0.53510242 -0.11509993  0.39315535  1.78505403  0.60046197\n",
            " -1.39376781 -0.84913763  0.70758606 -0.46690386 -1.0753647  -0.26494364\n",
            "  1.21325587  0.64772766 -0.05953757  1.26849056  1.21610602  1.76393224\n",
            " -1.31762386 -0.59529882 -0.33810497  1.96395903  0.60017642  0.06488966\n",
            " -0.18329932 -1.14297356  0.40012384  0.57858919 -0.22399018 -0.85313593\n",
            "  0.914275    0.26466054 -0.27684644  0.79785685 -0.6662362  -0.60996568\n",
            "  0.49259242 -0.95876531 -0.30139258  1.08983743  0.606337    1.54931487\n",
            " -0.2589634   2.82512205]\n",
            "training:  0.9532822848631699\n",
            "test:  0.4742104395843967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 遺伝子50個を使用し、Lasso回帰を使ってみる\n",
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = np.array(df.loc[:,\"gene_1\":]) # 説明変数50個\n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "ss = StandardScaler()\n",
        "x_train_ss = ss.fit_transform(x_train)\n",
        "x_test_ss = ss.transform(x_test)\n",
        "\n",
        "# モデル選択＆学習、ここでLassoを指定。alphaは正則化の強さ\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "lr50 = Lasso(alpha=0.3)\n",
        "lr50.fit(x_train_ss, y_train)\n",
        "\n",
        "# 係数b、誤差e\n",
        "b = lr50.coef_\n",
        "e = lr50.intercept_\n",
        "print(\"Coefficient=\", b)\n",
        "# print(\"Intercept=\"  , e)\n",
        "\n",
        "# モデルの評価: 決定係数R2\n",
        "r2_train_ss = lr50.score(x_train_ss, y_train)  # トレーニングデータ\n",
        "r2_test_ss = lr50.score(x_test_ss, y_test)    # テストデータ\n",
        "print(\"training: \", r2_train_ss)\n",
        "print(\"test: \"    , r2_test_ss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1fgfK0BoizF",
        "outputId": "7388a95a-7dd7-4814-d85e-bc70d3b56750"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient= [ 0.         -0.11812517 -0.         -0.         -0.15500275 -1.15870018\n",
            " -1.24451302  0.         -0.          0.          1.23222698  0.\n",
            " -0.09158115 -0.1844029  -0.         -0.04862532 -0.10412297 -0.\n",
            "  0.56375472  0.         -0.          0.79286073  0.          1.14764214\n",
            " -0.76752587 -0.         -0.          1.53766705  1.8091931   0.\n",
            "  0.         -0.         -0.         -0.         -0.         -0.\n",
            "  0.          0.         -0.          0.         -0.77590328 -0.\n",
            " -0.         -1.10141944  0.          0.92980714  0.06533088  1.27741822\n",
            " -0.34611079  1.18367153]\n",
            "training:  0.8549133051184812\n",
            "test:  0.6799953079933205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lasso回帰だと何をやってるのかイメージしやすいと思います。\n",
        "\n",
        "正則化を強めていくことで偏回帰係数が0となる説明変数が増えていくので、少ない説明変数で目的変数を説明しようとするモデルになっていきます。\n",
        "\n",
        "$$ 正則化なしだと\\space y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + ...  + \\beta_{k} x_{k} + e $$\n",
        "\n",
        "$$ y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + ...  + \\beta_{k} x_{k} + e $$\n",
        "\n",
        "$$ y = \\beta_{1} x_{1} + \\beta_{2} x_{2} + ...  + \\beta_{k} x_{k} + e $$\n"
      ],
      "metadata": {
        "id": "uiYna5WchycO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実習2\n",
        "\n",
        "使う変数を自由に選択したり(50遺伝子全て使っても良いし、30遺伝子でスタートしても良い)、\n",
        "Lasso回帰やRidge回帰(ElasticNetも使い方を調べて使ってみても良いです)を使用したり正則化の強さを調整したりして、時間内で作ることが出来る最高のモデル(テストデータの$R^2$が最高のもの)を作ってみてください。"
      ],
      "metadata": {
        "id": "sV4ajNUngQo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用する変数\n",
        "import numpy as np\n",
        "x = \n",
        "y = np.array(df[\"phenotype\"])   # 目的変数\n",
        "\n",
        "# データ分割: 100サンプルを「トレーニングデータ70%、テストデータ30%」に分ける\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# モデル選択＆学習"
      ],
      "metadata": {
        "id": "ytJr2RI-gK0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-YywDR48RdG"
      },
      "source": [
        "---\n",
        "\n",
        "## まとめ\n",
        "\n",
        "　使用する説明変数の数を増やして、予測モデルの改善をおこないました。「機械学習 - 線形回帰（1）-」のテキストよりは良いスコア（決定係数）が得られたと思います。興味があれば、さらに良い予測モデルの構築をおこなってみてください。\n",
        "\n",
        "　次回は、目的関数（コスト関数）の最小値を探すアルゴリズム **勾配法 Gradient method** を学びます。勾配法は、機械学習では非常に重要なアルゴリズムです。高度な機械学習モデル、ニューラルネットワーク（Neural network）などにも使われています。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AMFfdtV_AeDJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}